{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means & PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how Principal Component Analysis (PCA) can be used as a tool for simplifying a high dimensional dataset by properly reducing its features dimensions. \n",
    "\n",
    "Another tool for simplifying data is the K-means algorithm. The problem here is not necessarily that the data lives in high dimension, but that there is too much data to process. \n",
    "\n",
    "These algorithms are designed to __reduce the number of samples__ in a dataset (or the __data dimension__) and help us either to __simplify process__ or __understand the structure of our data__.\n",
    "\n",
    "In this first exercice, you will:\n",
    " - __Apply K-means__ in a 2D dataset to help gain an intuition of how the algorithm works.\n",
    " - __Apply K-means__ for image compression by reducing the number of colours that occur in an image to only those that are most common in that image.\n",
    " - __Apply PCA__ in a 2D dataset to get intuition on how PCA works, and then to a bigger dataset of face images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate & Plot Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`sklearn.datasets.make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) is an `sklearn` method that uses a Gaussian equation to generate separable datasets for clustering or classification.\n",
    "\n",
    "ðŸ‘‰Use it to generate some test data (X, y) with the following parameters:\n",
    "- 500 data points\n",
    "- 5 classes (or clusters)\n",
    "- `random_ state`=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot what your data looks like (it should be 4 distinct clusters of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, \n",
    "\n",
    "1) You've created a fake dataset X, with no target at all (no supervision with a `y` vector).\n",
    "\n",
    "2) This dataset has 500 observations, and 2 dimensions (or features), which are represented as two-dimensional positions on the plot you just made.\n",
    "\n",
    "3) This dataset still has a structure, which is 4 clusters in our case with different centers (or `centroids`)\n",
    "\n",
    "While fake here, you can find this kind of structure in real life - for example clients represented by their characteristics can belong to implicit segments etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to find automatically the clusters that gives you the inherent structure of your data. Those clusters are only represented by their center.\n",
    "\n",
    "What you have to determine though, is the __number of clusters__ that you think the data has.\n",
    "\n",
    "[`sklearn` has a `KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) package that does the math for you. \n",
    "\n",
    "ðŸ‘‰Import `KMeans` from `sklearn` and initiate a model with the follwing parameters:\n",
    "- `n_clusters=2`,\n",
    "- `random_state=5`\n",
    "\n",
    "ðŸ‘‰Then, fit the created data to the model, using `fit_predict`, which both fits the Kmeans model AND predicts back on the observations. \n",
    "**It hence returns an array of cluster index assignment for every observation in X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot the prediction of clusters that KMeans gives you. In order to have a better overview of what the KMeans has found as clusters (here only 2), we want to color every observation with a distinct color corresponing to a different cluster. \n",
    "\n",
    "ðŸ‘‰In order to do that, just plot your scattered observations, and pass the prediction vector that you just got as the color of the observation using `c=prediction_vector` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must see, the KMeans has found 2 clusters as required. It's not the modeling we want. We want to find the optimal number of clusters that represents our data the best. How do we find that optimal number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the Appropriate Number of Clusters\n",
    "\n",
    "We can use two techniques to find the appropriate number of clusters. \n",
    "\n",
    "You have to know that the Kmeans also returns a property named `inertia_` that **returns the sum of squared distances of samples to their closest cluster center**. \n",
    "\n",
    "It can serve as an indicator to determine how much we have of **variance explained** the data by the found clusters. \n",
    "\n",
    "### The Elbow Method\n",
    "\n",
    "This is a technique that is used to help us find the appropriate number of cluster in K-Means. \n",
    "\n",
    "This method looks at the percentage of variance explained as a function of the number of clusters: one should choose a number of clusters so that __adding another cluster doesn't give much better modeling of the data__ \n",
    "\n",
    "More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but __at some point the marginal gain will drop, giving an angle in the graph__. \n",
    "\n",
    "The number of clusters is chosen at this point, hence the \"elbow criterion\". \n",
    "\n",
    "NB: This \"elbow\" cannot always be unambiguously identified.\n",
    "\n",
    "ðŸ‘‰__To do that, you want to `fit` a KMeans for every number of cluster between 1 and 10, and save the explained variance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you want to plot the inertias as a function of the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an \"elbow\" where the inertia drops dramatically at 4 (since we generated that data, we know it's the right answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Hierarchical Clustering\n",
    "\n",
    "From the previous Elbow Method we can say that the optimal clusters are 4. This can also be confirmed by another statistical technique called [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering).\n",
    "\n",
    "Try to plot the dendogram linkage of the hierarchical clustering, that minimizes the within-cluster variance with the 'ward' method. You should get 4 clusters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit K-Means with 4 Optimal Clusters\n",
    "\n",
    "Now that we have found the optimal number of clusters, we can go on and fit a KMeans over our observations, and color every cluster with a different color to look at our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Note:** Does feature scaling always improve the clustering results? \n",
    "\n",
    "[Additional learning](https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality reduction with K-means\n",
    "\n",
    "Let's move on to a more difficult problem. \n",
    "\n",
    "In the previous category we have seen the following setting : 4 clusters (or categories / classes / output / target), 500 observations (or samples) of two dimensions (x position and y position).\n",
    "\n",
    "Now we're gonna apply KMeans but to a different problem : an image is gonna be our dataset. It means that every pixel represents one observation or training sample. And every pixel has 3 components (red, green & blue).\n",
    "\n",
    "ðŸ‘‰You can execute the code below to create an image dataset - use `pip install scikit-image` to install the skimage library which has a bunch of utilities to manipulate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "\n",
    "# Show image\n",
    "caller = getattr(data, 'astronaut')\n",
    "img = caller()\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent it correctly for machine learning use, we need to transform the representation of the image.\n",
    "\n",
    "For that, you need to reshape the image that currently has a size of `width x height x 3` to a long column vector of size N x 3 where `N = width x height`. Do it and put this dataset into a matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of image.\n",
    "img_size = img.shape\n",
    "\n",
    "# Reshape\n",
    "X = img.reshape(img_size[0] * img_size[1], img_size[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use it to fit a KMeans algorithm over the pixels, with the number of clusters being the number of colors you which to compress your image with. \n",
    "\n",
    "That means that if you choose let's say 64 clusters, it's gonna try to find 64 groups of similar pixels (close in colors). \n",
    "\n",
    "The center of each group (or clusters) is gonna be the average color of the pixels that belong to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first let's try to fit a KMeans with `32` clusters (aka reducing the image to 32 colors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just created our compression! Why is that? Because the 64 cluster centers are now our new colors. \n",
    "\n",
    "Take a look at the `labels_` output of your KMean model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have an assignment of every pixel to the cluster it belongs to. We can use that to find what this color is and plot our compressed image.\n",
    "\n",
    "You can retrieve the simplified color for every pixel, which is actually the cluster center it belongs to (you can find it in `cluster_centers`). Put that in a new `image_compressed` variable and look at what the color values look like for every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the problem is that these colors are mean values that are not good to represent. We need integers between 0 and 255 to have a color that the computer can understand. You can execute the line below to transform it to manageable colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_compressed = np.clip(image_compressed.astype('uint8'), 0, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now reshape this vector into the right dimensions (the original dimension of width x height x 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now just plot this image using `imgshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and the compressed image.\n",
    "fig, ax = plt.subplots(1, 2, figsize = (7, 7))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(image_compressed)\n",
    "ax[1].set_title('Compressed Image')\n",
    "for ax in fig.axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Increase or decrease n_clusters = number of colours if you want to see the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Use the elbow method\n",
    "\n",
    "You can try to use the Elbow method to find the optimal compression that loses the less color information (very long training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
