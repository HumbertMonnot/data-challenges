{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a fast and flexible unsupervised method for dimensionality reduction in data. \n",
    "\n",
    "In this section, PCA will be applied to perform dimensionality reduction.\n",
    "\n",
    "Lets start with a small dataset to get intuition on how PCA works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate and plot a 2d dataset.\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 100)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the linear relationship between the x and y?\n",
    "\n",
    "In unsupervised learning instead of predicting y from x, we attempt to learn about the relationship between them. In PCA, this relationship is quantified by finding a list of the principal axes in the data. Lets do this using Sklearn.\n",
    "\n",
    "First, import the PCA and call PCA with `n_components=2`. Fir the PCA to the `X` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is two objects we're interested in that `pca` object : \n",
    "\n",
    "- `pca.components_`: it's a set of eigenvectors which point to the directions where the variance is maximally explained. In other words, the directions of maximum variance.\n",
    "\n",
    "- `pca.explained_variance_`: it's the corresponding squared length of the proportion of explained variance\n",
    "\n",
    "What we can do is visualize those two components by plotting those components, with size squareroot of the corresponding explained variance. \n",
    "\n",
    "To do that you can call the `draw_vector` function below.\n",
    "\n",
    "They respectively the 2 directions of maximum variance \n",
    "and (squared) proportion of explained variance in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->', color='red', linewidth=2)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the vector is a measure of the variance of the data when projected onto that axis!\n",
    "\n",
    "We can then use those directions to \"explain\" most of our observations behaviour - most of the distinction between observations happens along thoses axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what happens when we use those components?\n",
    "\n",
    "We can use those components (`pca.components_`, the red arrows on the plot) to transform every sample of our dataset and see it in a new space where its variance is more clear and hence more easy to visualize.\n",
    "\n",
    "To do that, first transform the dataset into the new space by using the function `transform` of your `pca` object, then you can plot this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have stretched the whole dataset into it's nicer form, where we can now study the behviour __between__ the observations. It's not crushed anymore into a line that was too packed.\n",
    "\n",
    "__Try to understand how we passed from the original dataset, then took the two red arrows which represent the directions of most variance, to transform the observations to this new plot.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with PCA\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "We can then use those new representations as features to feed any model we want. It can be very useful since you often have lots of features, and you want to transform and keep a packed number of features that are the most representative of what you want to model.\n",
    "\n",
    "__Lets load a face image dataset and apply PCA.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have access to\n",
    "- `faces.images` which are the matrix of __50 x 37 pixels__ you can plot \n",
    "- the flattened version in `faces.data` of size __1850 x 1__ (because 50 x 37 = 1850)\n",
    "- `faces.targets` which is annotation of every face to the corresponding person (as a number index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s visualize some faces.\n",
    "fig = plt.figure(figsize=(7,10))\n",
    "for i in range(15):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.title(faces.target_names[faces.target[i]], size=12)\n",
    "    plt.imshow(faces.images[i], cmap=plt.cm.gray)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scikit dataset we have 50 Ã— 37 pixel images (1850 dimensions!). Often, so many dimensions is a lot to train algorithms we studied in the previous exercises (for example SVM). \n",
    "\n",
    "That's why we use PCA to reduce these features to a more manageable size, while maintaining most of the information of the dataset.\n",
    "\n",
    "__ðŸ‘‰Apply PCA to the dataset (both fit and transform), to reduce dimensions to 150, by setting `n_components=150`__\n",
    "\n",
    "__ðŸ‘‰Put your transformation into a variable named `projected`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The face dataset was projected onto only the first 150 principal components! Again, what we call components are directions of most variance of the dataset.\n",
    "\n",
    "It means that now, we don't need 1850 pixels anymore to describe each images but just 150 values. \n",
    "\n",
    "__How is that possible?__\n",
    "\n",
    "The pca has found to be the most representative directions of what distinguishes faces between each other with just 150 values for every image. \n",
    "\n",
    "They are the directions of most variance.\n",
    "\n",
    "You can access them in `pca.components_`. Look at the first component of this array of components, and its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's a vector of 1850 values. **We have now 150 components of 4096 values each.**\n",
    "\n",
    "One face is now described as a combination (sum) of those components.\n",
    "\n",
    "We're gonna reconstruct one image from its reduced representation to see how it works.\n",
    "\n",
    "__ðŸ‘‰study the code below__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "num_dimensions = 150\n",
    "\n",
    "# We do our reconstruction over the 10th image\n",
    "image_original = faces.images[12];\n",
    "image_compressed = projected[12];\n",
    "\n",
    "# we start the reconstruction from the mean over all images (computed by pca)\n",
    "image_reconstructed = pca.mean_; \n",
    "\n",
    "# Reconstruct the image by weighing every entry of its compressed representation to the corresponding component\n",
    "for i in range(num_dimensions):\n",
    "    image_reconstructed += pca.components_[i] * image_compressed[i]\n",
    "    \n",
    "# Plot the original and the compressed image.\n",
    "fig, ax = plt.subplots(1, 2, figsize = (5,5))\n",
    "ax[0].imshow(image_original, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(image_reconstructed.reshape(faces.images[0].shape), cmap=plt.cm.gray)\n",
    "ax[1].set_title('Compressed reconstructed Image')\n",
    "for ax in fig.axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose the Number of Components?\n",
    "\n",
    "In practice, it is very important to find how many components are needed to describe the data without losing too much information. This can be determined visually by plotting the cumulative sum of `explained_variance_ratio_` as a function of the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total variance is contained within the first components. For example:\n",
    "- The first 20 components contain more than 75% of the variance,\n",
    "- while we need about only 70 components to describe 90% of the variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now as a machine learning specialist, what is your use of this transformation?\n",
    "\n",
    "You have this dataset of faces and you want to build a face recognition engine. \n",
    "\n",
    "You can now use this low-dimensional new transformation you just created, that is still representative of the faces to train your supervised algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train test split the face dataset\n",
    "\n",
    "Use the train test split function from scikit to separate __the original faces dataset__ into training and testing set, `Xtrain`, `ytrain`, `Xtest`, `ytest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform your training set to reduce the number of dimensions / features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a PCA __over the training data only__ and transform your training data into the reduced dimension (150 features for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this same PCA (only trained on the training set) transform your testing set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cross validate your choice of best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call a cross validated grid search for an SVM where you fine tune the hyperparameters C between 1000 and 10000 and gamma between 0.0001 and 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train that cross validation grid search over our newly transformed training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) of your best model over the testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve this score with the best choice of PCA components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
