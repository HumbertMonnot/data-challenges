{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email spam classification\n",
    "\n",
    "We're gonna try to classify emails in a dataset into spams.\n",
    "\n",
    "The first step will be to load the emails, then we're gonna need to clean them a bit.\n",
    "\n",
    "The second step will be to transform text features into a representation encoding that the machine can understand. We will use TF-IDF encoding here (term frequency inverse document frequency).\n",
    "\n",
    "And finally, we're gonna use Multinomial Naive Bayes to represent our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data and preprocess\n",
    "\n",
    "Click this [link](https://wagon-public-datasets.s3.amazonaws.com/Data-Challenges_ML-Day02-Ex03_emails-dataset.pickle) to download the **spam email dataset**. Then put the file in this **exercise folder**.\n",
    "\n",
    "Loading the dataset using with [`open [DOC]`](https://www.pythonforbeginners.com/files/reading-and-writing-files-in-python). The file is a pickle binary file so you're gonna need to use the 'rb' parameter.\n",
    "\n",
    "To load the pickle file into the data frame, simply `import pickle` and then use it like this\n",
    "```python\n",
    "emails_df = pickle.load(file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text strings are still in \"Bytes\" format (you can see they start with `b'`). \n",
    "\n",
    "We need to convert them into regular encoded strings. For that you can use a lambda function over the \"message\" column\n",
    "\n",
    "`.apply(lambda x: x.decode('latin-1'))`\n",
    "\n",
    "Please take a look at lambda functions, and string encodings, they're really common procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def clean_email(email):    \n",
    "    email = re.sub(r'http\\S+', ' ', email)\n",
    "    email = re.sub(\"\\d+\", \" \", email)\n",
    "    email = email.replace('\\n', ' ')\n",
    "    email = email.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    email = email.lower()\n",
    "    return email\n",
    "\n",
    "emails_df['message'] = emails_df['message'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call head to see what it looks like, and maybe some histograms in the repartition of spam vs ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Count the occurences in the email\n",
    "\n",
    "There is a problem with how to encode the email strings. We want to have an encoding that we can feed to our machine learning models. One way of doing it is having a dictionary of the size of the whole indexed vocabulary, and representing every document with the number of occurences of each word.\n",
    "\n",
    "Let's say you have 3 emails you want to encode\\\n",
    "'name is Thomas'\\\n",
    "'name is David'\\\n",
    "'you have time today'\n",
    "\n",
    "The whole encoding would be \n",
    "\n",
    "| | name | is | Thomas | David | you | have | time | today |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "x1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "x2 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
    "x3 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 |\n",
    "\n",
    "(As you can notice, it's a very sparse representation of the data !)\n",
    "\n",
    "We're gonna use the utility class of sklearn [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "Use the param:\n",
    "`stop_words`: in order to remove the words that don't bring any information (like 'and', 'for', 'the' etc)\n",
    "`max_df`: eliminate dataset specific stop words by removing words that appear in too many documents (let's say 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `get_feature_names()` on your vectorizer gives you a dictionary of the words (= features) used for your representation of each email. Get the number of features you're working on. You should be at around 144143 unique words/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are satisfied with the representation of our document, we're gonna split the dataset into testing / training set. use the `train_test_split` function over our new representation of the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the shape of your dataset. With a test size of 30%, it should have about the size of 23601 and 10115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Call and fit multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the naive bayes classifier over our training set. Make sure to test the score on the training set as well. Careful to use the right Naive Bayes model (Multinomial or Gaussian?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Observe classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try on a few values\n",
    "\n",
    "You can now observe the quality of your classifier by entering email strings and seeing if it classifies correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_samples = [\"Hello George, how about a game of tennis tomorrow?\",\n",
    "         \"Dear Sara, I prepared the annual report. Please check the attachment.\",\n",
    "         \"Hi David, will we go for cinema tonight?\",\n",
    "         \"Best holidays offers only here!!!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the classifier `coef_` parameter, try to get the top 10 features and use the `get_feature_names()` to find the corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe missclassification\n",
    "\n",
    "Try to extract some missclassified emails and look at what might have made the classifier an error.\n",
    "Hint : you can use `np.where`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optional - Minimum train size\n",
    "\n",
    "Using `learning_curve` Find the minimum training size that you would need to get 97%+ performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Optional - use tf-idf\n",
    "\n",
    "Try to see if you can improve the performance using [`tf-idf`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "with [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) instead of a simple count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
