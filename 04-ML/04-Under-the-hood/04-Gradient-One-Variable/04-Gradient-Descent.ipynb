{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Gradient Descent\n",
    "\n",
    "The next part of the linear regression algorithm is the optimization of our parameters $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "In Exercise 03, we have seen what is a cost function and how to use it to calculate the error associated with estimations on the whole dataset. Then, we have tried multiple parameter combinations to see the effect on the error and find good parameter values.\n",
    "\n",
    "However, it is quite computing demanding to calculate the error for a lot of combinations: imagine doing so with big functions like neural networks that can have millions of parameters!\n",
    "\n",
    "Fortunately, there is another way: we can *minimize* the cost function to find good parameter values. To do so, we need to find how to update each parameter (should we increase or decrease the its value). For each parameter, the right direction is given by the slope on the cost function. And since the slope of a function is given by its derivative, we use the cost function derivative! Don't worry, we'll do that one step at a time!\n",
    "\n",
    "When we say: \"for each parameter\", this means that we calculate the derivative with respect to each parameter separately. This is called *partial derivatives*. Then, we can stack all these partial derivatives (one per parameter) in a vector. This vector is called the *gradient* of the function. This why we call this procedure *gradient descent*: we use the gradient to minimize the function.\n",
    "\n",
    "However, we will simplify the problem in this exercise and use only one variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start with regular imports and functions from previous exercises. You will also load some data using the file format `h5` to be sure that you can start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/ciqual_small.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get these functions from previous exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def h(x, theta0, theta1):\n",
    "    # TODO: Take h() from Exercise 01\n",
    "    pass\n",
    "\n",
    "def L(x, y, theta0, theta1):\n",
    "    # TODO: Take L() from Exercise 02\n",
    "    pass\n",
    "\n",
    "def plot_reg_line(x, y, theta0, theta1):\n",
    "    # TODO: Take plot_reg_line() from Exercise 02\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here some variables from Exercise 03 that we'll need here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "hf = h5py.File('../data/data_exercises.h5', 'r')\n",
    "\n",
    "cost_all_stand = np.array(hf.get('cost_all_stand'))\n",
    "cost_all = np.array(hf.get('cost_all'))\n",
    "\n",
    "best_params = np.array(hf.get('best_params'))\n",
    "best_params_stand = np.array(hf.get('best_params_stand'))\n",
    "\n",
    "best_theta0 = np.array(hf.get('best_theta0')).reshape(-1)[0]\n",
    "best_theta1 = np.array(hf.get('best_theta1')).reshape(-1)[0]\n",
    "best_theta0_stand = np.array(hf.get('best_theta0_stand')).reshape(-1)[0]\n",
    "best_theta1_stand = np.array(hf.get('best_theta1_stand')).reshape(-1)[0]\n",
    "\n",
    "theta0_grid = np.array(hf.get('theta0_grid'))\n",
    "theta1_grid = np.array(hf.get('theta1_grid'))\n",
    "theta0_grid_zoom = np.array(hf.get('theta0_grid_zoom'))\n",
    "theta1_grid_zoom = np.array(hf.get('theta1_grid_zoom'))\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Our Cost Function Convex?\n",
    "\n",
    "#### Important Note\n",
    "\n",
    "In this part, we will use the standardized data. It will be easier to look at the shape of the cost function! In latter parts, we will use raw data however. So for now, use the variables `cost_all_stand`, `theta0_grid_zoom`, `theta1_grid_zoom` and not `cost_all`, `theta0_grid` and `theta0_grid`!\n",
    "\n",
    "As a first task, you will check the shape of the cost function that you have implemented in Exercise 03. We should find that our cost function is *convex* (think of it as a bowl-shaped function), which is good for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to plot the cost functions separately for the two parameters `theta0` and `theta1`. You should do two plots:\n",
    "\n",
    "- the cost in function of `theta0` taking a fixed value of `theta1` (its best value: `best_theta1`)\n",
    "- the cost in function of `theta1` taking a fixed value of `theta0` (its best value: `best_theta0`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be even more informative to plot the cost function according to both parameters at the same time. To do that, you will create a three-dimensional plot with Matplotlib.\n",
    "\n",
    "Your task is to plot the cost function using a surface plot.\n",
    "\n",
    "<details>\n",
    "<summary>hint</summary>\n",
    "https://matplotlib.org/3.1.0/gallery/mplot3d/surface3d.html\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>hint 2</summary>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html\n",
    "</details>\n",
    "\n",
    "You can also run `%matplotlib notebook` to make the plot interactive and be able to move the shape. To come back to static plots, run `%matplotlib inline` instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a vertical line corresponding to the the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Note\n",
    "\n",
    "Be careful and remember that we use `cost_all_stand` in this exercise. This is only to visualize the gradient descent process. In a real setting, you don't need to calculate the cost for all combinations of parameters. This is the point of gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "#### Important Note\n",
    "\n",
    "In this part, we will use the raw data instead of the standardized data. This will allows us to see issues like learning rate more clearly. So be sure to use the variables `cost_all` and `data`, `theta0_grid`, `theta1_grid` and not `cost_all_stand`, `data_stand`, `theta0_grid_zoom`, `theta0_grid_zoom`!\n",
    "\n",
    "The goal of gradient descent it to tell at any point on the convex curve: what is the direction that would minimize the function, according to each direction (each parameter)?\n",
    "\n",
    "In this exercise, we'll simplify the procedure and consider only one variable. Let's say that we constrain `theta0` (the intercept) to be 0: we only need to optimize the cost function with respect to `theta1` (the slope).\n",
    "\n",
    "So we'll freeze the value of `theta0` to 0 and zero-initialize the value of `theta1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what gradient descent do, let's plot again the cost in function of `theta1`, this time with `theta0 = 0`. You can also try to plot the initial value of `theta1` as a point on the curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have represented the cost function that we want to minimize. We also initialized our single parameter `theta1`. We have represented this initial value on the curve.\n",
    "\n",
    "Now, let's say that you could slide this point on the curve. What direction would you use to decrease the error?\n",
    "\n",
    "- 1. Increase `theta1`\n",
    "- 2. Decrease `theta1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good to be able to do that as a human. However, we need to let the algorithm do it alone. The solution is calculate the slope at this point:\n",
    "\n",
    "- if the slope is negative, we need to increase the parameter value (red curve on the figure)\n",
    "- if the slope is positive, we need to decrease the parameter value (green curve on the figure)\n",
    "\n",
    "<img src=\"images/slopes.png\" width=\"500\">\n",
    "\n",
    "Minimizing the function means finding the bottom of the bowl.\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "To find the slope and thus the right direction for the next value of `theta1`, we need to calculate the derivative of the cost function.\n",
    "\n",
    "To help you, here is the derivative of the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_derivative(x, y, theta0, theta1):\n",
    "    m = x.shape[0]\n",
    "    derivative = (1 / m) * np.sum((h(x, theta0, theta1) - y) * x)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the cost function and its derivative implemented. Your task is to use the derivative function to calculate the slope of the cost function at our initial point. This point corresponds to our initial `theta1` (we initialized it to 0) and to our fixed `theta0` (also 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find $-419.72169811320754$.\n",
    "\n",
    "The derivative is negative: this means that the slope of the tangent line at this point corresponding to the initial `theta1` value is negative (the $y$ value decreases when the $x$ value increases).\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "In addition to the sign of the derivative, the value is giving you some information as well. A large value means that the tangent line is steep and a small value means that the line is flat.\n",
    "\n",
    "The gradient descent procedure uses this property: we update the parameter by subtracting the value of the derivative to the current value of theta. Your task is to implement this `update()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def update(x, y, theta0, theta1):\n",
    "    # TODO: the update function returns theta0 and theta1: the new value of theta1\n",
    "    # and, since we freeze theta0, the unmodified value of it\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your function with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 419.72169811320754)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update(x=data['Zinc (mg/100g)'],\n",
    "       y=data['Phosphorus (mg/100g)'],\n",
    "       theta0=0,\n",
    "       theta1=theta1)\n",
    "# You should get (0, 419.72169811320754)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your tests are good, update your value of `theta1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0, theta1 = update(x=data['Zinc (mg/100g)'],\n",
    "                y=data['Phosphorus (mg/100g)'],\n",
    "                theta0=0,\n",
    "                theta1=theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the new value of `theta1` on the cost function curve, like you did with the initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the cost with the updated `theta1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting All Together\n",
    "\n",
    "You have all the building blocks needed to implement gradient descent. Before fixing the issue of the increasing cost, you will put all these blocks together to implement the gradient descent function. To summarize, here is the procedure:\n",
    "\n",
    "1. Give parameters initial values (zero-initialization)\n",
    "2. Calculate the estimated values with these parameters for all training examples\n",
    "3. Calculate the error between the estimated values and the true values with the cost function `L()`\n",
    "4. Calculate the derivative of the cost function according to these parameters\n",
    "5. Update the parameters by subtracting the derivative (`theta0` stays 0)\n",
    "6. Repeat steps 2 to 5 (each repetition corresponds to one *epoch*)\n",
    "\n",
    "You can try to write a function that will take `x`, `y` and the initial value of `theta1` as inputs and implement these steps. Your function should return the history of the cost values (this will help us to check that the cost is decreasing) and the last `theta1` value.\n",
    "\n",
    "We also provide you with a function that you can use to visualize how the gradient descent perform at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_viz(x, y, theta0, theta1):\n",
    "    cost_for_theta1 = L(x, y, theta0, theta1)\n",
    "    \n",
    "    plt.figure()\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=False)\n",
    "    \n",
    "    ax1.plot(theta1_grid, cost_all[best_params[0]])\n",
    "    ax1.scatter(theta1, cost_for_theta1)\n",
    "\n",
    "    plot_reg_line(x, y, theta0, theta1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Cost: {cost_for_theta1}\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use it like this:\n",
    "\n",
    "```python\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "if viz:\n",
    "    gradient_descent_viz(x, y, theta1)\n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "```\n",
    "\n",
    "The function `time.sleep(0.5)` allows us to wait 0.5 second between each iteration (change this value if you want to have more time to see the evolution). This is only to have time to see what is going on.\n",
    "\n",
    "You can add the arguments `viz` to choose if you want visualization and `num_epochs` to set the number of times you want to do the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def gradient_descent(x, y, theta0, theta1, alpha, viz=True, num_epochs=5):\n",
    "    # TODO: implement the gradient descent function using all the building blocks\n",
    "    # we constructed today!\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your gradient descent function, you can run the following cell, check that you see the plots updating at each epoch and compare your results after the end of the 5 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1: 19351.8035126978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30944.596320754714,\n",
       " 214102.84392615716,\n",
       " 1601368.5941733664,\n",
       " 12108710.12517079,\n",
       " 91692759.56151654]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_history, theta1 = gradient_descent(x=data['Zinc (mg/100g)'],\n",
    "                                        y=data['Phosphorus (mg/100g)'],\n",
    "                                        theta0=0,\n",
    "                                        theta1=-10,\n",
    "                                        num_epochs=5,\n",
    "                                        viz=False)\n",
    "cost_history\n",
    "# You should get:\n",
    "# [30944.596320754714,\n",
    "#  214102.84392615716,\n",
    "#  1601368.5941733664,\n",
    "#  12108710.12517079,\n",
    "#  91692759.56151654,\n",
    "#  694473292.201679]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the test means that you implemented correctly your gradient descent algorithm! This is great! However, it looks like it is not working 🤔.\n",
    "\n",
    "To investigate the issue, you can try to plot the cost history. Create a function that take the array of costs and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def plot_cost_history(cost_history):\n",
    "    # TODO: plot the cost history\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to plot the cost evolution, you can see that the algorithm is getting worse and worse. You will understand why in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the evolution of the cost, you can see that it increases at each epoch. This is not good: it should be decreasing in order to find the minimum of the cost function! This illustrate one common problem with gradient descent: a too large *learning rate*.\n",
    "\n",
    "<img src=\"images/learning_rate.png\" width=\"400px\">\n",
    "\n",
    "The learning rate, named $\\alpha$ (alpha) is simply a number that we use to scale the parameter update: instead of subtracting the derivative from `theta1`, we will multiply the derivative with a small number (for instance $\\alpha=0.1$) before doing the subtraction.\n",
    "\n",
    "You can create a new `update()` function that implements the learning rate. Make it as a parameter: it will allows us to play with this hyper-parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def update_lr(x, y, theta0, theta1, alpha):\n",
    "    # TODO: add the learning to the update function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your new `update_lr()` function, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1245.3562264150944)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_lr(x=data['Zinc (mg/100g)'],\n",
    "          y=data['Phosphorus (mg/100g)'],\n",
    "          theta0=0,\n",
    "          theta1=-300,\n",
    "          alpha=1)\n",
    "# You should get (0, 1245.3562264150944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -145.46437735849054)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_lr(x=data['Zinc (mg/100g)'],\n",
    "          y=data['Phosphorus (mg/100g)'],\n",
    "          theta0=0,\n",
    "          theta1=-300,\n",
    "          alpha=0.1)\n",
    "# You should get (0, -145.46437735849054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can modify the function `gradient_descent()` to be sure that it uses the update including the learning rate. Use also $\\alpha$ as a parameter of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def gradient_descent_lr(x, y, theta0, theta1, alpha, num_epochs=5, viz=True):\n",
    "    # TODO: this function should be similar to gradient_descent() but using\n",
    "    # the learning rate\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this new function, you will train your linear regression algorithm with a learning rate $\\alpha$ of 0.1. Then, plot the evolution of the cost to see if it is better! To make it more visual, you can start with a very bad `theta1` value. Use -300 as an initial value of `theta1`, and still 5 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! The cost is decreasing: this means that your algorithm is learning. As a final step, you can train your algorithm with more epochs (`viz=False` to speed up the learning phase) and plot the regression line. You can play with the initial value of `theta1`, or the value of the hyper-parameter `alpha` to see if you can get a lower cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The gradient descent algorithm we implemented used the whole dataset at each epoch. This is called *batch gradient descent*. It can be slow if the dataset is large. In contrary, with *stochastic gradient descent* (SGD), only one data sample is used at each iteration. This allows us to fit large datasets since only one sample would be in memory at a time. It can also be useful when the cost function is not perfectly convex and has local minima: the stochastic nature of the SGD makes it more equipped to escape those local minima.\n",
    "\n",
    "You will implement SGD by modifying the previous function `gradient_descent_lr()`. At each epoch, you will update `theta1` once per data sample. To avoid cycles, you can choose a random index at each step. The update uses only the data sample corresponding to the random index.\n",
    "\n",
    "To save the `cost_history`, take the average of the cost for an entire epoch. It will be useful to see if the cost is decreasing. And storing the cost at each step would be very noisy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def SGD(x, y, theta0, theta1, alpha, num_epochs=5, viz=True):\n",
    "    # TODO: Implement the SGD algorithm\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at how the cost is decreasing using your function and plotting the cost history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! To conclude, the cost history plot shows that the cost is decreasing but is way more noisy than with batch gradient descent. This is due to the stochastic aspect of the algorithm. This is also possible to make a trade-off between these approaches and fit a certain number of data sample at each step (and not the whole dataset like with batch gradient descent or a single sample with stochastic gradient descent).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
