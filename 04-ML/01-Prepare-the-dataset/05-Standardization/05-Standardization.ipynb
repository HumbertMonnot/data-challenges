{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain Machine Learning algorithms assume that all features have a somewhat normal distribution, centered around zero and with a similar variance. However, that is rarely the case in wild datasets. A feature that has a significantly larger variance can overpower others and prevent the model to learn from them. \n",
    "\n",
    "Standaridization transforms each feature by removing its mean value (u) and dividing it by its standard deviation (s). As such, it is centered at zero.\n",
    "\n",
    "$\\huge z = \\frac{(x - u)}{s\n",
    "}$\n",
    "\n",
    "Standardization can be done using Sklearn's `StandardScaler` (doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) . In the cell below, standardize the the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0,0],\n",
    "        [1,1],\n",
    "        [2,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler is now stored in memory and can reproduce the equivalent transformation on new data. Transform the new data to verify it does the right transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [[1,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling to range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another transformation option is to scale to a range. The advantage of this method is its resistance to very small standard deviations. It also preserves zero entries in sparse datasets. There are two ways to scale to a range in Sklearn:\n",
    "\n",
    "- `MinMaxScaler` transforms to a chosen range - (doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html))\n",
    "\n",
    "- `MaxAbsScaler` transforms to a range [-1,1] - (doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html))\n",
    "\n",
    "Below, use `MinMaxScaler` to transform the data to a (-2,2) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0,0],\n",
    "        [1,1],\n",
    "        [2,2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MaxAbsScaler` works in similar fashion but transforms the data to a range [-1,1]. That transformation is better suited to data already centered at zero (standardized).\n",
    "\n",
    "Below, standardize the data before scaling it with `MaxSbsScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "data = [[-1,-1],\n",
    "        [1,1],\n",
    "        [3,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic approach to spotting outliers in a dataset is to graph a `boxplot` of your data, easily done with `matplotlib`.\n",
    "\n",
    "Check out the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html) and graph a boxplot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,1,3],\n",
    "        [2,2,1],\n",
    "        [3,10,2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the presence of outliers, the mean value of a feature can be extremely distorted and does not offer a robust base for standardizing.\n",
    "\n",
    "In such a case, a more stable solution is to exploit the median value and to scale according to the Interquartile Range (IQR). If the data was to be split into 4 quarters, the IQR represents the 2nd and 3rd quarters. By excluding the outermost quarters (1st and 4th), the algorithm intends to exclude the outliers.\n",
    "\n",
    "Sklearn's `RobustScaler` does just that! - (doc [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html))\n",
    "\n",
    "Go ahead, scale the data according to the quantile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same thing but instead of using the IQR by default, set a manual range to exclude the extreme fifths of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
