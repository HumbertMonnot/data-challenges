{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training Machine learning algorithms, preprocessed text needs to be transformed into numerical data. This process is called feature extraction, or vectorization. \n",
    "\n",
    "In this exercice, you will familiarize yourself with three popular feature extraction algorithms:\n",
    "- Bag-of-Words\n",
    "- N-grams\n",
    "- Tfidf\n",
    "- Part of Speech tagging\n",
    "\n",
    "The following operations will be completed on the following corpus (set of texts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['i love this game',\n",
    "          'football is a game',\n",
    "          'i love football']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag of words is a count of word occurences within a text.\n",
    "\n",
    "The algorithm is made up of two parts. First, it creates a dictionary from the entire corpus. Then, it transforms each text in the corpus as a vector of word occurences. It is called a \"bag\" because it disregards the order of the words within the text and focuses on content. \n",
    "\n",
    "A Bag of Words can be obtained using Sklearn's CountVectorizer. Initiate a default vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use the default vectorizer to transform the corpus to a Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are printed the original corpus, the features of our Bag of Words, and the corresponding vectors. Make sense of the code, you will need it later ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*corpus,sep='\\n') #Prints original corpus with each text at a new line\n",
    "\n",
    "print(' ') # Print a blank line to air things out ;)\n",
    "\n",
    "print(default_vectorizer.get_feature_names()) # Prints features of your Bag of Words\n",
    "\n",
    "print(' ') # Another blank line.....\n",
    "\n",
    "print( bag_of_words.todense() ) # Prints Bag of words vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, vectors represent the occurences of each word in each sentence. It is those vectors that are then used for Machine Learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A N-gram is a combination of N number of words treated as a single feature, as opposed to single word features in the Bag of Word. The idea is to extract contextual information and enrich data. For example the word \"good\" is always positive individually but can be negative when preceeded by \"not\". In certain cases, \"not good\" is an informative bigram.\n",
    "\n",
    "N-grams can also be extracted through sklearn's CountVectorizer, but with a specific parameter. Find out how that parameter works and initiate a Bigram (2-gram) vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform corpus to bigram vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the original corpus, the bigram features, and the corresponding vectors. Separate by a blank line to air out ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (Tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than counting occurences, the TfIdf vectorizer computes an importance value for each word in its text and according the entire corpus. That value is the product of the TF and the IDF.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "Don't you worry, the abovementioned steps of the Tfidf algorithm are automatized in Sklearn's TfidfVectorizer package.\n",
    "\n",
    "Import the package, initiate a default Tfidf vectorizer, and transform your corpus. Then, print the original corpus, tfidf features, and corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech tagging is a way to enrich text data by specifying the function of a word within its sentence (noun, verb, adjective, etc...).  \n",
    "\n",
    "If executed prior to Bag-of-Word vectorization, it can, for example, differentiate two words that are written the same but have different grammatical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"After I complete the bootcamp my life will be complete \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before being POS tagged, a sentence needs to be tokenized into words. Go ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now import and use NLTK's pos_tag package on your tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the two different words \"complete\" have a different function within the sentence. To check the meaning of each tag, use the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.help.upenn_tagset('VBP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Richness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the task you are working on, you may need to engineer your own feature. \n",
    "\n",
    "Assume that your task is to classify texts according to their author. In such case, Lexical Richness caries author specific information. One may have significantly more vocabulary than the other. However, there is no NLTK package to compute such a feature....\n",
    "\n",
    "Engineer a feature extraction function that returns a ratio of (unique words / total words). Apply it to the example below. There are a few tricks here ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"I love Malmo but it is too cold and i do not like the cold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "# Should return 0.8666666666666667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
