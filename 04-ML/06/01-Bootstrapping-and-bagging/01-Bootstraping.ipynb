{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction : Bias-Variance Trade-off\n",
    "\n",
    "In ML, we strive to minimize error. This error is due to:\n",
    "- Bias\n",
    "- Variance\n",
    "- Irreducible error (due to noise)\n",
    "\n",
    "Your goal as a data scientist is to make the Bias and Variance as small as possible. What's the problem? In reality, when we lower the variance, the bias increases and when we lower the bias, the variance increases.\n",
    "\n",
    "You have to find a good balance, a trade-off, between these 2 things :\n",
    "![bias-variance trade-off](https://www.francescopochetti.com/wp-content/uploads/2017/02/biasvariance-492x270.png).\n",
    "\n",
    "Another great visualization that helps understand this concept is the bull's eye:\n",
    "![bias-variance trade-off](https://qph.fs.quoracdn.net/main-qimg-a55358a5a12b02c3f71010c965a2c4dc).\n",
    "\n",
    "You have seen this before in the context of overfitting.\n",
    "\n",
    "So, the question is :\n",
    "- Is it possible to achieve lower variance and lower bias at the same time?\n",
    "- The trade-off occurs in the context of altering the complexity of the same model\n",
    "- Could we combine several models? We will explore this concept trough this challenge...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrapping (aka resampling) is a key concept. \n",
    "Combining several models to help us get lower variance and lower bias at the same time.\n",
    "\n",
    "What's \"magic\" about this :\n",
    "- you have the same data;\n",
    "- you calculate the same thing several times;\n",
    "- you get better results !\n",
    "\n",
    "With this first part, we will use bootstrap for the mean and variance values, not for a whole model.\n",
    "\n",
    "In the following cell implement the following algorithm in the function `bootstrap_mean_std` :\n",
    "```\n",
    "Given X = {X_1, X_2, ..., X_N}\n",
    "for b = 1..B:\n",
    "  X_b = sample_with_replacement(X) # Size X_b = N\n",
    "  sample_means[b] = X.mean()\n",
    "b_mean = sample_means.mean()\n",
    "b_std =sample_means.std()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def bootstrap_mean(X, B, N):\n",
    "    '''TO DO: this function return a tuple (b_mean, b_std) of\n",
    "        samples X thanks to the bootstrapping method'''\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can run the following code and have a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 200\n",
    "N = 20\n",
    "X = np.random.randn(N)\n",
    "\n",
    "# Call of your function\n",
    "b_mean, b_std = bootstrap_mean(X, B, N)\n",
    "print(\"The mean of X is : \", X.mean())\n",
    "print(\"The bootstrap mean of X is : \", b_mean)\n",
    "\n",
    "# Calculate the lower and upper bound for 95% Confidence Interval (Bootstrap)\n",
    "lower = b_mean + norm.ppf(0.025) * b_std\n",
    "upper = b_mean + norm.ppf(0.975) * b_std\n",
    "\n",
    "# Calculate the lower and upper bound for 95% Confidence Interval\n",
    "lower2 = X.mean() + norm.ppf(0.025)*X.std()/np.sqrt(N)\n",
    "upper2 = X.mean() + norm.ppf(0.975)*X.std()/np.sqrt(N)\n",
    "\n",
    "# Plot the vertical lines for Confidence Interval (CI)\n",
    "plt.axvline(x=lower, linestyle=\"--\", color=\"g\", label=\"Lower bound for 95% CI (bootstrap)\")\n",
    "plt.axvline(x=upper, linestyle=\"--\", color=\"g\", label=\"Upper bound for 95% CI (bootstrap)\")\n",
    "plt.axvline(x=lower2, linestyle=\"--\", color=\"r\", label=\"Lower bound for 95% CI\")\n",
    "plt.axvline(x=upper2, linestyle=\"--\", color=\"r\", label=\"Upper bound for 95% CI\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by resampling again and again, computing the estimates for the mean, we average around the exact value, with upper and lower boundaries.\n",
    "\n",
    "These approximations were made with bootstraping. The same thing applies while sampling to estimate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap aggregating)\n",
    "\n",
    "Well done ðŸŽ‰, now you understand the algorithm but it's not very usefull, right? Now we are going to apply bagging (aka bootstrap aggregating). \n",
    "\n",
    "As you will discover, it works exactly the same as bootstrapping except instead of calculating several means, we train several models. We will apply this new concept to regression problem and then to a classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Tree Regressor\n",
    "\n",
    "The algorithm to train your models for regression works like this :\n",
    "```\n",
    "# Training\n",
    "models = []\n",
    "for b=1..B:\n",
    "  model = Model()\n",
    "  X_b, Y_b = resample(X)\n",
    "  model.fit(X_b, Y_b)\n",
    "  models.append(model)\n",
    "```\n",
    "\n",
    "And to make predictions :\n",
    "```\n",
    "# regression\n",
    "def predict(X):\n",
    "    return np.mean([model.predict(X) for model in models], axis=1)\n",
    "```\n",
    "\n",
    "**Implement the methods `fit` and `predict` to train your model and make predictions respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class BaggedTreeRegressor:\n",
    "    '''TO DO: implement the fit and predict method applying\n",
    "            the bagging algorithm for a regression problem'''\n",
    "    def __init__(self, B):\n",
    "        self.B = B\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it's done, complete the following code in order to :\n",
    "- plot the data (x_axis, y_axis) vs the predictions made by the **DecisionTreeRegressor (from Scikit-Learn)**\n",
    "- plot the data vs the predictions made by your **BaggedTreeRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def f(x_values):\n",
    "    return np.array([x**2/(x**2 - 2*x + 2) for x in x_values])\n",
    "\n",
    "T = 100\n",
    "x_axis = np.linspace(0,10, T)#np.linspace(0,4*np.pi, T)\n",
    "y_axis = f(x_axis)#np.sin(x_axis)\n",
    "\n",
    "N = 30\n",
    "idx = np.random.choice(T, size=N, replace=False)\n",
    "Xtrain = x_axis[idx].reshape(N, 1)\n",
    "Ytrain = y_axis[idx]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 4)\n",
    "\n",
    "################################\n",
    "# USE THE DECISION TREE MODEL  #\n",
    "################################\n",
    "# Train + predict + plot with the decision tree model\n",
    "\n",
    "\n",
    "##################################\n",
    "# USE  THE BAGGED TREE REGRESSOR #\n",
    "##################################\n",
    "# Train + predict + plot with your \"custom\" bagged tree regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the difference? You can try to change the function f(x): \n",
    "- $f(x) = sin(2*x)$ where x = np.linspace(0, 4*pi, 100)\n",
    "- $f(x) = 1/log(x)$ where x = np.linspace(0.00001, 5, 100)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Tree Classifier\n",
    "\n",
    "Congratulations ! Now, we will apply the same algorithm but in order to **classify** some data. To train your model it works exactly the same as for regression but to make predictions, it's a little bit different. We will use votes for classification problems.\n",
    "\n",
    "For classification between 2 categories (binary):\n",
    "```\n",
    "# classification (Binary)\n",
    "def predict(X):\n",
    "    output = np.zeros(N)\n",
    "    for model in models:\n",
    "        output += model.predict(X)\n",
    "    return np.round(output/B)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_decision_boundary\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "class BaggedTreeClassifier:\n",
    "    '''TO DO: implement the fit and predict method applying the\n",
    "          bagging algorithm for a classification problem'''\n",
    "    def __init__(self, n_estimators, max_depth=None):\n",
    "        self.B = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it's done, complete the following code in order to :\n",
    "- plot the data we provide vs the predictions made by the **DecisionTreeClassifier (from Scikit-Learn)**\n",
    "- plot the data vs the predictions made by your **BaggedTreeClassifier**\n",
    "\n",
    "We implemented for you a function \"plot_decision_boundary(X, model, ax1)\" in the module \"util.py\". You can use it to plot the boundaries between your data : \n",
    "- **X** : your data\n",
    "- **model** : the model you want to use (DecisionTreeClassifier on the first graph and BaggedTreeClassifier on the second)\n",
    "- **ax1** : the subplot on which you want to display the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%run util.py\n",
    "\n",
    "N = 500\n",
    "D = 2 # dimensionality\n",
    "X = np.random.randn(N,D)\n",
    "\n",
    "sep = 2\n",
    "X[:125] += np.array([sep, sep])\n",
    "X[125:250] += np.array([sep, -sep])\n",
    "X[250:375] += np.array([-sep, -sep])\n",
    "X[375:] += np.array([-sep, sep])\n",
    "Y = np.array([0]*125 + [1]*125 + [0]*125 + [1]*125)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 6)\n",
    "################################\n",
    "# USE THE DECISION TREE MODEL  #\n",
    "################################\n",
    "# Train + predict + plot with the decision tree model\n",
    "\n",
    "\n",
    "##################################\n",
    "# USE THE BAGGED TREE CLASSIFIER #\n",
    "##################################\n",
    "# Train + predict + plot with the bagged tree classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a score of 1 for the DecisionTreeClassifier (graph on the left). That means that your model fit perfectly your training data = OVERFITTING. On the right, you should have smoother boundaries (what could be translated as a lower variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
