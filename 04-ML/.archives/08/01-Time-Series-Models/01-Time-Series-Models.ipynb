{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Time Series Models\n",
    "\n",
    "## 01 - Background\n",
    "\n",
    "Our goal in this challenge is to apply the basic concepts of time series analysis on one-dimension data (sales depending on the date).\n",
    "\n",
    "In this challenge, we'll go through the following steps : \n",
    "1. load and visualize the data;\n",
    "2. train our models and make predictions;\n",
    "3. use an econometric approach to model the serie and be able to forecast it;\n",
    "4. use machine learning to hack this modelization.\n",
    "\n",
    "The dataset is loaded from [this url]( https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly_champagne_sales.csv). We provide you the code to read the csv from an url. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Data exploration\n",
    "\n",
    "Start by loading the libraries you need, read the data file and plot some graphs.\n",
    "\n",
    "You could need to install requests: `pip install requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly_champagne_sales.csv\"\n",
    "s=requests.get(url).content\n",
    "df=pd.read_csv(io.StringIO(s.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - Convert datetime to index\n",
    "\n",
    "Now, we should get the index as a datetime : \n",
    "- convert the column \"Date\" to datetime\n",
    "- reset the index of the DataFrame\n",
    "- Set the DataFrame index using the \"Month\" column.\n",
    "- Drop the column \"Month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Visualize and interpet the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, thanks to this \"reindexing\", you should now be able to plot the \"Sales\" (y-axis) as a function of the time (x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : plot the \"Sales\" as a function of the corresponding time and try to interpret the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, you should be able to see that this Time Serie (TS) is:\n",
    "- Not stationnary (mean and variance are not constant).\n",
    "- Exhibits strong saisonality (variations that occur regularly).\n",
    "- Seems to have a trend.\n",
    "\n",
    "Let's see a decomposition of the data between **trend**, **saisonality** and **noise**. In order to do that, you have to make use of the [seasonal_decompose from the statsmodels library]. You can install statsmodels with `pip install statsmodels` (https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html). Read the docs and make sure you understand what this function is doing and how to use it. When you feel more comfortable with it:\n",
    "1. plot the \"Sales\" with an \"additive\" model\n",
    "2. plot the \"Sales\" with a \"multiplicative\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : plot the Sales:\n",
    "# 1. with an additive model\n",
    "# 2. with a multiplicative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What can you interpret from this first analysis ?\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Answer\n",
    "</summary>\n",
    "<p>\n",
    "we can see first the strong seasonality term, that seems to have a coumpound effect. Indeed, the more champagne is sold, the more the seasonality effect can be seen. Also, we can clearly see the drop in champagne sell during 1970-1971.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 - Split the data (train/test)\n",
    "Let's now use a different approach to forecast the data. First we will use an **econometric approach**, and then a more \"Machine Learning one\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define how we will measure the errors of the predictions:\n",
    "- On which data ?\n",
    "- How do we measure the error ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split of the data has to be done \"time-wise\". For the purpose of this exercise, we will use data up to 1972 for training and after for the test.\n",
    "\n",
    "Your task have to : \n",
    "\n",
    "- split the data before 1972 for the training and after (or equal to) 1972 for the testing\n",
    "- check the shape of the training and testing dataset\n",
    "\n",
    "In addition, keep the indexes of the training set in a variable called `train_indexes` and the indexes of the test set in a variable called `test_indexes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: split the data : \n",
    "# df_train : data before 1972\n",
    "# df_test : data after (and equal to) 1972\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "index = df.index\n",
    "train_indexes = np.where(index < pd.datetime.strptime(\"1972\", \"%Y\"))[0]\n",
    "test_indexes = np.where(index >= pd.datetime.strptime(\"1972\", \"%Y\"))[0]\n",
    "df_train = df.iloc[train_indexes]\n",
    "df_test = df.iloc[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: output the corresponding shapes of the train and test datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 - Econometric approach\n",
    "\n",
    "We will analyse the data thanks to [ARIMA models](https://towardsdatascience.com/time-series-forecasting-arima-models-7f221e9eee06) (Auto Regressive Integrated Moving Average).\n",
    "\n",
    "\n",
    "We need to :\n",
    "- find how to stationarize the time serie;\n",
    "- find the moving average;\n",
    "- find the seasonality and auto-regressive part.\n",
    "\n",
    "In this exercise, we will follow these steps:\n",
    "\n",
    "### Step 1: Check stationarity\n",
    "\n",
    "If a time series has a trend or seasonality component, it must be made stationary before we can use ARIMA to forecast.\n",
    "\n",
    "Check the documentation of the Augmented Dick Fuller test and check the stationarity of `df[\"Sales\"]` thanks to [this method](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value should be  less than 0.05 to have a 95% confidence in the stationarity. If the p-value is larger than 0.05, we cannot reject the null hypothesis (null hypothesis = \"the process is not stationary\").\n",
    "\n",
    "Knowing that, what do you conclude for the \"Sales\" column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Difference\n",
    "\n",
    "If the time series is not stationary, it needs to be stationarized through *differencing*. It means that we take the difference between each value and the preceding one (if we take the *first difference*) for instance.\n",
    "\n",
    "Let's start with the first difference. Create a new column `sales_diff` containing the difference between each value and the preceding one.\n",
    "\n",
    "Then, check for stationarity of the stationarized column using again `adfuller`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Select AR and MA terms\n",
    "\n",
    "You will now use the ACF and PACF plots to decide whether to include an AR term(s), MA term(s), or both.\n",
    "\n",
    "In order to fit our ARIMA model we need to find the appropriate differenciation to use. The best way to determine whether or not the series is sufficiently differenced is to plot the differenced series and check to see if there is a constant mean and variance. Take as many differences as it takes. Make sure you check seasonal differencing as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can do the following plots:\n",
    "\n",
    "- The autocorrelation plot applied to the `df[\"Sales\"]` (`plot_acf` from `statsmodels.graphics.tsaplots`).\n",
    "\n",
    "- The partial autocorrelation applied to the `df[\"Sales\"]` (`plot_pacf` also from `statsmodels.graphics.tsaplots`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One goal of these plots is to find good values of the parameters `P` and `Q` needed for the ARIMA models. Try to find these values from the plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Build the model\n",
    "\n",
    "Now that you have chosen the values for `P` and `Q`, build the `arima_model` (from the `statsmodels` library).\n",
    "\n",
    "Then, fit the the model.\n",
    "\n",
    "Store also the parameters of the model (`.params`). You can also print the parameters optimized by the model.\n",
    "\n",
    "Finally, you can use `plot_predict()` to plot the data along with the prediction. Have a look [at the documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARMAResults.plot_predict.html) if you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - fit the appropriate ARIMA model with the values P and Q you determined thanks to the ACF and PACF graphs\n",
    "# - store the params of your model (`.params`)\n",
    "# - plot the predictions vs actual values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Validate model\n",
    "\n",
    "Compare the predicted values to the actuals in the validation sample.\n",
    "\n",
    "We created the following function for you : `evaluate_performance_month_prediction_arima`. This function evaluates the performance per month of your ARIMA model. Use it on your test dataset to check the performance of your model. From the output, extract the RMSE to measure the errors of your predictions. The function takes the data, the indexes of the test samples (`test_indexes`) and the parameters from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance_month_prediction_arima(df, test_indexes, params):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    i = 0\n",
    "    indexes = df.index\n",
    "#     print(\"Going to predict on indexes {}\".format(test_indexes))\n",
    "    for _indx in test_indexes:\n",
    "        train_data = df.iloc[:_indx]\n",
    "        current_ground_truth = df.iloc[_indx]\n",
    "        current_date = indexes[_indx]\n",
    "#         print(\"iteration number {}, indx: {}, date: {}\".format(i, _indx, current_date))\n",
    "        arima = arima_model.ARIMA(order=(1,1,4), endog=train_data)\n",
    "#         print(\"fitting\")\n",
    "        arima_fit = arima.fit(params)\n",
    "#         print(\"fitted, predicting\")\n",
    "        prediction = arima_fit.predict(start=current_date,end=current_date).values[0]\n",
    "#         print(\"predicted: {}\".format(prediction))\n",
    "        predictions.append(prediction)\n",
    "        ground_truth.append(current_ground_truth)\n",
    "        i += 1\n",
    "#     print(\"starting rmse computation\")\n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth, predictions))\n",
    "#     print(\"ALL done\")\n",
    "    return (ground_truth, predictions, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# - make use of the function `evaluate_performance_month_prediction_arima`\n",
    "# - from the output, print the RMSE to evaluate the performance of this model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 - Mix of Econometry and machine learning (SARIMA model)\n",
    "\n",
    "We saw that the previous ARIMA fitting was not bad, but we can do better.\n",
    "\n",
    "Now, try to:\n",
    "\n",
    "- create a new column name \"sales_season\" and assign it 12-month seasonality thanks to the `diff()` function\n",
    "- check the stationary of this new column\n",
    "- just as before, plot the ACF and PACF in order to determine `P` and `Q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the graphs and the test, we can infer that we have an MA(4) AR(1) with seasonality 12. \n",
    "\n",
    "We will use machine learning and a brute force method to find the \"best\" model that fits our training data.\n",
    "\n",
    "We will find the minimal value for the AIC on a grid of possible values: `[0, 1 or 2]` for the parameters `ma`, `ar`, `seasoned_ma` and `seasoned_ar`. You can use multiple nested for loops and train a `SARIMAX` for each combination and store the aic (`.aic`) in a dictionary along with the corresponding parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, find the best combination of parameters: corresponding to the lower AIC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, train again the SARIMAX with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the predictions on training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA models offer also diagnostics to see if our residuals are Gaussian. Something good to check. You can use `plot_diagnostics()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the performance of this model on the data using the following function `evaluate_performance_month_prediction_sarima()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance_month_prediction_sarima(df, test_indexes, params):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    i = 0\n",
    "    indexes = df.index\n",
    "#     print(\"Going to predict on indexes {}\".format(test_indexes))\n",
    "    for _indx in test_indexes:\n",
    "        train_data = df.iloc[:_indx]\n",
    "        current_ground_truth = df.iloc[_indx]\n",
    "        current_date = indexes[_indx]\n",
    "#         print(\"iteration number {}, indx: {}, date: {}\".format(i, _indx, current_date))\n",
    "        sarima_model = SARIMAX(df_train[\"Sales\"], order=(_ar_val, 0, _ma_val), seasonal_order=(_seas_ar_val, 0, _seas_ma_val, 12), enforce_invertibility=False, enforce_stationarity=False)\n",
    "#         print(\"fitting\")\n",
    "        sarima_model_fit = sarima_model.fit(params)\n",
    "#         print(\"fitted, predicting\")\n",
    "        prediction = sarima_model_fit.predict(start=current_date,end=current_date).values[0]\n",
    "#         print(\"predicted: {}\".format(prediction))\n",
    "        predictions.append(prediction)\n",
    "        ground_truth.append(current_ground_truth)\n",
    "        i += 1\n",
    "#     print(\"starting rmse computation\")\n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth, predictions))\n",
    "#     print(\"ALL done\")\n",
    "    return (ground_truth, predictions, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a lower RMSE in comparison to the use of ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 - Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit a non linear model such as a random forest. The idea is to predict a value from the last ones. You will try to create new columns in `df` that are shifted version of `df['Sales']`. Do it with a shift from 1 to 12.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random forest algorithm and use only the shifted version of the time series that you just created. You can use the following function to test it. It takes the true y values (`data`), the indexes of the test samples (`test_indexes`), the predictor (`predictor`: your random forest algorithm) and the shifted columns (`full_X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each predictor, design a method to evaluate its performance on the test set:\n",
    "def evaluate_performance_month_prediction_lm_rf(data, test_indexes, predictor, full_X):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    i = 0\n",
    "    for _indx in test_indexes:\n",
    "        train_data = data[:_indx]\n",
    "        current_ground_truth = data[_indx]\n",
    "        current_ground_truth_features = full_X[_indx,:]\n",
    "        train_features = full_X[:_indx]\n",
    "        predictor.fit(train_features, train_data)\n",
    "        prediction = predictor.predict(current_ground_truth_features.reshape(1,-1))[0]\n",
    "        predictions.append(prediction)\n",
    "        ground_truth.append(current_ground_truth)\n",
    "        i += 1\n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth, predictions))\n",
    "    return ground_truth, predictions, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve our model, we can do some feature engineering. You will add moving averages to the data used to train the random forest.\n",
    "\n",
    "Try to create 3 new columnns in `df`: one which is the rolling average of `df[Sales]` with a window of 12, one with a window of 3, and one with a window of 2. This will have the effect to isolate the trend and allow the algorithm to learn it. Also, plot these data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a more smoothing predictor using the exponential moving average (hint: method `.ewm` with `halflife` of 2, 3 and 12), that statistically optimizes an AR process. Plot also the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a better RMSE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to look at the importance of each feature. What do you find?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
