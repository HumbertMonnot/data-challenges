{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First RNN\n",
    "\n",
    "**Exercise objectives**:\n",
    "\n",
    "- Better understand temporal data\n",
    "- Build your first Recurrent Neural Network\n",
    "\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data\n",
    "\n",
    "Let's start with simple sequences of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10, 3)\n",
      "[[  0.803     10.728925 102.      ]\n",
      " [  0.5       10.728925 102.      ]\n",
      " [  0.603     10.728925 102.      ]\n",
      " [  0.711     10.728925 130.      ]\n",
      " [  0.937     10.728925 130.      ]\n",
      " [  1.051     10.728925 130.      ]\n",
      " [  1.118     32.       108.      ]\n",
      " [  4.87      32.       108.      ]\n",
      " [  4.973     33.       108.      ]\n",
      " [  5.199     39.       108.      ]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "6.499\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ The data describes the evolution of the **employment status of a person, year after year**: each sequence corresponds to 10 consecutive years, where each year describes a job situation, comprising of 3 components\n",
    "- the salary,\n",
    "- the number of persons under one's responsability,\n",
    "- the size of the company. \n",
    "\n",
    "So, from this 25000 sequences, each of 10 consecutive observations, the goal is to predict the salary on the 11th year based on the past observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Take some sequences and plot the evolution of their salaries, of the persons under their responsibility and of the company sizes. You might see some correlation between the three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1010001e+00, 6.6484495e-06, 5.0000000e+01],\n",
       "       [2.1530001e+00, 6.6484495e-06, 5.0000000e+01],\n",
       "       [2.2090001e+00, 6.6484495e-06, 5.0000000e+01],\n",
       "       [2.2990000e+00, 6.6484495e-06, 5.0000000e+01],\n",
       "       [2.3950000e+00, 6.6484495e-06, 6.6000000e+01],\n",
       "       [2.5420001e+00, 6.6484495e-06, 1.0000000e+00],\n",
       "       [2.6610000e+00, 0.0000000e+00, 1.0000000e+00],\n",
       "       [2.7200000e+00, 0.0000000e+00, 1.0000000e+00],\n",
       "       [2.8350000e+00, 0.0000000e+00, 1.0000000e+00],\n",
       "       [2.9460001e+00, 0.0000000e+00, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = 2\n",
    "\n",
    "for k in range(3):\n",
    "    plt.plot(X[obs][:,k])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Plot the distribution of all the salaries, persons under one's responsibility, and company sizes to get a better understanding of the variability of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.1494e+04, 1.9310e+03, 7.7400e+02, 3.8700e+02, 2.0900e+02,\n",
       "         9.3000e+01, 6.9000e+01, 2.4000e+01, 7.0000e+00, 8.0000e+00,\n",
       "         3.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.0805e+04, 2.2670e+03, 9.5700e+02, 4.6100e+02, 2.3700e+02,\n",
       "         1.0700e+02, 7.7000e+01, 3.6000e+01, 2.2000e+01, 1.4000e+01,\n",
       "         9.0000e+00, 4.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.9846e+04, 2.7120e+03, 1.1670e+03, 5.6900e+02, 3.2400e+02,\n",
       "         1.5500e+02, 9.0000e+01, 4.9000e+01, 3.2000e+01, 1.9000e+01,\n",
       "         1.3000e+01, 1.0000e+01, 6.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "         2.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.8902e+04, 3.0050e+03, 1.4010e+03, 7.2700e+02, 3.9600e+02,\n",
       "         2.2500e+02, 1.3000e+02, 7.0000e+01, 5.4000e+01, 3.0000e+01,\n",
       "         2.6000e+01, 1.4000e+01, 7.0000e+00, 4.0000e+00, 3.0000e+00,\n",
       "         4.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.8005e+04, 3.2700e+03, 1.5480e+03, 9.0600e+02, 5.1300e+02,\n",
       "         2.8000e+02, 1.8700e+02, 9.9000e+01, 6.5000e+01, 4.8000e+01,\n",
       "         3.3000e+01, 1.3000e+01, 1.2000e+01, 8.0000e+00, 3.0000e+00,\n",
       "         7.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.7185e+04, 3.4330e+03, 1.7400e+03, 1.0300e+03, 6.0700e+02,\n",
       "         3.6600e+02, 2.3500e+02, 1.4300e+02, 8.5000e+01, 5.9000e+01,\n",
       "         5.0000e+01, 1.8000e+01, 1.5000e+01, 1.5000e+01, 5.0000e+00,\n",
       "         7.0000e+00, 3.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.6463e+04, 3.5320e+03, 1.9050e+03, 1.1560e+03, 6.9700e+02,\n",
       "         4.4500e+02, 2.8600e+02, 1.8300e+02, 9.9000e+01, 7.6000e+01,\n",
       "         6.2000e+01, 2.9000e+01, 2.1000e+01, 2.3000e+01, 7.0000e+00,\n",
       "         6.0000e+00, 5.0000e+00, 3.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.5784e+04, 3.6350e+03, 2.0500e+03, 1.2520e+03, 7.8500e+02,\n",
       "         5.0800e+02, 3.2800e+02, 2.2800e+02, 1.4500e+02, 9.4000e+01,\n",
       "         6.9000e+01, 3.7000e+01, 3.2000e+01, 2.4000e+01, 1.0000e+01,\n",
       "         7.0000e+00, 6.0000e+00, 3.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "        [1.5143e+04, 3.7050e+03, 2.2010e+03, 1.3530e+03, 8.7900e+02,\n",
       "         5.7200e+02, 3.6600e+02, 2.5900e+02, 1.8100e+02, 1.2200e+02,\n",
       "         7.1000e+01, 5.0000e+01, 3.8000e+01, 2.6000e+01, 1.3000e+01,\n",
       "         7.0000e+00, 7.0000e+00, 3.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "         1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "        [1.4623e+04, 3.6880e+03, 2.2900e+03, 1.4260e+03, 1.0050e+03,\n",
       "         6.4200e+02, 4.0800e+02, 3.0500e+02, 2.0400e+02, 1.4500e+02,\n",
       "         8.0000e+01, 5.9000e+01, 4.7000e+01, 3.0000e+01, 2.4000e+01,\n",
       "         7.0000e+00, 9.0000e+00, 3.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00]]),\n",
       " array([  0.  ,  25.84,  51.68,  77.52, 103.36, 129.2 , 155.04, 180.88,\n",
       "        206.72, 232.56, 258.4 , 284.24, 310.08, 335.92, 361.76, 387.6 ,\n",
       "        413.44, 439.28, 465.12, 490.96, 516.8 , 542.64, 568.48, 594.32,\n",
       "        620.16, 646.  ], dtype=float32),\n",
       " <a list of 10 BarContainer objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X[:,:,1], bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓  Split your dataset between a train and test set (20/80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = X[:round(0.2*len(X))]\n",
    "X_test = X[round(0.2*len(X)):]\n",
    "y_train = y[:round(0.2*len(y))]\n",
    "y_test = y[round(0.2*len(y)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A Simple RNN\n",
    "\n",
    "Now, you will create your first Recurrent Neural Network. Let's start simple\n",
    "\n",
    "❓ **Question** ❓ Write a model that has: \n",
    "- a `Normalization` layer adapted on the training set\n",
    "- a `SimpleRNN` layer with 20 `units` - don't forget to choose the `tanh` activation function\n",
    "- a Dense layer with 10 neurons\n",
    "- a last Dense layer specific to your task (predict a salary)\n",
    "\n",
    "Remember, you don't need to specify an `input_shape`: as soon as your normalizer has been adapted to your train set, it has memorized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Flatten, Normalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Normalization())\n",
    "model.add(SimpleRNN(units=20, activation = 'tanh'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yf/53d25rm10gq_l7wdtlp_205w0000gn/T/ipykernel_57882/3470139634.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2450\u001b[0m     \"\"\"\n\u001b[1;32m   2451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2452\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2453\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Compute the number of trainable parameters of your model using `model.summary`. \n",
    "\n",
    "Then, try to recompute them by hand\n",
    "\n",
    "💡 Hints: \n",
    "- Your `normalization` layer has 7 non-trainable params, which comprise of the mean and standard deviation of each 3 features (salary, persons under responsibility, company size) plus one bias set to 0\n",
    "\n",
    "- If $\\color{green}{n_h}$ RNN units are applied in parallel to $\\color{red}{n_x}$ features, the layer has $\\color{green}{n_h}(\\color{green}{n_h} + \\color{red}{n_x} + 1)$ parameters to train. Notice how this number is *independent* of the length of each sequences (here 10 days)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Compile your model. Remember to first use the `rmsprop` optimizer (instead of Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Run your model on your data. Use a validation split of 20% and an early stopping criterion (patience=5). Evaluate your performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "110/110 [==============================] - 2s 5ms/step - loss: 5.4666 - val_loss: 4.5304\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 4.3692 - val_loss: 2.8342\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 2.8747 - val_loss: 2.2741\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 2.3013 - val_loss: 1.9088\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 2.0381 - val_loss: 1.6555\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.6138 - val_loss: 1.4755\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.4811 - val_loss: 1.2856\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.2599 - val_loss: 1.1451\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.3384 - val_loss: 1.0754\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.1826 - val_loss: 1.1039\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 1.1119 - val_loss: 1.1052\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.9868 - val_loss: 0.9747\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.9512 - val_loss: 0.9270\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.9143 - val_loss: 0.8679\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.8480 - val_loss: 0.9794\n",
      "Epoch 16/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7899 - val_loss: 0.8082\n",
      "Epoch 17/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.8396 - val_loss: 0.8714\n",
      "Epoch 18/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7971 - val_loss: 0.7500\n",
      "Epoch 19/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7976 - val_loss: 0.7019\n",
      "Epoch 20/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7568 - val_loss: 0.6639\n",
      "Epoch 21/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7723 - val_loss: 0.7501\n",
      "Epoch 22/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7275 - val_loss: 0.6200\n",
      "Epoch 23/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7274 - val_loss: 0.6400\n",
      "Epoch 24/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.7129 - val_loss: 0.7110\n",
      "Epoch 25/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6653 - val_loss: 0.6530\n",
      "Epoch 26/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6716 - val_loss: 0.5430\n",
      "Epoch 27/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6076 - val_loss: 0.8987\n",
      "Epoch 28/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.5067\n",
      "Epoch 29/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5048\n",
      "Epoch 30/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6626 - val_loss: 0.5483\n",
      "Epoch 31/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.7696\n",
      "Epoch 32/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6335 - val_loss: 0.4967\n",
      "Epoch 33/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6446 - val_loss: 0.6376\n",
      "Epoch 34/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5279\n",
      "Epoch 35/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.6771 - val_loss: 0.6376\n",
      "Epoch 36/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.5222\n",
      "Epoch 37/150\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.5916 - val_loss: 0.6310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153481130>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience = 5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 150, validation_split=0.3, callbacks = [es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline comparison?\n",
    "\n",
    "\n",
    "In the case of a usual regression model, a baseline prediction for `y_test` could be to predict the average of `y_train`.\n",
    "\n",
    "With temporal data, it often happens that you try to predict a value that you have already seen in the past: here, the salary. In that case, a baseline model could be to predict for instance that the 11-th salary is equal to the 10-th salary.\n",
    "\n",
    "❓ **Question** ❓ Compute the Mean Absolute Error of a model that would predict that the salary remains constant between the 10-th and 11-th year and compare it to your RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_test[:,9,0]\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = abs(y_pred-y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5973297493781329"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 883us/step - loss: 0.5555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.555487871170044"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have probably seen that your prediction is a little bit better than the baseline model\n",
    "\n",
    "# LSTM\n",
    "\n",
    "❓ **Question** ❓ Write the exact same model, but with a `LSTM` instead of a `SimpleRNN` and evaluate your performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "110/110 [==============================] - 3s 7ms/step - loss: 4.1697 - val_loss: 2.4679\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 2.5530 - val_loss: 2.0754\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 2.0839 - val_loss: 1.5845\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.5972 - val_loss: 1.1979\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.2449 - val_loss: 1.1210\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 1.0439 - val_loss: 0.9249\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.8821 - val_loss: 0.7628\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.9107 - val_loss: 0.7823\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.8340 - val_loss: 0.7318\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.7507 - val_loss: 0.9343\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.7635 - val_loss: 0.7900\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6964 - val_loss: 0.7401\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6801 - val_loss: 0.6998\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6872 - val_loss: 0.7222\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6700 - val_loss: 0.5608\n",
      "Epoch 16/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6719 - val_loss: 0.7953\n",
      "Epoch 17/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6806 - val_loss: 0.5446\n",
      "Epoch 18/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6605 - val_loss: 0.5113\n",
      "Epoch 19/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6102 - val_loss: 0.7150\n",
      "Epoch 20/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6141 - val_loss: 0.4669\n",
      "Epoch 21/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6232 - val_loss: 0.7430\n",
      "Epoch 22/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.5931 - val_loss: 0.5438\n",
      "Epoch 23/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.5601 - val_loss: 0.6157\n",
      "Epoch 24/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.5124 - val_loss: 0.5758\n",
      "Epoch 25/150\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.6132 - val_loss: 0.6521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1537c1850>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Normalization())\n",
    "model2.add(LSTM(units=20, activation = 'tanh'))\n",
    "model2.add(Dense(10, activation = 'relu'))\n",
    "model2.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model2.compile(optimizer = 'rmsprop',\n",
    "              loss = 'mae')\n",
    "\n",
    "model2.fit(X_train, y_train, epochs = 150, validation_split=0.3, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.5365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5364834666252136"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏁 Congratulation\n",
    "\n",
    "You now know how to run RNN on sequence data!\n",
    "\n",
    "Note: The sequences you worked with are totally fake. In case you need to train and reproduce similar data, you can find bellow the functions that have been used to simulate this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils (for reference only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(number):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(number):\n",
    "        x_i, y_i = create_individual_sequence(10)\n",
    "        X.append(x_i)\n",
    "        y.append(y_i)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "            \n",
    "def create_individual_sequence(length):\n",
    "    company_sizes = []\n",
    "    nb_persons = []\n",
    "    salaries = []\n",
    "    \n",
    "    \n",
    "    # Education level\n",
    "    educ_level = [max(0, int(np.random.normal(10, 2)))]*length\n",
    "    \n",
    "    # Company size\n",
    "    current_size = int(1 + np.random.beta(.4, 4)*500)\n",
    "    for i in range(length):\n",
    "        if not np.random.randint(4): # Change 1 out of 3 possibilities \n",
    "            current_size = int(max(1, np.random.normal(current_size, 50)))\n",
    "        company_sizes.append(current_size)\n",
    "    \n",
    "    # Number of persons\n",
    "    nb_iter = np.random.beta(.15, 4)*300\n",
    "    for i in range(length):\n",
    "        if not np.random.randint(2): # Change 1 out of 2 possibilities\n",
    "            R_1 = np.random.beta(0.5, 8)*3\n",
    "            nb_iter = nb_iter + max(-2, R_1*company_sizes[i] + np.random.randint(-2, 2))\n",
    "            nb_iter = max(0, nb_iter)\n",
    "            nb_iter = int(min(company_sizes[i]-1, nb_iter))\n",
    "        nb_persons.append(nb_iter)\n",
    "        \n",
    "    \n",
    "    # Salary\n",
    "    salary_iter = max(800, int(np.random.normal(1200, 300)+ 0.05*company_sizes[0] +  np.random.normal(40, 400)))\n",
    "    salaries.append(salary_iter)\n",
    "    for i in range(1, length + 1):\n",
    "        R_1 = np.random.normal(100, 50)\n",
    "        change_person = nb_persons[i-1] - nb_persons[i-2]\n",
    "        change_company = max(0, company_sizes[i-1] - company_sizes[i-2])\n",
    "        salary_iter = salary_iter + 0.05*change_company + change_person*R_1 + np.random.normal(100, 50)\n",
    "        salary_iter = max(int(salary_iter), 500)\n",
    "        \n",
    "        salaries.append(salary_iter)\n",
    "\n",
    "    y = salaries[-1]/1000\n",
    "    salaries = [_/1000 for _ in salaries[:-1]]\n",
    "    \n",
    "    return np.array([salaries, nb_persons, company_sizes]).T, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = create_sequences(25000)\n",
    "\n",
    "#np.save('X', X.astype(np.float32))\n",
    "#np.save('y', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
