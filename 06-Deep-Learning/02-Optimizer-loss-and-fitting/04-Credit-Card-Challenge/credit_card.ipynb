{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0060abc6",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c584e",
   "metadata": {},
   "source": [
    "All the previous exercises made you take a closer look at all the different parts of a neural network: \n",
    "* the architecture of a sequential Dense Neural Network, \n",
    "* the compilation method\n",
    "* the fitting.\n",
    "\n",
    "Let's now work on a real-life dataset that has **a lot of data**!\n",
    "\n",
    "üí∏ **The dataset : `Credit Card Transactions`** üí∏\n",
    "\n",
    "For this open challenge, you will `work with data extracted from credit card transactions`. \n",
    "\n",
    "As these are `sensitive data`, from all the 31 columns, only 3 columns are known: the rest are data that have been transformed to `anonymize` them (in fact, they are `PCA projections of initial data`).\n",
    "\n",
    "The other three known columns are:\n",
    "\n",
    "* `TIME`: the time elapsed between the transaction and the first transaction in the dataset\n",
    "* `AMOUNT`: the amount of the transaction\n",
    "* `CLASS` (our target): \n",
    "    * `0 : valid transaction` \n",
    "    * `1 : fraudulent transaction`\n",
    "\n",
    "‚ùì **Question** ‚ùì Start by downloading the dataset:\n",
    "* on the Kaggle website [here](https://www.kaggle.com/mlg-ulb/creditcardfraud) \n",
    "* or from our [URL](https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/creditcard.csv) \n",
    "\n",
    "Load data to create `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf23eca",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfc3e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f676034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=\"Class\")\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30cf70",
   "metadata": {},
   "source": [
    "## 1. Rebalancing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe9b1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check class balance\n",
    "pd.Series(y).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1d4a3",
   "metadata": {},
   "source": [
    "‚òùÔ∏è in this `fraud detection` challenge, **the classes are extremely imbalanced**:\n",
    "* 99.8 % of normal transactions\n",
    "* 0.2 % of fraudulent transactions\n",
    "\n",
    "**We won't be able to detect frauds unless we apply some serious rebalancing strategies!**\n",
    "\n",
    "‚ùì **Question** ‚ùì\n",
    "1. **First**, create three separate splits `Train/Val/Test` from your dataset. It is extremely important to keep validation and testing sets **unbalanced** so that when you evaluate your model, it is done in true conditions, without data leakage. Keep your test set for the very last cell of this notebook... !\n",
    "\n",
    "&nbsp;\n",
    "2. **Second**, rebalance you training set (and only this one). You have many choices:\n",
    "\n",
    "- Simply oversample the minority class randomly using plain Numpy functions (not the best option since you are duplicating rows and hence creating data leakage)\n",
    "- Or use <a href=\"https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\">**`Synthetic Minority Oversampling Technique - SMOTE`**</a> to generate new datapoints by weighting the existing ones\n",
    "- In addition, you can also try a <a href=\"https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\">**`RandomUnderSampler`**</a> to downsample the majority class a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "accdcd48",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train,  y_test = train_test_split(X, y, train_size=0.6)\n",
    "X_train, X_val, y_train,  y_val = train_test_split(X_train, y_train, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34db6e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85442, 85442, 113923)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c697e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a79e6669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.9.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88f99ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_train_r, y_train_r = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "331fb7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((170618, 30), (85442, 30))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_r.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d2c729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_recall(history, title=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    \n",
    "    # --- LOSS --- \n",
    "    \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('Model loss')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylim((0,3))\n",
    "    ax[0].legend(['Train', 'Test'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- ACCURACY\n",
    "    \n",
    "    ax[1].plot(history.history['recall'])\n",
    "    ax[1].plot(history.history['val_recall'])\n",
    "    ax[1].set_title('Model recall')\n",
    "    ax[1].set_ylabel('recall')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='best')\n",
    "    ax[1].set_ylim((0,1))\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1c865",
   "metadata": {},
   "source": [
    "## 2. Neural Network iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575fc40",
   "metadata": {},
   "source": [
    "Now that you have rebalanced your classes, try to fit a neural network to optimize your test score. Feel free to use the following hints:\n",
    "\n",
    "- Normalize your inputs!\n",
    "    - Use preferably a [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) layer inside the model to \"pipeline\" your preprocessing within your model. \n",
    "    - Or use sklearn's [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) outside of your model, applied your `X_train` and `X_val` and `X_test`.\n",
    "- Make your model overfit, then, regularize  it using:\n",
    "    - Early Stopping criteria \n",
    "    - [`Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers\n",
    "    - or [`regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) layers\n",
    "- üö® Think carefully about the metrics you want to track and the loss function you want to use... !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e0c6f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85442, 30)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "02eb1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa175982",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalization() # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "be913e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience = 6, restore_best_weights=True, monitor=\"val_recall\", mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5b9c9673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x15427ae20>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_learning_rate = 0.1 # start with default Adam value\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    # Every 5000 iterations, multiply the learning rate by 0.7\n",
    "    initial_learning_rate, decay_steps = 4000, decay_rate = 0.7)\n",
    "    \n",
    "opt_schedule = Adam(learning_rate=lr_schedule)\n",
    "opt_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4e9ac4ff",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def initialize_model(lr):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    #reg_l1 = regularizers.L1(0.01)\n",
    "    #reg_l1_l2 = regularizers.l1_l2(l1=0.005, l2=0.005)\n",
    "    model.add(normalizer)\n",
    "    model.add(layers.Dense(15, activation='relu', input_dim=30))\n",
    "    model.add(layers.Dropout(rate=0.3))\n",
    "    model.add(layers.Dense(10, activation='relu', input_dim=30))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=lr),\n",
    "                  metrics=Recall(\n",
    "    thresholds=None, top_k=None, class_id=None, name='recall', dtype=None\n",
    "))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b417c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5332/5332 [==============================] - 5s 892us/step - loss: 0.1044 - recall: 0.9729 - val_loss: 0.0803 - val_recall: 0.8968\n",
      "Epoch 2/1000\n",
      "5332/5332 [==============================] - 5s 989us/step - loss: 0.0852 - recall: 0.9804 - val_loss: 0.0491 - val_recall: 0.8968\n",
      "Epoch 3/1000\n",
      "5332/5332 [==============================] - 5s 945us/step - loss: 0.0770 - recall: 0.9827 - val_loss: 0.0566 - val_recall: 0.8839\n",
      "Epoch 4/1000\n",
      "5332/5332 [==============================] - 5s 919us/step - loss: 0.0771 - recall: 0.9844 - val_loss: 0.1154 - val_recall: 0.8645\n",
      "Epoch 5/1000\n",
      "5332/5332 [==============================] - 5s 910us/step - loss: 0.0715 - recall: 0.9839 - val_loss: 0.0654 - val_recall: 0.8903\n",
      "Epoch 6/1000\n",
      "5332/5332 [==============================] - 5s 907us/step - loss: 0.0683 - recall: 0.9847 - val_loss: 0.1205 - val_recall: 0.8903\n",
      "Epoch 7/1000\n",
      "5332/5332 [==============================] - 5s 944us/step - loss: 0.0785 - recall: 0.9870 - val_loss: 0.1023 - val_recall: 0.8839\n",
      "2671/2671 [==============================] - 1s 518us/step - loss: 0.0803 - recall: 0.8968\n",
      "[0.896774172782898]\n",
      "CPU times: user 51.8 s, sys: 9.97 s, total: 1min 1s\n",
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results =[]\n",
    "\n",
    "model = initialize_model(0.05)\n",
    "history = model.fit(X_train_r, y_train_r, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[es], epochs = 1000, verbose = 1)\n",
    "results.append(model.evaluate(X_val, y_val)[1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9ddb4",
   "metadata": {},
   "source": [
    "By optimizing on the recall, we are \"sacrificing\" the precision!\n",
    "\n",
    "üéØ As a bank manager, you want all the frauds to be detected.\n",
    "\n",
    "‚úÖ It's fine to predict False Positives, False Alarms: `Better be safe than sorry...`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72e581",
   "metadata": {},
   "source": [
    "## 3. Score your model on the unseen Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a8c3f",
   "metadata": {},
   "source": [
    "‚ùì **Questions** ‚ùì: \n",
    "\n",
    "* Evaluate your model on the test \n",
    "* Print the Confusion Matrix\n",
    "* What are your `precision` and `recall` on the test set ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "60f1c5e4",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3561/3561 [==============================] - 2s 503us/step - loss: 0.0978 - recall_37: 0.8186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09781114012002945, 0.8186274766921997]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "864729ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111165,   2554],\n",
       "       [    20,    184]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, [0 if k <0.5 else 1 for k in model.predict(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "955abc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 167 / (70 + 167)\n",
    "recall = 167 / (37 + 167)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f105ed0",
   "metadata": {},
   "source": [
    "### üß™ Test your score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee8987",
   "metadata": {},
   "source": [
    "Store below your real test performance on a (`X_test`, `y_test`) representative sample of the original unbalanced dataset into `precision` and `recall` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256f166",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dbe064d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /Users/humbert/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/humbert/code/HumbertMonnot/data-challenges/06-Deep-Learning/02-Optimizer-loss-and-fitting/04-Credit-Card-Challenge\n",
      "plugins: anyio-3.4.0, dash-2.0.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/test_solution.py::TestSolution::test_is_score_good_enough \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "tests/test_solution.py::TestSolution::test_is_test_set_representative \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/solution.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed solution step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('solution',\n",
    "    precision=precision,\n",
    "    recall=recall,\n",
    "    fraud_number=len(y_test[y_test == 1]),\n",
    "    non_fraud_number=len(y_test[y_test == 0]),\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04b243",
   "metadata": {},
   "source": [
    "## üèÅ Optional : Read Google's solution for this challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e09b5e",
   "metadata": {},
   "source": [
    "Congratulations for finishing all the challenges for this session!\n",
    "\n",
    "To conclude, take some time to read Google's own solution direcly [on Colab here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb). \n",
    "\n",
    "You will discover interesting techniques and best practices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
