{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 2 - Playground Part II\n",
    "\n",
    "### objective:\n",
    "Better understand Neural Network hyperparameters\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "Open again the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03295&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&regularizationRate_hide=true) to learn about Neural Network. \n",
    "\n",
    "Keep in mind that as the algorithm is stochastic, the results might differ from one run to the other. For that reason, do not hesitate to rerun the algorithms multiple times to be sure of your deduction and reasonings.\n",
    "\n",
    "### Let's explore the different things we have seen  during the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The batch size\n",
    "\n",
    "‚ùì **Question** ‚ùì Select the circle dataset (classification). Build a model with one hidden layer with 3 neurons, a learning rate of 0.03 and the tanh activation function. Do not put any noise (=0).\n",
    "\n",
    "Now, select a batch size of 30 and look at the convergence of the algorithm. Does it seem slow or fast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, run the same neural network on the same data but with a batch-size of 1? Be sure to run at least 150 epochs. What do you notice on the train and test loss? What is the reason of this instability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now, you can see the effect by reading the values of the train and test loss: pause the iterations and run it step by step (iteration per iteration) thanks to the \"Step\" button (at the right side of the play/stop button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì To observe once again the **lack of generalization** select the \"Exclusive or\" dataset, with a noise of 50. Keep add a second hidden layer with again 8 neurons. Try to fit your model with different batch_size of 20 and then 1: it should overfit in all cases, although small_batch will overfit faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Can we **regularize** your network to avoid overfitting? Keep batch size to 1, and add L2 regularization. Increase it until it smooth-out the prediction! Notice how the test loss doesn't increase anymore with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Change now to the spiral dataset, remove regularization, and increase ratio of training to test data of 80%. For the neural network, use 3 hidden layers with 8 neurons on the first layer, 7 on the second and 6 on the third. Run the algorithm with a batch size of 30. Be sure to run it for at least 1500 epochs. Then, compare it to the same run but with a batch size of 1. You can check what happens on the train and test loss step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The learning rate\n",
    "\n",
    "Go back to the circle dataset with no noise and a ratio of training to test data of 50%. Use a batch size of 20. \n",
    "\n",
    "Use a neural netwok with one layer of 5 neurons (no regularization, and the tanh activation function)\n",
    "\n",
    "‚ùì **Question** ‚ùì For each learning rate (from 0.00001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "‚ö†Ô∏è Warning ‚ö†Ô∏è When you change the learning rate, be sure to reinitialize the neural network (circular arrow, left to the play/pause button)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [0.00001, 0.0001, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "test_loss = [### YOUR LOSS VALUES]\n",
    "\n",
    "\n",
    "plt.plot(np.log(learning_rates), test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning ‚ö†Ô∏è It is important to understand that, even though small and high learning rates have high test loss, this is absolutely not for the same reason. On the one hand, small learning rates do converge as middle learning rate, but way slower. If you would like, you can try to run it all night long to see that it finally converge to the same test loss but with much more epochs. On the other hand, large learning rates make the algorithm diverge.\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì To convince yourself of the lack of convergence of high learning rates, select a learning equal to 10 and run it 10 times, each time with 400 epochs. Report the values in the following list to see the variability of values, which corresponds to the fact that the algorithms converge to different local minima (which is also to be seen on the final prediction which is each time different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = [##YOUR LOSS VALUES]\n",
    "\n",
    "plt.boxplot(test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì You can play around with different dataset and make the noise vary. Look at the effect of different learning rate. You can especially change the learning rate _during_ the learning to see how it affects the train and test losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulation! Don't forget to commit and push your notebook** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
