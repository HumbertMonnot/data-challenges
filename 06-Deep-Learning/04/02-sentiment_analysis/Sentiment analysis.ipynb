{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is the task of classifying sentences according to a subjective notion or an affective state. In this notebook, we will perform such an analysis on movie reviews from the imdb database. This database contains thousands of movie reviews and the associated sentiment (positive or negative). We will train differend models to classify the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is an example code to load the data. We obtain list of arrays of integers. Each integer represents a word (lowest integers correspond to most frequent words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "vocab_size = 10000 # maximum number of words to keep (keeps the most frequent)\n",
    "maxlen = 200 # maximum number of words in a review (otherwise the review is truncated)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=vocab_size,\n",
    "                                                      seed=0,\n",
    "                                                      index_from=3,\n",
    "                                                      maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are some methods to recover the actual text from the word numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "def word_from_id(i):\n",
    "    return id_to_word[i]\n",
    "\n",
    "def words_from_id_list(l):\n",
    "    return ' '.join(word_from_id(i) for i in l)\n",
    "\n",
    "print(len(id_to_word))\n",
    "words_from_id_list(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 1D Convolution.\n",
    "\n",
    "In this part, we will consider that the sentences all have the same lengths. This can be achieved using `sequence.pad_sequences(corpus, maxlen=maxlen)` where sequence is a module in `tensorflow.keras.preprocessing`. This method pads the sentences at their beginning using a specific padding word id.\n",
    "\n",
    "#### Pad the test and train sentences to the right up to `maxlen=200` words. Use the `pad_sequences` method. Check the shapes of the padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a model which takes a sentence as input (i.e. 200 word ids) and performs an embedding, a conv1d and a sigmoid operation to classify. Check the documentation for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_cnn_model():\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile your model. Compile it with the appropriate loss and optimizer. Can you think of a metric to monitor as well ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit your model, use a stopping criterion on some validation data splitted from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a new model, this time containing an embedding, an LSTM and a Dense layer, to perform the same classification task. Then, train it and evaluate it as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "def build_lstm_model():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
