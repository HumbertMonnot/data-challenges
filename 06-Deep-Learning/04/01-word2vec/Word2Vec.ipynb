{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim is the package to use for word2vec word embeddings\n",
    "# don't try to code word2vec yourself !\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding with word2vec\n",
    "\n",
    "In this notebook, we will look at word2vec, a technique for word embedding.\n",
    "\n",
    "The goal of word2vec is to provide a mapping from the dictionary of words present in a text to vectors of fixed dimension. The main motivation is that working on words directly is not possible (except with one-hot encoding in really large dimensions which do not behave well).\n",
    "\n",
    "There are two ways to train a word2vec model. The first is to use continuous bag of words (CBOW). In this CBOW setting, the text is preprocessed to build:\n",
    "- small sets (e.g. size 5) of neighborhing words in the text\n",
    "- one other neighboring word in the text.\n",
    "and it trains a neural network to predict this neighboring word from the other close words.\n",
    "Example: text = 'the cat climbed the tree'\n",
    "The bag of words and other word could be:\n",
    "- 'the', 'climbed' -> cat\n",
    "- 'cat', 'the' -> climbed\n",
    "- 'climbed', 'tree' -> 'the'\n",
    "\n",
    "To do so, the network learns en embedding i.e. a function which maps each word to a continuous vector in dimension 64. Then there is one dense layer and the network outputs a softmax on the number of words.\n",
    "\n",
    "The second way is skip-gram, where this time, given a single word for context, the network attempts to find several words that are likely to occur next to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we load the data: we will work on imdb database (if you have a dataset of interest, try it ! word2vec is extremely robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), _ = imdb.load_data()\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is x_train ? What is y_train ? In your opinion, why doesn't it look like text ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the text already tokenized i.e. the words have been encoded in integers. This is the right **input** format for an embedding layer. Now in our case, we will feed word2vec with texts, so we need to go back to texts !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is id_to_word dictionary ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives correspondence between words and integers. Some extra words exist, we will see how they are useful in the next exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list `sentences_train` which contains the list of strings corresponding to x_train. Similarly, create a list `sentences_test`.\n",
    "Print some elements to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec object from gensim cannot be fed with `sentences` object directly, but we need to define an iterator abstraction for it, as done below. Iterate through this object. Do you understand what `yield` does ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in sentences:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line\n",
    "            \n",
    "corpus = MyCorpus()\n",
    "\n",
    "# iterate !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we run the word2vec model on our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "model = gensim.models.Word2Vec(sentences=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Below we print the vocabulary i.e. the whole set of words present in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model.wv object has a `most_similar` method which, given a word in the vocabulary, gives you the most similar words according to the computed embedding. Try it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is an example of arithmetic on the word embeddings (possible because all activations in word2vec are linear). Add yours !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.similar_by_vector(model.wv['king'] - model.wv['man'] + model.wv['woman']))\n",
    "print(model.similar_by_vector(model.wv['queen'] + model.wv['man'] - model.wv['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now do a classification based on each review embedding. Write a method `compute_mean_embedding` which, given a list of words, returns the mean of the embeddings of each words (systematically check if the words are in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_embedding(l):\n",
    "    \"\"\"\n",
    "    l: list of words\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a numpy array `mean_vectors` which contains the mean embedding of each sentence in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now classification using mean vector !\n",
    "import numpy as np\n",
    "mean_vectors = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, in a 10-fold fashion, a logistic regression on `mean_vectors` to predict the class of the review, which is given in y_train. What is the mean test accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you think of this accuracy estimation ? Is there some data leakage somewhere ? How would you do otherwise ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible continuation: use the word2vec embedding as initialization for an embedding layer to perform the same classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
