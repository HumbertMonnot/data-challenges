{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to sequence auto encoder\n",
    "\n",
    "In this notebook, we implement a sequence to sequence autoencoder which we will train for a english/french language translation task. \n",
    "\n",
    "#### You first  need to download the data on your computer. Go to http://www.manythings.org/anki/, pick your favorite language pairs (must be some_language/english !), and place the `.txt` file next to this notebook.\n",
    "\n",
    "### The idea\n",
    "We will use a first LSTM, which will be fed english sentences (after embedding of the words). We will capture the final hidden state of the LSTM, and use it to initialize the hidden state of a decoder LSTM.\n",
    "Then, the decoder LSTM produces a sequence of outputs which correspond to the translated sentence (one-hot representations of the french words).\n",
    "\n",
    "To simplify, during training, the decoder LSTM is fed, at each step, with the right french sentences (with an extra \"<start>\" word on the left), but with a delay of one. This is called teacher forcing.\n",
    "\n",
    "During inference, the decoder LSTM is fed with its own output at every step. This makes the inference task more difficult sadly. There are ways to circumvent this, but they are out of the scope of today !\n",
    "\n",
    "![image_seq2seq.png](image_seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the data from a txt file. `input_texts` is a list of english sentences and `target_texts` is the list of their french translations.\n",
    "\n",
    "#### You may need to change the path !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 50  # Latent dimensionality of the encoding space.\n",
    "num_samples = 50000  # Number of samples to train on.\n",
    "\n",
    "data_path = 'fra.txt'\n",
    "\n",
    "en_texts = []\n",
    "fr_texts = []\n",
    "\n",
    "def clean_string(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"'\", \" \")\n",
    "    s = s.replace(\"â€™\", \" \")\n",
    "    s = s.replace(\",\", \" \")\n",
    "    s = s.replace(\" essaye \", \" essaie \")\n",
    "    s = s.replace(\"0\",\"\")\n",
    "    s = s.replace(\"1\",\"\")\n",
    "    s = s.replace(\"2\",\"\")\n",
    "    s = s.replace(\"3\",\"\")\n",
    "    s = s.replace(\"4\",\"\")\n",
    "    s = s.replace(\"5\",\"\")\n",
    "    s = s.replace(\"6\",\"\")\n",
    "    s = s.replace(\"7\",\"\")\n",
    "    s = s.replace(\"8\",\"\")\n",
    "    s = s.replace(\"9\",\"\")\n",
    "    s = s.replace(\" re \", \" are \")\n",
    "    s = s.replace(\"he s \", \"he is \")\n",
    "    s = s.replace(\"we re \", \"we are \")\n",
    "    s = s.replace(\"they re \", \"they are \")\n",
    "    s = s.replace(\"i m \", \"i am \")\n",
    "    return s\n",
    "    \n",
    "#eng_prefixes = (\n",
    "#    \"i am \", \"i m \",\n",
    "#    \"he is\", \"he s \",\n",
    "#    \"she is\", \"she s \",\n",
    "#    \"you are\", \"you re \",\n",
    "#    \"we are\", \"we re \",\n",
    "#    \"they are\", \"they re \"\n",
    "#)\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \"\n",
    ")\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')[:2]\n",
    "    target_text = ''.join((c for c in unicodedata.normalize('NFD', target_text) if unicodedata.category(c) != 'Mn'))\n",
    "    input_text = clean_string(input_text)\n",
    "    if input_text.startswith(eng_prefixes) and len(target_text.split(' ')) < 10:\n",
    "        en_texts.append(clean_string(input_text))\n",
    "        fr_texts.append(target_text)\n",
    "            \n",
    "\n",
    "    \n",
    "fr_texts = ['<start> ' + elt + ' <end>' for elt in fr_texts]\n",
    "\n",
    "print(len(en_texts))\n",
    "for (i,o) in zip(en_texts[:10], fr_texts[:10]):\n",
    "    print(i, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of the text.\n",
    "\n",
    "The tokenizer object in tensorflow.keras.preprocessing.text is useful to convert words into integers, with some extra cleaning operation. Its useful methods in our case are:\n",
    "1. `fit_on_texts` method on the list of sentences, which fits the tokenizer to the given vocabulary. To be called before doing anything else.\n",
    "2. The `text_to_sequences` on the list of sentences: it returns a list of lists of integers, where each integer correspond to a word.\n",
    "3. `sequences_to_texts` does the opposite operation.\n",
    "\n",
    "#### Fit one tokenizer per language, and compute the sequences (i.e. lists of lists of integers) `en_sequences`, `fr_sequences` corresponding to `en_texts` and `fr_texts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "# Keep only vocab_size tokens.\n",
    "vocab_size = 10000 # maximum vocab size for both languages\n",
    "\n",
    "en_tokenizer = Tokenizer(num_words=vocab_size, oov_token='<UNK>')\n",
    "fr_tokenizer = Tokenizer(num_words=vocab_size, oov_token='<UNK>', filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "# fit the tokenizers and compute the sequences:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the sizes of the english and french vocabularies (the number of different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the maximum lengths of the english and french sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_maxlen = ?\n",
    "fr_maxlen = ?\n",
    "\n",
    "print('Max length english:', en_maxlen)\n",
    "print('Max length french:', fr_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the sequences back to texts, and look at the result.\n",
    "The text has been cleaned. Rare words have been replaced by an unknown token, all characters have been lowered etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now prepare the inputs.\n",
    "\n",
    "All of these operations will be done by you and no high-level functions.\n",
    "\n",
    "#### First, prepare an array `input_decoder`. This array should contain the english sequences, padded on the right. Its shape should be `(len(en_sequences), en_maxlen)`. We adopt the convention that padding is the value 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_encoder = np.zeros((len(en_sequences), en_maxlen), dtype=np.float32)\n",
    "\n",
    "## FILL THE ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, prepare an array `input_decoder` containing the french sequences. Its shape should be `(len(fr_sequences), fr_maxlen)`. Each element of this array corresponds to a sequence (with the \"start\" word: think about what would happen, or test yourself when you have the model, if we omit this \"start\" word in this input_decoder). Again the sequences are padded to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_decoder = np.zeros((len(fr_sequences), fr_maxlen), dtype=np.float32)\n",
    "\n",
    "## FILL THE ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare an array `output_decoder` containing the french sequences. This time:\n",
    "- don't include the \"start\" word (which is always the first in the fr_sequences)\n",
    "- do include the \"end\" word\n",
    "- do a one-hot encoding of the classes (without to_categorical) because we will use a `categorical_cross_entropy` on the predictions (which will be softmax over the fr vocabulary length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_decoder = np.zeros((len(fr_sequences), fr_maxlen, fr_vocab_size), dtype=np.bool)\n",
    "    \n",
    "## FILL THE ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now define the model.\n",
    "\n",
    "#### We first define the encoder part, using the 'complicated' keras syntax. Complete the code below using the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "\n",
    "\n",
    "# Declare an embedding layer, for the english language\n",
    "encoder_embedding =\n",
    "\n",
    "# Define an LSTM layer (100s of units this time), make it return its state\n",
    "encoder_lstm = \n",
    "\n",
    "# We now apply the embedding and the lstm to the input\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "embedded_encoder_inputs = encoder_embedding(encoder_inputs)\n",
    "_,state_h, state_c = encoder_lstm(embedded_encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [No code needed]  We now define the decoder part. Look at this code, try to understand what is happening !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))# french word embeddings\n",
    "decoder_embedding =  Embedding(fr_vocab_size, embedding_size)\n",
    "\n",
    "embedded_decoder_inputs = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# decoder lstm\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(embedded_decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(fr_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# While training, model takes eng and french words and outputs #translated french word\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [No code needed] What does the `translate` method do ?\n",
    "The `epoch_end` is just a wrapping of the `translate` as a keras callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "embedded_decoder_inputs = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(embedded_decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)# sampling model will take encoder states and decoder_input(seed initially) and output the predictions(french word index) We dont care about decoder_states2\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "print(encoder_model.summary())\n",
    "print(decoder_model.summary())\n",
    "\n",
    "def translate(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    h, c = encoder_model.predict([input_seq])\n",
    "    states_value = [h, c]\n",
    "    if len(h) > 1:\n",
    "        states_value = [h[-1:], c[-1:]]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fr_tokenizer.word_index['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)# Sample a token\n",
    "        sampled_word_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = fr_tokenizer.sequences_to_texts([[sampled_word_index]])[0]\n",
    "        #sampled_word = fr_index_to_word[sampled_word_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        # Exit condition: either hit max length\n",
    "        if (sampled_word == '<end>' or len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_word_index\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence\n",
    "    \n",
    "# The corresponding callback:\n",
    "def on_epoch_end(epoch, _):\n",
    "    for _ in range(1):\n",
    "        i = np.random.randint(0, len(en_sequences))\n",
    "        sentence = cleaned_en_sentences[i]\n",
    "        translated_sentence = translate(en_sequences[i])\n",
    "        translations = model.predict([[input_encoder[i]], [input_decoder[i]]])\n",
    "        translations_sequences = [[np.argmax(elt) for elt in trans] for trans in translations]\n",
    "        translated_forced = fr_tokenizer.sequences_to_texts(translations_sequences)[0]\n",
    "        translated_forced = translated_forced[:translated_forced.find('<end>')]\n",
    "        print('-')\n",
    "        print('Sentence:', sentence)\n",
    "        print('Translated:', translated_sentence)\n",
    "        print('Translated with teacher forcing:', translated_forced)\n",
    "        #print('Real translation', real_translation)\n",
    "    \n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "#for i in range(20):\n",
    "#    print(translate(en_sequences[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### We now fit the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [No code needed] What does this cell do ? Are you satisfied with the results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translations = model.predict([input_encoder, input_decoder])\n",
    "translations_sequences = [[np.argmax(elt) for elt in trans] for trans in translations]\n",
    "translations = fr_tokenizer.sequences_to_texts(translations_sequences)\n",
    "translations = [elt[:elt.find('<end>')] for elt in translations]\n",
    "for (tr, en) in zip(translations, cleaned_en_sentences):\n",
    "    print(tr, en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What do you think of this output ? What do you think is critically limiting this model ? How would you improve the results ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### More generally, what do you think of this seq2seq model ? Do you think it is biased in some ways ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
