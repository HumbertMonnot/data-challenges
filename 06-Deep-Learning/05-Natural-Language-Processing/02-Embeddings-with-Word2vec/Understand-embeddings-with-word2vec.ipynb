{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand embeddings with Word2Vec\n",
    "\n",
    "### Exercise objectives:\n",
    "- Convert words to vector representations thanks to embeddings\n",
    "- Discover the powerful Word2Vec algorithm\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "_Embeddings_ are representation of words thanks to vectors. These embeddings can be learnt within a Neural Network. But it can take time to converge. Another option is to learn them as a first step. Then, use them directly to feed the word representation into an RNN. \n",
    "\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "Keras provides many datasets, among which the ÌMDB dataset: it corresponds to sentences that are movie reviews. Each of them is related to a score given by the review writer.\n",
    "\n",
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that a too large number of sentences will make your compute slow down, or even freeze - your RAM can even overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "⚠️ **DISCLAIMER** ⚠️ **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 11:30:31.539283: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-18 11:30:31.570033: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, we jointly learnt a representation for the words, and feed this representation to a RNN, as shown here : \n",
    "\n",
    "<img src=\"layers_embedding.png\" width=\"400px\" />\n",
    "\n",
    "However, this increases the number of parameters to learn, which can slow the convergence, and make it harder!\n",
    "\n",
    "For that reason, we will separate the steps of learning the word representation and feeding it to a RNN. As shown here : \n",
    "\n",
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "We will learn the embedding with the word2vec.\n",
    "\n",
    "The drawback is indeed that the learnt embedding is not _specifically_ designed for our task. However, learning it independently of the task at hand (sentiment analysis) has some advantages : \n",
    "- it is very fast to do in general (with word2vec)\n",
    "- the representation learnt by word2vec is still meaningful \n",
    "- the convergence of the RNN alone will be easier and faster\n",
    "\n",
    "So let's learn an embedding with word2vec and see how meaningful it is!\n",
    "\n",
    "\n",
    "# Embedding with Word2Vec\n",
    "\n",
    "Let's use Word2Vec to embed the words of our sentences. Word2Vec will be able to convert each word to a fixed-size vectorial representation.\n",
    "\n",
    "For instance, we will have:\n",
    "- 'dog' --> [0.1, -0.3, 0.8]\n",
    "- 'cat' --> [-1.1, 2.3, 0.7]\n",
    "- 'apple' --> [3.1, 0.9, -4.7]\n",
    "\n",
    "Here, your embedding space is of size 3.\n",
    "\n",
    "What you expect is to have representation such as words with close meanings are close in this embedding space. As on this example:\n",
    "\n",
    "![Embedding](word_embedding.png)\n",
    "\n",
    "❓ **Question** ❓ Let's run Word2Vec! The following code imports word2vec from [GENSIM](https://radimrehurek.com/gensim/), a great python package that makes the use of the word2vec algorithm easy, fast and accurate - which is not an easy task. The second line learns the embedding representation of the words thanks to the sentences in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/humbert/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from gensim) (1.7.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the embedded representation of some words.\n",
    "\n",
    "You can use `word2vec.wv` as a dictionary.\n",
    "For instance, `word2vec.wv['dog']` will return a representation of `dog` in the embedding space.\n",
    "\n",
    "❓ **Question** ❓ Try different words - especially, try non-existing words to see that they don't have any representation (which is perfectly normal as their representation were not learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_other = Word2Vec(sentences=X_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15643266,  0.33435693, -0.23366775,  0.2813018 , -0.06015401,\n",
       "       -0.3247563 ,  0.26593184,  0.7909876 , -0.1884312 , -0.34094706,\n",
       "       -0.02625695, -0.48829427,  0.1413697 ,  0.1832338 , -0.09167619,\n",
       "       -0.20158379,  0.24933021, -0.19483404,  0.02383106, -0.3536339 ,\n",
       "        0.1257421 , -0.01322551,  0.1716626 , -0.29368004,  0.02738187,\n",
       "        0.03229668, -0.29172075, -0.19693182, -0.32873565, -0.05513848,\n",
       "        0.17672272, -0.03335209,  0.12025582, -0.56569934, -0.20316178,\n",
       "        0.27316704,  0.1364251 , -0.20875664, -0.39918387, -0.25136122,\n",
       "       -0.261277  , -0.36314073, -0.3872287 ,  0.21884948,  0.5352526 ,\n",
       "       -0.12732477, -0.1233341 , -0.15305407,  0.5883032 ,  0.27347234,\n",
       "        0.1514674 , -0.22549902, -0.29780993,  0.07541078, -0.34306324,\n",
       "        0.2549272 ,  0.2706402 , -0.0867511 , -0.06323722,  0.13940597,\n",
       "        0.31730366, -0.10724132, -0.04988686,  0.27300525, -0.08706812,\n",
       "        0.20461924, -0.1615899 ,  0.23136964, -0.36312258,  0.32964838,\n",
       "       -0.21278511,  0.12573   ,  0.35048273, -0.31588027,  0.4187244 ,\n",
       "        0.13733847,  0.01877535,  0.26027787, -0.13490562,  0.05010239,\n",
       "       -0.33169246, -0.3121743 , -0.20131375,  0.11219577, -0.17573561,\n",
       "       -0.25851056, -0.12543793,  0.37682337,  0.5590124 ,  0.04365706,\n",
       "        0.11022668,  0.3469591 ,  0.06404802,  0.11518132,  0.54426074,\n",
       "       -0.02712112,  0.19492601, -0.05654912,  0.08718093,  0.1827065 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.0501831e-01,  7.3407519e-01,  1.2081716e+00,  1.2036262e+00,\n",
       "        3.9037783e+00,  6.3459623e-01,  3.3966073e-01,  6.5524292e-01,\n",
       "        4.2478058e-01,  6.8266410e-01,  6.4848608e-01,  6.5254378e-01,\n",
       "       -2.7798958e+00,  2.0444266e-01,  1.4286186e+00,  5.2821344e-01,\n",
       "        6.9730735e-01,  5.4862732e-01,  2.3428869e+00,  4.0071693e-01,\n",
       "        6.7607351e-02,  1.1319041e+00,  6.0196811e-01,  4.7773981e-01,\n",
       "       -6.3815123e-01, -2.1211631e+00,  6.1959749e-01,  6.1057067e-01,\n",
       "        4.2678079e-01, -2.0190909e+00,  6.0705286e-01,  1.2013780e+00,\n",
       "        6.8422717e-01,  5.5766332e-01,  1.1044139e+00,  8.3218586e-01,\n",
       "        6.5301913e-01,  5.8150798e-01,  6.1348367e-01,  6.1959779e-01,\n",
       "        9.8669279e-01,  7.8622591e-01,  8.3216226e-01,  8.5222393e-01,\n",
       "        6.9723022e-01, -2.6848620e-01,  6.0028267e-01,  1.2149249e+00,\n",
       "        7.0817971e-01,  3.2506171e-01,  8.7681198e-01,  5.0484598e-01,\n",
       "        1.0674266e+00, -4.3424149e+00,  6.5786159e-01,  9.6915627e-01,\n",
       "        4.9912596e-01,  1.3607501e+00,  6.6911715e-01,  9.2866188e-01,\n",
       "        9.6905273e-01,  1.3713989e+00,  8.6994097e-02,  5.2436066e-01,\n",
       "        4.6717617e-01,  3.3475560e-01,  1.3448099e+00,  4.0207815e-01,\n",
       "        7.2300041e-01,  5.5004960e-01,  8.9555889e-01,  8.1983608e-01,\n",
       "        6.5335613e-01,  9.0480888e-01,  6.4565587e-01,  7.8862321e-01,\n",
       "       -1.5475372e+00,  4.9148560e-01,  1.0105391e+00, -1.1934035e+01,\n",
       "        6.6456038e-01,  9.7833109e-01,  6.3084197e-01,  5.9812009e-01,\n",
       "        9.5361900e-01,  9.2339343e-01,  4.3972675e+01,  7.3946470e-01,\n",
       "        7.4782085e-01,  5.4005012e-02,  2.6049817e-01,  8.5439718e-01,\n",
       "        6.9483973e-02,  5.9575599e-01,  5.3905076e-01,  4.1545594e-01,\n",
       "        7.2393245e-01,  6.1357170e-01,  4.2835660e+00,  1.9567395e+02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((word2vec.wv['dog'] - word2vec_other.wv['dog'])/word2vec.wv['dog'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ What is the size of each word representation, and therefore, what is the size of the embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv['dog'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to know if this embedding make any sense? To do that, we will check that words with a close meaning have close representations. \n",
    "\n",
    "Let's use the `word2vec.most_similar(...)` method that, given an input word, display the \"closest\" words in the embedding space. If the embedding is well done, then words that have close meanings will have close representation in the embedding space.\n",
    "\n",
    "❓ **Question** ❓ Test the `most_similar` method on different words. \n",
    "\n",
    "❗ **Remark** ❗ Indeed, the quality of the closeness will depend on the quality of your embedding, and thus, depend on the number of sentences that you have loaded and from which you create your embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('using', 0.9886091351509094),\n",
       " ('fallon', 0.988295316696167),\n",
       " ('passing', 0.9874628782272339),\n",
       " ('enormous', 0.9864641427993774),\n",
       " ('bright', 0.9864473938941956),\n",
       " ('wayne', 0.9861370325088501),\n",
       " ('loggia', 0.9857607483863831),\n",
       " ('green', 0.9855518341064453),\n",
       " ('tommy', 0.9854166507720947),\n",
       " (\"girl's\", 0.9850045442581177)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('dog', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to `most_similar` used on words directly, we can use `similar_by_vector` on vectors to do the same thing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('using', 0.9886091351509094),\n",
       " ('fallon', 0.988295316696167),\n",
       " ('passing', 0.9874628782272339),\n",
       " ('enormous', 0.9864641427993774),\n",
       " ('bright', 0.9864473938941956),\n",
       " ('wayne', 0.9861370325088501),\n",
       " ('loggia', 0.9857607483863831),\n",
       " ('green', 0.9855518341064453),\n",
       " ('tommy', 0.9854166507720947),\n",
       " (\"girl's\", 0.9850045442581177)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector('dog', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic on words\n",
    "\n",
    "Now, let's do mathematical operations on words - meaning on their vector representations!\n",
    "\n",
    "As any word is represented as a vector, you can do basic arithmetic as:\n",
    "\n",
    "$$W2V(good) - W2V(bad)$$\n",
    "\n",
    "❓ **Question** ❓ Do this mathematical operation and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "vec = word2vec.wv['good'] - word2vec.wv['bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, image for a second that, somehow, the following equality holds true - just for a second\n",
    "\n",
    "$$W2V(good) - W2V(bad) = W2V(nice) - W2V(stupid)$$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$W2V(good) - W2V(bad) + W2V(stupid) = W2V(nice)$$\n",
    "\n",
    "❓ **Question** ❓ Let's, just for fun (as it would be foolish of us to think that this equality holds true ...), do the operation $W2V(good) - W2V(bad) + W2V(stupid)$ and store it in a `res` variable (which will be a vector of size 100 that you can print)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "res = word2vec.wv['good'] - word2vec.wv['bad'] + word2vec.wv['stupid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We earlier said that, for any vector, it is possible to see the closest vectors in the embedding space.\n",
    "\n",
    "❓ **Question** ❓ Look at the closest vector (thanks to the `word2vec.wv.similar_by_vector` function) of `res`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.794916033744812),\n",
       " ('potential', 0.7929612398147583),\n",
       " ('always', 0.7898537516593933),\n",
       " ('done', 0.783433198928833),\n",
       " ('decent', 0.7773710489273071),\n",
       " ('reworking', 0.7765313982963562),\n",
       " ('rarely', 0.7746520042419434),\n",
       " ('written', 0.7716220021247864),\n",
       " ('known', 0.7715383768081665),\n",
       " ('given', 0.7694361209869385)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(res, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredible right! You can do arithmetic operations on words!\n",
    "\n",
    "❓ **Question** ❓ You can try on arithmetic such as \n",
    "\n",
    "$$W2V(Boy) - W2V(Girl) = W2V(Man) - W2V(Woman)$$\n",
    "\n",
    "or \n",
    "\n",
    "$$W2V(Queen) - W2V(King) = W2V(actress) - W2V(actor)$$\n",
    "\n",
    "❗ **Remark** ❗ You will probably see that the results are not perfect. But don't forget that you trained your model on a very small corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 0.9710970520973206),\n",
       " ('role', 0.873417317867279),\n",
       " ('actress', 0.8567383885383606),\n",
       " ('performance', 0.8553652763366699),\n",
       " ('job', 0.8025171756744385),\n",
       " ('guy', 0.7906687259674072),\n",
       " ('character', 0.7826564908027649),\n",
       " ('man', 0.7694869041442871),\n",
       " ('emmy', 0.7589439749717712),\n",
       " ('villain', 0.7445894479751587)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.similar_by_vector(word2vec.wv['queen'] - word2vec.wv['king'] + word2vec.wv['actor'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder where does this magic comes from (at quite a low price, you just run a line of code on a very small corpus and it was trained within few minutes). The magic comes from the way Word2Vec is trained. The details are quite complex, but you can remember that Word2vec, in `word2vec = Word2Vec(sentences=X_train)` , actually trains a internal neural network (that you don't see).  \n",
    "\n",
    "In a nutshell, this internal NN predicts a word from the surroundings words in a sentences. So it chooses many splits in the different sentences, choose some words as inputs $X$  and a word as output $y$ which it tries to predict, in the embedding space.\n",
    "\n",
    "And as any neural network, Word2Vec has some hyperparameters. Let's check some. \n",
    "\n",
    "\n",
    "# Word2Vec hyperparameters\n",
    "\n",
    "\n",
    "❓ **Question** ❓ The first important hyperparameter is the `vector_size` argument. It corresponds to the size of the embedding space. Learn a new `word2vec_2` model, still trained on the `X_train`, but with a smaller or higher `vector_size`.\n",
    "\n",
    "Verify on some words that the corresponding embedding is of your selected size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "word2vec_2 = Word2Vec(X_train, vector_size = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Use the `word2vec.wv.key_to_index` attribute to display the size of the learnt vocabulary. On the other hand, compare it to the number of different words in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'a',\n",
       " 3: 'and',\n",
       " 4: 'of',\n",
       " 5: 'to',\n",
       " 6: 'is',\n",
       " 7: 'br',\n",
       " 8: 'in',\n",
       " 9: 'i',\n",
       " 10: 'it',\n",
       " 11: 'this',\n",
       " 12: 'that',\n",
       " 13: 'was',\n",
       " 14: 'as',\n",
       " 15: 'for',\n",
       " 16: 'with',\n",
       " 17: 'but',\n",
       " 18: 'movie',\n",
       " 19: 'film',\n",
       " 20: 'on',\n",
       " 21: 'not',\n",
       " 22: 'you',\n",
       " 23: 'his',\n",
       " 24: 'are',\n",
       " 25: 'have',\n",
       " 26: 'one',\n",
       " 27: 'be',\n",
       " 28: 'he',\n",
       " 29: 'all',\n",
       " 30: 'at',\n",
       " 31: 'by',\n",
       " 32: 'they',\n",
       " 33: 'an',\n",
       " 34: 'so',\n",
       " 35: 'like',\n",
       " 36: 'who',\n",
       " 37: 'from',\n",
       " 38: 'her',\n",
       " 39: 'or',\n",
       " 40: 'just',\n",
       " 41: 'if',\n",
       " 42: 'out',\n",
       " 43: 'about',\n",
       " 44: \"it's\",\n",
       " 45: 'has',\n",
       " 46: 'what',\n",
       " 47: 'some',\n",
       " 48: 'there',\n",
       " 49: 'good',\n",
       " 50: 'more',\n",
       " 51: 'when',\n",
       " 52: 'very',\n",
       " 53: 'no',\n",
       " 54: 'up',\n",
       " 55: 'she',\n",
       " 56: 'my',\n",
       " 57: 'time',\n",
       " 58: 'even',\n",
       " 59: 'which',\n",
       " 60: 'would',\n",
       " 61: 'really',\n",
       " 62: 'only',\n",
       " 63: 'had',\n",
       " 64: 'story',\n",
       " 65: 'me',\n",
       " 66: 'see',\n",
       " 67: 'can',\n",
       " 68: 'their',\n",
       " 69: 'were',\n",
       " 70: 'well',\n",
       " 71: 'than',\n",
       " 72: 'much',\n",
       " 73: 'get',\n",
       " 74: 'do',\n",
       " 75: 'great',\n",
       " 76: 'been',\n",
       " 77: 'we',\n",
       " 78: 'first',\n",
       " 79: 'bad',\n",
       " 80: 'because',\n",
       " 81: 'into',\n",
       " 82: 'other',\n",
       " 83: 'will',\n",
       " 84: 'how',\n",
       " 85: 'also',\n",
       " 86: 'most',\n",
       " 87: 'then',\n",
       " 88: 'too',\n",
       " 89: 'him',\n",
       " 90: 'made',\n",
       " 91: \"don't\",\n",
       " 92: 'people',\n",
       " 93: 'way',\n",
       " 94: 'them',\n",
       " 95: 'make',\n",
       " 96: 'its',\n",
       " 97: 'movies',\n",
       " 98: 'could',\n",
       " 99: 'any',\n",
       " 100: 'films',\n",
       " 101: 'think',\n",
       " 102: 'watch',\n",
       " 103: 'plot',\n",
       " 104: 'after',\n",
       " 105: 'two',\n",
       " 106: 'best',\n",
       " 107: 'many',\n",
       " 108: 'acting',\n",
       " 109: 'life',\n",
       " 110: 'character',\n",
       " 111: 'characters',\n",
       " 112: 'never',\n",
       " 113: 'seen',\n",
       " 114: 'where',\n",
       " 115: 'being',\n",
       " 116: 'did',\n",
       " 117: 'over',\n",
       " 118: 'little',\n",
       " 119: 'these',\n",
       " 120: 'love',\n",
       " 121: 'end',\n",
       " 122: 'man',\n",
       " 123: 'show',\n",
       " 124: 'better',\n",
       " 125: 'off',\n",
       " 126: 'ever',\n",
       " 127: 'know',\n",
       " 128: 'still',\n",
       " 129: 'does',\n",
       " 130: 'say',\n",
       " 131: 'while',\n",
       " 132: 'your',\n",
       " 133: 'scene',\n",
       " 134: 'go',\n",
       " 135: 'why',\n",
       " 136: 'should',\n",
       " 137: 'through',\n",
       " 138: 'such',\n",
       " 139: 'here',\n",
       " 140: 'scenes',\n",
       " 141: 'something',\n",
       " 142: 'real',\n",
       " 143: 'back',\n",
       " 144: 'now',\n",
       " 145: 'thing',\n",
       " 146: \"i'm\",\n",
       " 147: \"doesn't\",\n",
       " 148: 'find',\n",
       " 149: 'before',\n",
       " 150: \"didn't\",\n",
       " 151: 'though',\n",
       " 152: 'makes',\n",
       " 153: 'old',\n",
       " 154: 'again',\n",
       " 155: 'watching',\n",
       " 156: 'funny',\n",
       " 157: 'new',\n",
       " 158: 'those',\n",
       " 159: 'every',\n",
       " 160: 'director',\n",
       " 161: 'actors',\n",
       " 162: 'years',\n",
       " 163: 'going',\n",
       " 164: 'nothing',\n",
       " 165: 'actually',\n",
       " 166: 'another',\n",
       " 167: 'look',\n",
       " 168: 'same',\n",
       " 169: 'work',\n",
       " 170: 'part',\n",
       " 171: '10',\n",
       " 172: 'pretty',\n",
       " 173: 'want',\n",
       " 174: 'quite',\n",
       " 175: 'few',\n",
       " 176: 'young',\n",
       " 177: 'thought',\n",
       " 178: 'down',\n",
       " 179: 'bit',\n",
       " 180: 'horror',\n",
       " 181: 'own',\n",
       " 182: 'cast',\n",
       " 183: \"can't\",\n",
       " 184: 'give',\n",
       " 185: 'around',\n",
       " 186: 'fact',\n",
       " 187: 'lot',\n",
       " 188: \"that's\",\n",
       " 189: 'world',\n",
       " 190: 'things',\n",
       " 191: 'enough',\n",
       " 192: 'point',\n",
       " 193: 'series',\n",
       " 194: 'both',\n",
       " 195: 'may',\n",
       " 196: 'done',\n",
       " 197: 'long',\n",
       " 198: 'seems',\n",
       " 199: 'got',\n",
       " 200: 'however',\n",
       " 201: 'gets',\n",
       " 202: 'take',\n",
       " 203: 'role',\n",
       " 204: 'us',\n",
       " 205: 'always',\n",
       " 206: 'original',\n",
       " 207: 'between',\n",
       " 208: 'big',\n",
       " 209: 'right',\n",
       " 210: 'saw',\n",
       " 211: 'music',\n",
       " 212: 'times',\n",
       " 213: 'without',\n",
       " 214: \"i've\",\n",
       " 215: \"isn't\",\n",
       " 216: 'family',\n",
       " 217: 'far',\n",
       " 218: 'whole',\n",
       " 219: 'might',\n",
       " 220: 'minutes',\n",
       " 221: 'am',\n",
       " 222: 'come',\n",
       " 223: 'least',\n",
       " 224: \"there's\",\n",
       " 225: '2',\n",
       " 226: 'action',\n",
       " 227: 'script',\n",
       " 228: 'must',\n",
       " 229: 'tv',\n",
       " 230: 'since',\n",
       " 231: 'kind',\n",
       " 232: 'away',\n",
       " 233: 'interesting',\n",
       " 234: 'almost',\n",
       " 235: 'guy',\n",
       " 236: 'fun',\n",
       " 237: 'performance',\n",
       " 238: 'rather',\n",
       " 239: 'especially',\n",
       " 240: 'anything',\n",
       " 241: 'hard',\n",
       " 242: 'last',\n",
       " 243: 'actor',\n",
       " 244: 'sure',\n",
       " 245: 'believe',\n",
       " 246: 'three',\n",
       " 247: 'girl',\n",
       " 248: 'comedy',\n",
       " 249: \"he's\",\n",
       " 250: 'ending',\n",
       " 251: 'making',\n",
       " 252: 'worst',\n",
       " 253: 'looking',\n",
       " 254: 'anyone',\n",
       " 255: 'woman',\n",
       " 256: 'screen',\n",
       " 257: 'day',\n",
       " 258: 'trying',\n",
       " 259: 'maybe',\n",
       " 260: 'found',\n",
       " 261: 'course',\n",
       " 262: 'watched',\n",
       " 263: 'each',\n",
       " 264: 'having',\n",
       " 265: 'set',\n",
       " 266: 'probably',\n",
       " 267: 'yet',\n",
       " 268: 'although',\n",
       " 269: 'book',\n",
       " 270: 'comes',\n",
       " 271: 'feel',\n",
       " 272: 'money',\n",
       " 273: 'everything',\n",
       " 274: 'played',\n",
       " 275: \"wasn't\",\n",
       " 276: 'night',\n",
       " 277: 'true',\n",
       " 278: 'goes',\n",
       " 279: 'instead',\n",
       " 280: 'put',\n",
       " 281: 'sense',\n",
       " 282: 'reason',\n",
       " 283: 'place',\n",
       " 284: 'high',\n",
       " 285: 'dvd',\n",
       " 286: 'audience',\n",
       " 287: 'main',\n",
       " 288: 'looks',\n",
       " 289: 'later',\n",
       " 290: '1',\n",
       " 291: 'different',\n",
       " 292: 'job',\n",
       " 293: 'second',\n",
       " 294: 'once',\n",
       " 295: 'shows',\n",
       " 296: 'during',\n",
       " 297: 'american',\n",
       " 298: 'else',\n",
       " 299: 'together',\n",
       " 300: 'takes',\n",
       " 301: 'worth',\n",
       " 302: 'someone',\n",
       " 303: 'plays',\n",
       " 304: 'fan',\n",
       " 305: 'wife',\n",
       " 306: 'our',\n",
       " 307: 'said',\n",
       " 308: 'play',\n",
       " 309: 'star',\n",
       " 310: 'year',\n",
       " 311: 'read',\n",
       " 312: 'effects',\n",
       " 313: 'version',\n",
       " 314: 'home',\n",
       " 315: 'special',\n",
       " 316: 'half',\n",
       " 317: 'black',\n",
       " 318: 'idea',\n",
       " 319: 'boring',\n",
       " 320: 'seeing',\n",
       " 321: 'given',\n",
       " 322: 'help',\n",
       " 323: '3',\n",
       " 324: 'less',\n",
       " 325: 'war',\n",
       " 326: 'beautiful',\n",
       " 327: 'house',\n",
       " 328: 'seem',\n",
       " 329: 'himself',\n",
       " 330: 'death',\n",
       " 331: \"you're\",\n",
       " 332: 'production',\n",
       " 333: 'left',\n",
       " 334: 'mind',\n",
       " 335: 'excellent',\n",
       " 336: 'simply',\n",
       " 337: 'john',\n",
       " 338: 'top',\n",
       " 339: 'either',\n",
       " 340: 'short',\n",
       " 341: 'camera',\n",
       " 342: 'hollywood',\n",
       " 343: 'stupid',\n",
       " 344: 'poor',\n",
       " 345: 'shot',\n",
       " 346: 'couple',\n",
       " 347: 'father',\n",
       " 348: 'used',\n",
       " 349: 'perfect',\n",
       " 350: 'line',\n",
       " 351: 'dead',\n",
       " 352: 'until',\n",
       " 353: 'nice',\n",
       " 354: \"couldn't\",\n",
       " 355: 'kids',\n",
       " 356: 'understand',\n",
       " 357: 'try',\n",
       " 358: 'enjoy',\n",
       " 359: 'classic',\n",
       " 360: 'completely',\n",
       " 361: 'mean',\n",
       " 362: 'along',\n",
       " 363: 'wrong',\n",
       " 364: 'awful',\n",
       " 365: 'rest',\n",
       " 366: 'perhaps',\n",
       " 367: 'everyone',\n",
       " 368: 'start',\n",
       " 369: 'need',\n",
       " 370: 'performances',\n",
       " 371: 'truly',\n",
       " 372: 'men',\n",
       " 373: 'boy',\n",
       " 374: \"i'd\",\n",
       " 375: 'name',\n",
       " 376: 'finally',\n",
       " 377: 'liked',\n",
       " 378: 'budget',\n",
       " 379: 'keep',\n",
       " 380: 'small',\n",
       " 381: 'gives',\n",
       " 382: 'head',\n",
       " 383: 'felt',\n",
       " 384: 'wonderful',\n",
       " 385: 'full',\n",
       " 386: 'face',\n",
       " 387: 'style',\n",
       " 388: '\\x96',\n",
       " 389: 'early',\n",
       " 390: 'mr',\n",
       " 391: 'stars',\n",
       " 392: 'let',\n",
       " 393: 'low',\n",
       " 394: 'friends',\n",
       " 395: 'tell',\n",
       " 396: 'drama',\n",
       " 397: 'hope',\n",
       " 398: 'written',\n",
       " 399: 'school',\n",
       " 400: 'dialogue',\n",
       " 401: 'recommend',\n",
       " 402: 'terrible',\n",
       " 403: 'case',\n",
       " 404: 'worse',\n",
       " 405: 'based',\n",
       " 406: 'sort',\n",
       " 407: 'moments',\n",
       " 408: 'run',\n",
       " 409: 'playing',\n",
       " 410: 'next',\n",
       " 411: 'video',\n",
       " 412: 'supposed',\n",
       " 413: 'mother',\n",
       " 414: 'often',\n",
       " 415: 'getting',\n",
       " 416: 'yes',\n",
       " 417: 'become',\n",
       " 418: 'use',\n",
       " 419: 'loved',\n",
       " 420: 'human',\n",
       " 421: 'came',\n",
       " 422: 'absolutely',\n",
       " 423: 'others',\n",
       " 424: 'totally',\n",
       " 425: 'called',\n",
       " 426: 'lines',\n",
       " 427: 'fans',\n",
       " 428: 'certainly',\n",
       " 429: 'side',\n",
       " 430: \"they're\",\n",
       " 431: 'days',\n",
       " 432: 'doing',\n",
       " 433: 'entertaining',\n",
       " 434: 'example',\n",
       " 435: 'dark',\n",
       " 436: 'overall',\n",
       " 437: 'live',\n",
       " 438: 'art',\n",
       " 439: 'killer',\n",
       " 440: 'definitely',\n",
       " 441: 'lost',\n",
       " 442: 'went',\n",
       " 443: 'seemed',\n",
       " 444: 'city',\n",
       " 445: 'beginning',\n",
       " 446: 'friend',\n",
       " 447: 'tries',\n",
       " 448: 'problem',\n",
       " 449: 'town',\n",
       " 450: 'picture',\n",
       " 451: 'turn',\n",
       " 452: 'god',\n",
       " 453: 'remember',\n",
       " 454: 'against',\n",
       " 455: 'evil',\n",
       " 456: 'entire',\n",
       " 457: 'care',\n",
       " 458: 'wanted',\n",
       " 459: 'already',\n",
       " 460: 'fine',\n",
       " 461: 'sex',\n",
       " 462: 'actress',\n",
       " 463: 'close',\n",
       " 464: 'women',\n",
       " 465: 'itself',\n",
       " 466: 'particularly',\n",
       " 467: 'title',\n",
       " 468: 'history',\n",
       " 469: 'act',\n",
       " 470: 'throughout',\n",
       " 471: 'writing',\n",
       " 472: 'myself',\n",
       " 473: 'episode',\n",
       " 474: \"won't\",\n",
       " 475: 'white',\n",
       " 476: 'under',\n",
       " 477: 'cinema',\n",
       " 478: 'etc',\n",
       " 479: 'sound',\n",
       " 480: \"she's\",\n",
       " 481: 'unfortunately',\n",
       " 482: 'anyway',\n",
       " 483: 'oh',\n",
       " 484: 'wants',\n",
       " 485: 'hand',\n",
       " 486: 'direction',\n",
       " 487: 'piece',\n",
       " 488: 'cannot',\n",
       " 489: 'daughter',\n",
       " 490: 'stuff',\n",
       " 491: '5',\n",
       " 492: 'behind',\n",
       " 493: 'despite',\n",
       " 494: 'themselves',\n",
       " 495: 'humor',\n",
       " 496: 'lives',\n",
       " 497: 'children',\n",
       " 498: 'eyes',\n",
       " 499: 'lead',\n",
       " 500: 'highly',\n",
       " 501: 'able',\n",
       " 502: 'late',\n",
       " 503: 'moment',\n",
       " 504: 'hour',\n",
       " 505: 'person',\n",
       " 506: 'becomes',\n",
       " 507: 'kid',\n",
       " 508: 'waste',\n",
       " 509: 'child',\n",
       " 510: 'parts',\n",
       " 511: 'leave',\n",
       " 512: 'favorite',\n",
       " 513: 'viewer',\n",
       " 514: 'kill',\n",
       " 515: 'slow',\n",
       " 516: 'says',\n",
       " 517: 'thinking',\n",
       " 518: \"you'll\",\n",
       " 519: 'guys',\n",
       " 520: 'murder',\n",
       " 521: 'soon',\n",
       " 522: 'decent',\n",
       " 523: 'age',\n",
       " 524: 'past',\n",
       " 525: 'expect',\n",
       " 526: 'girls',\n",
       " 527: 'type',\n",
       " 528: 'guess',\n",
       " 529: 'voice',\n",
       " 530: '4',\n",
       " 531: 'happen',\n",
       " 532: 'brilliant',\n",
       " 533: 'b',\n",
       " 534: 'obvious',\n",
       " 535: 'matter',\n",
       " 536: 'several',\n",
       " 537: 'starts',\n",
       " 538: 'feeling',\n",
       " 539: 'michael',\n",
       " 540: 'heart',\n",
       " 541: 'final',\n",
       " 542: 'genre',\n",
       " 543: 'ok',\n",
       " 544: 'looked',\n",
       " 545: 'directed',\n",
       " 546: 'except',\n",
       " 547: 'involved',\n",
       " 548: 'flick',\n",
       " 549: 'police',\n",
       " 550: 'chance',\n",
       " 551: 'sometimes',\n",
       " 552: 'laugh',\n",
       " 553: 'blood',\n",
       " 554: 'opinion',\n",
       " 555: 'including',\n",
       " 556: 'experience',\n",
       " 557: 'number',\n",
       " 558: 'strong',\n",
       " 559: 'alone',\n",
       " 560: 'power',\n",
       " 561: 'gave',\n",
       " 562: 'robert',\n",
       " 563: 'attempt',\n",
       " 564: 'score',\n",
       " 565: 'works',\n",
       " 566: 'possible',\n",
       " 567: 'writer',\n",
       " 568: 'david',\n",
       " 569: 'told',\n",
       " 570: 'annoying',\n",
       " 571: 'somewhat',\n",
       " 572: 'known',\n",
       " 573: \"i'll\",\n",
       " 574: 'wonder',\n",
       " 575: 'stop',\n",
       " 576: 'cut',\n",
       " 577: \"aren't\",\n",
       " 578: 'episodes',\n",
       " 579: 'enjoyed',\n",
       " 580: 'killed',\n",
       " 581: 'took',\n",
       " 582: 'turned',\n",
       " 583: 'wish',\n",
       " 584: 'happens',\n",
       " 585: 'words',\n",
       " 586: 'turns',\n",
       " 587: \"film's\",\n",
       " 588: 'amazing',\n",
       " 589: 'english',\n",
       " 590: 'happened',\n",
       " 591: 'today',\n",
       " 592: 'lady',\n",
       " 593: 'view',\n",
       " 594: 'whose',\n",
       " 595: 'sequel',\n",
       " 596: 'horrible',\n",
       " 597: 'hit',\n",
       " 598: 'ago',\n",
       " 599: 'jane',\n",
       " 600: 'simple',\n",
       " 601: 'female',\n",
       " 602: 'quality',\n",
       " 603: 'scary',\n",
       " 604: 'coming',\n",
       " 605: 'seriously',\n",
       " 606: 'mystery',\n",
       " 607: 'save',\n",
       " 608: 'james',\n",
       " 609: 'relationship',\n",
       " 610: 'group',\n",
       " 611: 'please',\n",
       " 612: 'roles',\n",
       " 613: 'novel',\n",
       " 614: 'single',\n",
       " 615: 's',\n",
       " 616: 'middle',\n",
       " 617: 'musical',\n",
       " 618: 'hell',\n",
       " 619: 'across',\n",
       " 620: 'word',\n",
       " 621: \"wouldn't\",\n",
       " 622: 'level',\n",
       " 623: 'serious',\n",
       " 624: 'lack',\n",
       " 625: 'usually',\n",
       " 626: 'mostly',\n",
       " 627: 'shown',\n",
       " 628: 'eye',\n",
       " 629: 'king',\n",
       " 630: 'red',\n",
       " 631: 'sad',\n",
       " 632: 'crap',\n",
       " 633: 'bring',\n",
       " 634: 'york',\n",
       " 635: 'body',\n",
       " 636: 'running',\n",
       " 637: 'car',\n",
       " 638: 'jokes',\n",
       " 639: 'herself',\n",
       " 640: 'hours',\n",
       " 641: 'light',\n",
       " 642: 'self',\n",
       " 643: 'happy',\n",
       " 644: '7',\n",
       " 645: 'gore',\n",
       " 646: 'events',\n",
       " 647: 'obviously',\n",
       " 648: \"'\",\n",
       " 649: 'room',\n",
       " 650: 'basically',\n",
       " 651: 'british',\n",
       " 652: 'fight',\n",
       " 653: 'important',\n",
       " 654: 'miss',\n",
       " 655: 'comic',\n",
       " 656: 'cool',\n",
       " 657: 'son',\n",
       " 658: 'husband',\n",
       " 659: 'interest',\n",
       " 660: 'extremely',\n",
       " 661: 'documentary',\n",
       " 662: 'due',\n",
       " 663: 'local',\n",
       " 664: 'stories',\n",
       " 665: 'hilarious',\n",
       " 666: 'none',\n",
       " 667: 'predictable',\n",
       " 668: 'shots',\n",
       " 669: 'thriller',\n",
       " 670: 'started',\n",
       " 671: 'huge',\n",
       " 672: 'brother',\n",
       " 673: 'straight',\n",
       " 674: 'elements',\n",
       " 675: 'taken',\n",
       " 676: 'ten',\n",
       " 677: 'lame',\n",
       " 678: 'call',\n",
       " 679: 'dull',\n",
       " 680: 'oscar',\n",
       " 681: 'above',\n",
       " 682: 'yourself',\n",
       " 683: 'add',\n",
       " 684: 'richard',\n",
       " 685: 'near',\n",
       " 686: 'surprised',\n",
       " 687: 'rating',\n",
       " 688: 'released',\n",
       " 689: 'game',\n",
       " 690: 'clearly',\n",
       " 691: 'hero',\n",
       " 692: 'sets',\n",
       " 693: 'usual',\n",
       " 694: 'ridiculous',\n",
       " 695: 'storyline',\n",
       " 696: 'disappointed',\n",
       " 697: 'complete',\n",
       " 698: 'talent',\n",
       " 699: 'romantic',\n",
       " 700: 'entertainment',\n",
       " 701: 'country',\n",
       " 702: 'television',\n",
       " 703: \"let's\",\n",
       " 704: 'lee',\n",
       " 705: 'fast',\n",
       " 706: 'supporting',\n",
       " 707: 'violence',\n",
       " 708: 'similar',\n",
       " 709: 'dialog',\n",
       " 710: '9',\n",
       " 711: 'falls',\n",
       " 712: 'heard',\n",
       " 713: 'apparently',\n",
       " 714: 'finds',\n",
       " 715: 'japanese',\n",
       " 716: 'nor',\n",
       " 717: 'giving',\n",
       " 718: 'twist',\n",
       " 719: 'beyond',\n",
       " 720: 'attention',\n",
       " 721: 'ones',\n",
       " 722: 'whether',\n",
       " 723: 'editing',\n",
       " 724: 'songs',\n",
       " 725: 'tale',\n",
       " 726: 'song',\n",
       " 727: 'die',\n",
       " 728: \"haven't\",\n",
       " 729: 'minute',\n",
       " 730: 'fall',\n",
       " 731: 'peter',\n",
       " 732: 'lots',\n",
       " 733: 'exactly',\n",
       " 734: 'figure',\n",
       " 735: 'saying',\n",
       " 736: 'taking',\n",
       " 737: 'moving',\n",
       " 738: 'possibly',\n",
       " 739: 'reality',\n",
       " 740: 'screenplay',\n",
       " 741: 'strange',\n",
       " 742: 'wait',\n",
       " 743: 'non',\n",
       " 744: 'jack',\n",
       " 745: 'typical',\n",
       " 746: 'comments',\n",
       " 747: 'stay',\n",
       " 748: 'means',\n",
       " 749: 'future',\n",
       " 750: 'realistic',\n",
       " 751: 'cinematography',\n",
       " 752: 'mention',\n",
       " 753: 'sit',\n",
       " 754: 'somehow',\n",
       " 755: 'sorry',\n",
       " 756: 'named',\n",
       " 757: 'animation',\n",
       " 758: 'indeed',\n",
       " 759: 'message',\n",
       " 760: 'talk',\n",
       " 761: 'living',\n",
       " 762: 'street',\n",
       " 763: 'modern',\n",
       " 764: 'rock',\n",
       " 765: 'ends',\n",
       " 766: \"who's\",\n",
       " 767: 'class',\n",
       " 768: 'earth',\n",
       " 769: 'kept',\n",
       " 770: 'brought',\n",
       " 771: 'imagine',\n",
       " 772: 'major',\n",
       " 773: 'third',\n",
       " 774: 'nearly',\n",
       " 775: 'sequence',\n",
       " 776: 'atmosphere',\n",
       " 777: 'bunch',\n",
       " 778: 'theme',\n",
       " 779: 'reviews',\n",
       " 780: 'christmas',\n",
       " 781: 'whom',\n",
       " 782: 'expected',\n",
       " 783: 'fantastic',\n",
       " 784: 'hate',\n",
       " 785: 'opening',\n",
       " 786: 'easily',\n",
       " 787: 'check',\n",
       " 788: 'imdb',\n",
       " 789: 'doubt',\n",
       " 790: 'enjoyable',\n",
       " 791: 'tells',\n",
       " 792: 'baby',\n",
       " 793: 'appears',\n",
       " 794: 'sounds',\n",
       " 795: 'career',\n",
       " 796: 'laughs',\n",
       " 797: 'writers',\n",
       " 798: 'air',\n",
       " 799: 'nature',\n",
       " 800: 're',\n",
       " 801: 'cheap',\n",
       " 802: 'dream',\n",
       " 803: 'sadly',\n",
       " 804: 'easy',\n",
       " 805: 'emotional',\n",
       " 806: 'within',\n",
       " 807: 'talking',\n",
       " 808: 't',\n",
       " 809: 'ways',\n",
       " 810: 'knew',\n",
       " 811: 'begins',\n",
       " 812: 'four',\n",
       " 813: 'five',\n",
       " 814: 'effort',\n",
       " 815: 'viewers',\n",
       " 816: 'silly',\n",
       " 817: 'material',\n",
       " 818: 'comment',\n",
       " 819: 'knows',\n",
       " 820: 'meet',\n",
       " 821: 'stand',\n",
       " 822: 'parents',\n",
       " 823: 'difficult',\n",
       " 824: 'hot',\n",
       " 825: 'upon',\n",
       " 826: 'leads',\n",
       " 827: 'avoid',\n",
       " 828: 'points',\n",
       " 829: 'suspense',\n",
       " 830: 'french',\n",
       " 831: 'gone',\n",
       " 832: 'believable',\n",
       " 833: 'weird',\n",
       " 834: 'wasted',\n",
       " 835: 'review',\n",
       " 836: 'period',\n",
       " 837: 'dramatic',\n",
       " 838: 'season',\n",
       " 839: 'leading',\n",
       " 840: 'wrote',\n",
       " 841: 'books',\n",
       " 842: 'george',\n",
       " 843: 'order',\n",
       " 844: 'directors',\n",
       " 845: \"what's\",\n",
       " 846: 'directing',\n",
       " 847: 'hear',\n",
       " 848: 'portrayed',\n",
       " 849: 'famous',\n",
       " 850: 'using',\n",
       " 851: 'casting',\n",
       " 852: '8',\n",
       " 853: 'release',\n",
       " 854: 'working',\n",
       " 855: 'change',\n",
       " 856: 'problems',\n",
       " 857: 'among',\n",
       " 858: 'feels',\n",
       " 859: 'theater',\n",
       " 860: 'tried',\n",
       " 861: 'appear',\n",
       " 862: 'setting',\n",
       " 863: 'needs',\n",
       " 864: 'married',\n",
       " 865: 'deal',\n",
       " 866: 'sequences',\n",
       " 867: 'decided',\n",
       " 868: 'perfectly',\n",
       " 869: 'begin',\n",
       " 870: 'hair',\n",
       " 871: 'clear',\n",
       " 872: 'interested',\n",
       " 873: \"you've\",\n",
       " 874: 'space',\n",
       " 875: 'return',\n",
       " 876: 'poorly',\n",
       " 877: 'depth',\n",
       " 878: 'western',\n",
       " 879: 'box',\n",
       " 880: 'odd',\n",
       " 881: 'follow',\n",
       " 882: 'social',\n",
       " 883: 'girlfriend',\n",
       " 884: 'meant',\n",
       " 885: 'particular',\n",
       " 886: 'premise',\n",
       " 887: 'otherwise',\n",
       " 888: 'forced',\n",
       " 889: 'shame',\n",
       " 890: 'potential',\n",
       " 891: 'de',\n",
       " 892: 'worked',\n",
       " 893: 'eventually',\n",
       " 894: 'meets',\n",
       " 895: 'sister',\n",
       " 896: 'stage',\n",
       " 897: 'fantasy',\n",
       " 898: 'animals',\n",
       " 899: 'became',\n",
       " 900: 'previous',\n",
       " 901: 'needed',\n",
       " 902: 'whatever',\n",
       " 903: 'earlier',\n",
       " 904: 'stewart',\n",
       " 905: 'waiting',\n",
       " 906: 'brings',\n",
       " 907: 'cute',\n",
       " 908: 'agree',\n",
       " 909: 'failed',\n",
       " 910: 'showing',\n",
       " 911: 'result',\n",
       " 912: 'effect',\n",
       " 913: 'bought',\n",
       " 914: 'gay',\n",
       " 915: 'badly',\n",
       " 916: 'note',\n",
       " 917: 'creepy',\n",
       " 918: 'fire',\n",
       " 919: 'paul',\n",
       " 920: 'apart',\n",
       " 921: 'move',\n",
       " 922: 'mary',\n",
       " 923: 'general',\n",
       " 924: 'form',\n",
       " 925: 'average',\n",
       " 926: 'doctor',\n",
       " 927: 'co',\n",
       " 928: 'pathetic',\n",
       " 929: 'admit',\n",
       " 930: 'sci',\n",
       " 931: 'fi',\n",
       " 932: 'feature',\n",
       " 933: 'frank',\n",
       " 934: 'honestly',\n",
       " 935: 'rent',\n",
       " 936: 'ask',\n",
       " 937: 'greatest',\n",
       " 938: 'band',\n",
       " 939: 'unless',\n",
       " 940: 'crime',\n",
       " 941: 'bored',\n",
       " 942: 'society',\n",
       " 943: 'hands',\n",
       " 944: 'beauty',\n",
       " 945: 'basic',\n",
       " 946: 'kelly',\n",
       " 947: 'fighting',\n",
       " 948: 'features',\n",
       " 949: 'dance',\n",
       " 950: \"we're\",\n",
       " 951: 'filmed',\n",
       " 952: 'hardly',\n",
       " 953: '20',\n",
       " 954: 'romance',\n",
       " 955: 'members',\n",
       " 956: 'cartoon',\n",
       " 957: 'older',\n",
       " 958: 'male',\n",
       " 959: 'weak',\n",
       " 960: 'copy',\n",
       " 961: 'ben',\n",
       " 962: 'free',\n",
       " 963: 'cop',\n",
       " 964: 'science',\n",
       " 965: 'certain',\n",
       " 966: 'monster',\n",
       " 967: 'write',\n",
       " 968: 'buy',\n",
       " 969: 'expecting',\n",
       " 970: 'situation',\n",
       " 971: 'surprise',\n",
       " 972: 'cat',\n",
       " 973: 'team',\n",
       " 974: 'consider',\n",
       " 975: 'state',\n",
       " 976: 'soundtrack',\n",
       " 977: 'credits',\n",
       " 978: 'actual',\n",
       " 979: 'footage',\n",
       " 980: 'break',\n",
       " 981: 'trash',\n",
       " 982: 'joe',\n",
       " 983: 'tom',\n",
       " 984: 'cheesy',\n",
       " 985: 'crazy',\n",
       " 986: 'front',\n",
       " 987: 'control',\n",
       " 988: 'business',\n",
       " 989: 'deserves',\n",
       " 990: 'decide',\n",
       " 991: 'sexual',\n",
       " 992: 'dumb',\n",
       " 993: 'present',\n",
       " 994: 'forward',\n",
       " 995: 'zombie',\n",
       " 996: 'sees',\n",
       " 997: 'joke',\n",
       " 998: 'attempts',\n",
       " 999: 'recently',\n",
       " 1000: '15',\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8006, 30419)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_2.wv.key_to_index) , len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important difference between the number of words in the train sentences and in the Word2Vec vocabulary, even though it has been train on the train sentence set. The reasons comes from the second important hyperparameter of Word2Vec :  `min_count`. \n",
    "\n",
    "`min_count` is a integer that tells you how many occurences a given word should have to be learn in the embedding space. For instance, let's say that the word \"movie\" appears 1000 times in the corpus and \"simba\" only 2 times. If `min_count=3`, the word \"simba\" will be skipped during the training.\n",
    "\n",
    "The intention is to have only words that are sufficiently present in the corpus to have a robust embedded representation\n",
    "\n",
    "❓ **Question** ❓ Learn a new `word2vec_3` model with a `min_count` higher than 5 (which is the default value) and a `word2vec_4` with a `min_count` smaller than 5, and then, compare the size of the vocabulary for all the different word2vec that you have trained (you can choose any `vector_size` you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "word2vec_3 = Word2Vec(X_train, vector_size=30, min_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8006, 9584)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_2.wv.key_to_index),len(word2vec_3.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we say that word2vec has an internal neural network that it optimizes based on some predictions? These predictions actually correspond to predicting a word based on surrounding words. The surroundings words are in a `window` which corresponds to the number of words taken into account. And you can train the word2vec with different `window` sizes.\n",
    "\n",
    "❓ **Question** ❓ Learn a new `word2vec_5` model with a `window` different than previously (default is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "word2vec_5 = Word2Vec(X_train, vector_size=30, min_count=4, window=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments you have seen (`vector_size`, `min_count` and `window`) are usually the one that you should start changing to get a better performance for your model.\n",
    "\n",
    "But you can also look at other arguments in the [documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Text8Corpus)\n",
    "\n",
    "\n",
    "\n",
    "# Convert our train and test set to RNN ready data\n",
    "\n",
    "Remember that word2vec is the first step to the overall process of feeding such a representation into a RNN, as shown here :\n",
    "\n",
    "<img src=\"word2vec_representation.png\" width=\"400px\" />\n",
    "\n",
    "\n",
    "\n",
    "Now, let's work on Step 2 by converting the training and test data into their vector representation to be ready to be feed in RNNs.\n",
    "\n",
    "❓ **Question** ❓ Now, write a function that, given a sentence, returns a matrix that corresponds to the embedding of the full sentence, which means that you have to embed each word one after the other and concatenate the result to output a 2D matrix (be sure that your output is a NumPy array)\n",
    "\n",
    "❗ **Remark** ❗ You will probably notice that some words you are trying to convert throw errors as they are said not to belong to the dictionary:\n",
    "\n",
    "- for the test set, this is understandable: some words were not in the train set and thus their embedded representation is unknown\n",
    "- for the train set, due to `min_count` hyperparameter, not all the words have a vector representation\n",
    "\n",
    "In any case, just skip the missing words here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"laaaaame\" in list(word2vec.wv.key_to_index.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "example = ['this', 'movie', 'is', 'the', 'worst', 'action', 'movie', 'ever']\n",
    "example_missing_words = ['this', 'movie', 'is', 'laaaaaaaaaame']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    X = []\n",
    "    #list_of_word = text_to_word_sequence(sentence)\n",
    "    for word in sentence:\n",
    "        if word in list(word2vec.wv.key_to_index.keys()):\n",
    "            X.append(word2vec.wv[word])\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "### Checks\n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (8, 100))\n",
    "\n",
    "embedded_sentence_missing_words = embed_sentence(word2vec, example_missing_words)  \n",
    "assert(type(embedded_sentence_missing_words) == np.ndarray)\n",
    "assert(embedded_sentence_missing_words.shape == (3, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sentence(word2vec, example_missing_words).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function that, given a list of sentence (each sentence being a list of words/strings), returns a list of embedded sentences (each sentence is a matrix). Apply this function to the train and test sentences\n",
    "\n",
    "Hint: Use the previous function `embed_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    return [embed_sentence(word2vec, sentence) for sentence in sentences]\n",
    "    \n",
    "X_train = embedding(word2vec, X_train)\n",
    "X_test = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 110)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0]), len(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ In order to have ready-to-use data, do not forget to pad them in order to have tensors that can be divided in batch sizes during the optimization. Store the padedd values in `X_train_pad` and `X_test_pad`. Do not forget the important arguments of the padding ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train, maxlen = 200, dtype='float32', padding = 'post')\n",
    "X_test_pad = pad_sequences(X_test, maxlen = 200, dtype='float32', padding = 'post')\n",
    "\n",
    "assert(len(X_train_pad.shape) == 3)\n",
    "assert(len(X_test_pad.shape) == 3)\n",
    "assert(X_train_pad.shape[2] == 100)\n",
    "assert(X_test_pad.shape[2] == 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
