{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and objectives:\n",
    "\n",
    "In this notebook, we propose to define a simple baseline cnn, train it and evaluate it on cifar-10 dataset. This dataset consists of 32x32 images of 10 different categories. \n",
    "\n",
    " We will then introduce data augmentation: a method designed to arbitrarily augment the training data set by computing perturbations of the train images (random crops, intensity changes etc) and show how it allows to improve the generalization.\n",
    "\n",
    "For now, we stick to computations on CPU, but bear in mind that a model training takes ~10 minutes on CPU in this notebook, so don't waste your trainings !\n",
    "\n",
    "When you reach the end of the data augmentation, please go to exercise 2 and 3 before coming back to dropout and batch normalization.\n",
    "\n",
    "\n",
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we load the data using the keras utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = 10\n",
    "(x_train, labels_train), (x_test, labels_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the maximum of x_train and x_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the image intensities by 255. Why are we doing that ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a few images (using `plt.imshow(img_array)`) and the corresponding classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many images are there ? What sizes have the images ? What sizes have the labels ? How do you deal with these categorical labels ? Convert the labels appropriately into `y_train` and `y_test` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the training data `x_train` and `y_train` into `x_val`, `x_train`, `y_val`, `y_train`: the validation set should contain the last 25000 images and the training set should contain the first 25000 images of the original training set returned by `keras.cifar10`. Print the shapes of all your data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a convolution network model to classify these images, using 3 convolution layers. Try to keep a low number of parameters (say < 50000 to keep fast experiments). Use `model.summary()` to see details about the different layers of your model.\n",
    "See https://keras.io/layers/convolutional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # model.add...\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile your model. What is the appropriate loss function ? What other metrics can you use to monitor training ? Use Adam with a learning rate of 1e-3 as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an EarlyStopping criterion on the validation loss evaluated on the validation data. (`restore_best_weights=True` as option to the EarlyStopping and a patience of no more than `5`to keep fast computations). In principle, do you think it is more advisable to define the stopping criterion on the  loss or on the accuracy metric ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model, specify the validation data using the `validation_data` keyword (so that everyone has the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), callbacks=[es], epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we provide a utility to plot histories from the estimation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('BCE loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `plot_history` to plot the previous history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your model on the test data. Are you satisfied with these performances ? What is the chance level on this task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the test set, find the images for which the true label was not among the 3 top labels predicted by the model. Looking at the largest errors of a model is a good way to understand what is happening and to obtain ideas for improvement. (hard question, ask for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding data augmentation\n",
    "A common idea in deep learning is the notion of data augmentation. To make the network more resilient to new unseen data, we enrich, at each epoch and on the fly, the training set by modifications of the input images. The augmentations are multiple and problem-dependent: gaussian noise addition, contrast change, random translation, random crop, horizontal and vertical flip, elastic transform... All these operations can be achieved using an ImageDataGenerator object from the keras package: https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is an image data generator. Call its `fit` method on the appropriate data.\n",
    "See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=(0.8, 1.),\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    rescale=1./255,)\n",
    "\n",
    "datagen.fit(??????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the first 10 images of the train set, plot the original image and a modification of this image using the data generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declaring the generator flow\n",
    "viz_flow = datagen.flow(x_train, shuffle=False, batch_size=1)\n",
    "\n",
    "# plot images and their augmented versions by the generator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model using these augmented data, store the history in `history_data_aug`. Do you think the estimation will be faster in terms of number of epochs ? in terms of computational time ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "\n",
    "model_data_aug = build_model()\n",
    "\n",
    "model.compile(????)\n",
    "\n",
    "# Defining the iterators from the generator:\n",
    "train_flow = datagen.flow(x_train, y_train, batch_size=16)\n",
    "val_flow = datagen.flow(x_val, y_val, batch_size=128, shuffle=False)\n",
    "\n",
    "# Stopping criterion\n",
    "es = EarlyStopping(????)\n",
    "\n",
    "history_data_aug = model_data_aug.fit_generator(????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the history information of the estimation, compare with the first run without data augmentation (on the same plot). Is the training faster or slower (in terms of number of epochs) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the test data. Do you see an improvement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout (GO TO NOTEBOOK 2 and 3 before coming back here!)\n",
    "Dropout is a technique to regularize a neural network and prevent overfitting. A Dropout layer with parameter $0 < p < 1$ randomly sets activations to zero. \n",
    "#### Add dropout layers to your model, with `p=0.1`, after each Conv2D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_with_dropout():\n",
    "    model = Sequential()\n",
    "    \n",
    "    ###\n",
    "    ### TO COMPLETE\n",
    "    ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dropout = build_model_with_dropout()\n",
    "model_dropout.compile(????)\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate your model on the data (without augmentation), with the same EarlyStopping criterion as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(????)\n",
    "history_dropout = model_dropout.fit(????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the history of the accuracies and loss, and compare the baseline run. Comments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate your trained model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch normalization\n",
    "#### As before, code a model with batch normalization after each Conv2D (don't forget to specify the right axis!), train it, plot the history and evaluate the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "def build_model_with_batch_norm():\n",
    "    model = Sequential()\n",
    "    \n",
    "    ####\n",
    "    #### TO COMPLETE\n",
    "    ####\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### declare optimizer, compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### fit and store history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### evaluate on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remarks\n",
    " 1. In these experiments, we stopped training quickly to have fast experiments. In practice, training must be allowed to last longer, with a stricter stopping criterion (lower delta and higher patience)\n",
    " 2. The usage of a specific data augmentation, batch norm or dropout should be motivated by the data and by the results (properly estimated). In practice, the best method is to code a proper cross-validation (e.g. 10-fold) coupled with a grid search on all the possible hyper parameters (dropout probabilities, amplitudes of data augmentation, learning rate...) and to keep the best performing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
