{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Day 2 - Exercise 3\n",
    "\n",
    "Now that you have mastered almost every part of Neural Networks, let's take a closer look at the `compile` part.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will here use the data from the Boston Housing dataset\n",
    "\n",
    "❓ **Question** ❓ First, load the data with the appropriate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To get a sense of a benchmark score you have to beat, what is the mean absolute error on the test set if your prediction corresponds to the mean value of $y$ computed on the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: The model\n",
    "\n",
    "❓ **Question** ❓ Now, write in a function `initialize_model` a neural network that has 3 layers: \n",
    "- a layer with 10 neurons and the `relu` activation function (appropriate input dimension)\n",
    "- a layer with 7 neurons and the `relu` activation function\n",
    "- an appropriate layer corresponding to the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : The optimizer\n",
    "\n",
    "❓ **Question** ❓ Write a function that takes as argument a model and a name of optimizer, that compiles the model and returns it - select the loss and metrics wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Initialize the model, compile it with the `adam` optimizer and fit it on the data. Evaluate your model on the test data.\n",
    "\n",
    "Do not forget to use an early stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Rerun the same model on the same data but with different optimizer (in a `for` loop). For each, plot the history and report the corresponding Mean Absolute Error. (see [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('Model Mean Absolute Error')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Are your predictions better than the benchmark prediction you evaluate at the beginning of the notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ Here, the optimizer is not central as the data are in low dimension and not many samples. However, in practice, you are advised to start with the `adam` optimizer by default which often works best. \n",
    "\n",
    "\n",
    "❗ **Remark** ❗ Internally, when you call any optimizer with a string, the neural network initializes the hyperparameters the optimizer rely on. Among this hyperparameters, there is quite an important one, the learning rate. This learning rate corresponds to the intensity of change of the weights at each optimization of the neural network. Different learning rates have different consequences, as shown here : \n",
    "\n",
    "<img src=\"learning_rate.png\" alt=\"Learning rate\" style=\"height:350px;\"/>\n",
    "\n",
    "\n",
    "As the learning rate is initialized with default values when you compile the model optimizer with a string, let's see how to do it differently.\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Instead of initializing the optimizer with a string, we will initialize a real optimizer directly. Look at the documentation of [adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and instanciate it with a learning rate of 0.1 - keep the other values to their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Use this optimizer in the `compile_model` function, fit the data and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, reproduce the same plots and results but for different learning rates.\n",
    "\n",
    "Remark: There is a chance that the y-axis is too large for you to visualize the results. In that case, rewrite the plot function to plot only the epochs > 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history, n_epochs):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced & Optional\n",
    "\n",
    "The next question is optional and intended for advanced users that are willing to go a step further in their Deep Learning skills. However, it is not essential and can be skiped as many algorithms can be run without such optimization. \n",
    "\n",
    "Instead of keeping a fixed learning rate, you can change it from one iteration to the other, with the intuition that you first need large learning rates, and as the neural network converges and get closer to the minimum, you decrease the value of the learning rate. This is called a scheduler. \n",
    "\n",
    "❓ **Question** ❓ Use the [exponential decay scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) in the adam optimizer and run it on the previous data. Plot the history and check how the loss and mae evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : The loss\n",
    "\n",
    "It is important to clearly understand the different between metrics and loss. The loss are part of the metrics, therefore some metrics can be used as loss.\n",
    "\n",
    "❓ **Question** ❓ Run the same neural network, once with the `mae` as the loss, and once with the `mse`. In both case, report the final mean absolute error and compare the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you see that even if the Mean Absolute Error is the final metric you optimize, it can be better for your algorithm to use the mean square error as the loss to optimize its weights and give the best MAE possible. \n",
    "\n",
    "# Part IV : Save and load a model\n",
    "\n",
    "Here, we will see how to save a model, and load it afterwards.\n",
    "\n",
    "❓ **Question** ❓ Rerun any model you want and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, save the model thanks to the `save_model` method that you can find [here](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, in a variable `loaded_model`, load the model you just saved thanks to the `load_model` [(documentation here)](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model), and evaluate it on the test data to see that it gives the same result as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V : Get the best Mean Absolute Error\n",
    "\n",
    "From here, you are free to:\n",
    "- design your own architecture: number of layers, number of neurons, activation function\n",
    "- choose the loss and metric\n",
    "- choose the optimizer and tune its hyperparameters\n",
    "- select the best options for the early stopping criterion\n",
    "\n",
    "to get the best mean absolute error.\n",
    "\n",
    "Once you have your best score on the test set, include the whole pipeline into a proper cross validation to report your score (mean ± std).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
