{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reboot\n",
    "\n",
    "In this reboot, we will go through all the building blocks of Neural Networks that will be used until the end of the week.\n",
    "\n",
    "### Generate data\n",
    "\n",
    "❓Generate random input data. You should have 1000 samples, each of size 10. You can here consider integers between 0 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.randint(0, 10, size=(1000, 10))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Generate random output data ; we here consider a regression task of size 1 for each sample where values are integers between 1 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(1, 100, size=(1000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the general architecture of your model\n",
    "\n",
    "❓ Import Keras and declare a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Add 3 dense layers to your model with you favorite number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(5, input_dim=10, activation='sigmoid'))\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Add a last layer that suits your regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Check the number of parameters of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                72        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 39        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define how your model is trained\n",
    "\n",
    "❓ Define the optimizer, the loss and the metrics of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓How is the loss different from the metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓Run the model on this random data, with a validation set of 30% of the data ; don't forget to select a number of epochs and a batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 1s 984us/sample - loss: 3368.2109 - mae: 50.4327 - val_loss: 3328.9716 - val_mae: 49.8831\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 67us/sample - loss: 3334.3758 - mae: 50.0931 - val_loss: 3289.5347 - val_mae: 49.4874\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 64us/sample - loss: 3292.7198 - mae: 49.6748 - val_loss: 3245.9016 - val_mae: 49.0446\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 3248.1021 - mae: 49.2242 - val_loss: 3203.0987 - val_mae: 48.6105\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 3209.1788 - mae: 48.8348 - val_loss: 3169.3806 - val_mae: 48.2655\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 66us/sample - loss: 3178.0399 - mae: 48.5191 - val_loss: 3141.0802 - val_mae: 47.9807\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s 83us/sample - loss: 3151.2115 - mae: 48.2497 - val_loss: 3116.1815 - val_mae: 47.7334\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 3126.9538 - mae: 48.0031 - val_loss: 3092.8337 - val_mae: 47.5003\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 3104.1018 - mae: 47.7699 - val_loss: 3070.8350 - val_mae: 47.2795\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 3082.3318 - mae: 47.5472 - val_loss: 3049.4481 - val_mae: 47.0664\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 3061.2014 - mae: 47.3350 - val_loss: 3028.9086 - val_mae: 46.8627\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 3040.6313 - mae: 47.1282 - val_loss: 3008.9033 - val_mae: 46.6634\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 59us/sample - loss: 3020.5703 - mae: 46.9245 - val_loss: 2989.0275 - val_mae: 46.4644\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 59us/sample - loss: 3000.7984 - mae: 46.7248 - val_loss: 2969.6933 - val_mae: 46.2700\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2981.3576 - mae: 46.5286 - val_loss: 2950.7565 - val_mae: 46.0788\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2962.2298 - mae: 46.3380 - val_loss: 2931.9140 - val_mae: 45.8876\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2943.3583 - mae: 46.1469 - val_loss: 2913.1326 - val_mae: 45.6962\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2924.6939 - mae: 45.9577 - val_loss: 2894.6350 - val_mae: 45.5068\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 66us/sample - loss: 2906.1856 - mae: 45.7699 - val_loss: 2876.6017 - val_mae: 45.3214\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2887.9148 - mae: 45.5876 - val_loss: 2858.7769 - val_mae: 45.1413\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2869.8996 - mae: 45.4070 - val_loss: 2840.7494 - val_mae: 44.9583\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2851.9621 - mae: 45.2278 - val_loss: 2823.0237 - val_mae: 44.7776\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2834.2153 - mae: 45.0490 - val_loss: 2805.4754 - val_mae: 44.5978\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 2816.5963 - mae: 44.8719 - val_loss: 2788.2022 - val_mae: 44.4201\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2799.1619 - mae: 44.6960 - val_loss: 2771.0129 - val_mae: 44.2425\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 65us/sample - loss: 2781.8705 - mae: 44.5239 - val_loss: 2753.8916 - val_mae: 44.0648\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2764.6909 - mae: 44.3519 - val_loss: 2736.9653 - val_mae: 43.8883\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2747.6774 - mae: 44.1826 - val_loss: 2720.0820 - val_mae: 43.7115\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 2730.7705 - mae: 44.0119 - val_loss: 2703.4073 - val_mae: 43.5361\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2714.0014 - mae: 43.8424 - val_loss: 2686.8045 - val_mae: 43.3639\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2697.3077 - mae: 43.6788 - val_loss: 2670.4834 - val_mae: 43.1945\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 2680.8355 - mae: 43.5111 - val_loss: 2653.9751 - val_mae: 43.0224\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 2664.3538 - mae: 43.3475 - val_loss: 2637.8561 - val_mae: 42.8535\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 68us/sample - loss: 2648.0463 - mae: 43.1837 - val_loss: 2621.8152 - val_mae: 42.6848\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2631.8600 - mae: 43.0196 - val_loss: 2605.8014 - val_mae: 42.5200\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2615.8215 - mae: 42.8616 - val_loss: 2589.7023 - val_mae: 42.3568\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2599.7430 - mae: 42.7024 - val_loss: 2574.1912 - val_mae: 42.1988\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2583.9345 - mae: 42.5480 - val_loss: 2558.4336 - val_mae: 42.0376\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2568.1367 - mae: 42.3911 - val_loss: 2542.9256 - val_mae: 41.8782\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2552.4790 - mae: 42.2359 - val_loss: 2527.4095 - val_mae: 41.7191\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2536.8914 - mae: 42.0836 - val_loss: 2512.0371 - val_mae: 41.5634\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2521.4026 - mae: 41.9347 - val_loss: 2496.7969 - val_mae: 41.4083\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2506.0255 - mae: 41.7878 - val_loss: 2481.6004 - val_mae: 41.2530\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2490.7101 - mae: 41.6398 - val_loss: 2466.6052 - val_mae: 41.0990\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2475.5405 - mae: 41.4926 - val_loss: 2451.5311 - val_mae: 40.9435\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2460.4756 - mae: 41.3456 - val_loss: 2436.4116 - val_mae: 40.7922\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 64us/sample - loss: 2445.3983 - mae: 41.2031 - val_loss: 2421.7339 - val_mae: 40.6449\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2430.4997 - mae: 41.0593 - val_loss: 2407.0511 - val_mae: 40.4968\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 64us/sample - loss: 2415.6662 - mae: 40.9174 - val_loss: 2392.4916 - val_mae: 40.3493\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 66us/sample - loss: 2400.9594 - mae: 40.7741 - val_loss: 2377.8790 - val_mae: 40.2006\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2386.3076 - mae: 40.6325 - val_loss: 2363.3902 - val_mae: 40.0535\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2371.7665 - mae: 40.4910 - val_loss: 2348.9370 - val_mae: 39.9068\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2357.2272 - mae: 40.3527 - val_loss: 2334.9038 - val_mae: 39.7638\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2342.8861 - mae: 40.2153 - val_loss: 2320.7337 - val_mae: 39.6186\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2328.6081 - mae: 40.0750 - val_loss: 2306.5283 - val_mae: 39.4724\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2314.3961 - mae: 39.9358 - val_loss: 2292.4272 - val_mae: 39.3270\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 61us/sample - loss: 2300.2598 - mae: 39.8002 - val_loss: 2278.4969 - val_mae: 39.1859\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 2286.2411 - mae: 39.6644 - val_loss: 2264.5972 - val_mae: 39.0445\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2272.2342 - mae: 39.5271 - val_loss: 2251.0704 - val_mae: 38.9061\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2258.4185 - mae: 39.3948 - val_loss: 2237.3534 - val_mae: 38.7652\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2244.6173 - mae: 39.2594 - val_loss: 2223.7817 - val_mae: 38.6250\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2230.9648 - mae: 39.1242 - val_loss: 2210.1097 - val_mae: 38.4879\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2217.3002 - mae: 38.9951 - val_loss: 2196.7465 - val_mae: 38.3558\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 59us/sample - loss: 2203.7817 - mae: 38.8624 - val_loss: 2183.3815 - val_mae: 38.2230\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 59us/sample - loss: 2190.3067 - mae: 38.7341 - val_loss: 2170.1912 - val_mae: 38.0913\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2176.9809 - mae: 38.6055 - val_loss: 2156.8383 - val_mae: 37.9573\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2163.6433 - mae: 38.4759 - val_loss: 2143.7763 - val_mae: 37.8258\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2150.4013 - mae: 38.3474 - val_loss: 2130.8957 - val_mae: 37.6970\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2137.3113 - mae: 38.2212 - val_loss: 2117.8251 - val_mae: 37.5657\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2124.2436 - mae: 38.0934 - val_loss: 2104.8678 - val_mae: 37.4348\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2111.2398 - mae: 37.9665 - val_loss: 2092.0988 - val_mae: 37.3052\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 64us/sample - loss: 2098.3576 - mae: 37.8411 - val_loss: 2079.3465 - val_mae: 37.1751\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2085.5097 - mae: 37.7149 - val_loss: 2066.7987 - val_mae: 37.0486\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 2072.7626 - mae: 37.5966 - val_loss: 2054.3173 - val_mae: 36.9235\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2060.0958 - mae: 37.4703 - val_loss: 2041.8855 - val_mae: 36.7983\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 64us/sample - loss: 2047.5040 - mae: 37.3508 - val_loss: 2029.4725 - val_mae: 36.6726\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 2035.0439 - mae: 37.2298 - val_loss: 2016.8831 - val_mae: 36.5445\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2022.5069 - mae: 37.1066 - val_loss: 2004.8500 - val_mae: 36.4216\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 2010.2134 - mae: 36.9874 - val_loss: 1992.5382 - val_mae: 36.3022\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 64us/sample - loss: 1997.8585 - mae: 36.8669 - val_loss: 1980.6267 - val_mae: 36.1861\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 1985.7115 - mae: 36.7485 - val_loss: 1968.4533 - val_mae: 36.0667\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 1973.5661 - mae: 36.6307 - val_loss: 1956.4038 - val_mae: 35.9480\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 1961.4916 - mae: 36.5095 - val_loss: 1944.5477 - val_mae: 35.8306\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1949.5438 - mae: 36.3926 - val_loss: 1932.6821 - val_mae: 35.7164\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 63us/sample - loss: 1937.6238 - mae: 36.2727 - val_loss: 1921.0137 - val_mae: 35.6084\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1925.7863 - mae: 36.1583 - val_loss: 1909.4735 - val_mae: 35.5010\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 1914.0846 - mae: 36.0430 - val_loss: 1897.7744 - val_mae: 35.3916\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1902.3708 - mae: 35.9234 - val_loss: 1886.3276 - val_mae: 35.2839\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1890.7823 - mae: 35.8096 - val_loss: 1874.8764 - val_mae: 35.1756\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1879.2003 - mae: 35.6937 - val_loss: 1863.7373 - val_mae: 35.0711\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 1867.7727 - mae: 35.5818 - val_loss: 1852.4676 - val_mae: 34.9651\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1856.4179 - mae: 35.4682 - val_loss: 1841.0851 - val_mae: 34.8574\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 1845.0549 - mae: 35.3552 - val_loss: 1829.9850 - val_mae: 34.7518\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 1833.8156 - mae: 35.2421 - val_loss: 1818.9618 - val_mae: 34.6464\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 1822.6592 - mae: 35.1303 - val_loss: 1807.9074 - val_mae: 34.5406\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 62us/sample - loss: 1811.5698 - mae: 35.0162 - val_loss: 1796.8922 - val_mae: 34.4376\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 1800.5364 - mae: 34.9046 - val_loss: 1786.0212 - val_mae: 34.3353\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 59us/sample - loss: 1789.5588 - mae: 34.7914 - val_loss: 1775.3583 - val_mae: 34.2345\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 60us/sample - loss: 1778.7013 - mae: 34.6805 - val_loss: 1764.6474 - val_mae: 34.1326\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 61us/sample - loss: 1767.8887 - mae: 34.5684 - val_loss: 1753.9918 - val_mae: 34.0306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13b4b9ad0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          batch_size=16,\n",
    "          verbose=0\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping criterion\n",
    "\n",
    "❓ What is the early stopping criterion made for? Can you instantiate one with a patience of 20 (what does the patience corresponds to)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Estimate the model with the early stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 - 0s - loss: 824.9717 - mae: 24.7673 - val_loss: 840.6967 - val_mae: 25.1638\n",
      "Epoch 2/100\n",
      "800/800 - 0s - loss: 824.9565 - mae: 24.7673 - val_loss: 840.6942 - val_mae: 25.1633\n",
      "Epoch 3/100\n",
      "800/800 - 0s - loss: 824.9746 - mae: 24.7676 - val_loss: 840.6927 - val_mae: 25.1630\n",
      "Epoch 4/100\n",
      "800/800 - 0s - loss: 824.9300 - mae: 24.7668 - val_loss: 840.6917 - val_mae: 25.1627\n",
      "Epoch 5/100\n",
      "800/800 - 0s - loss: 824.9501 - mae: 24.7672 - val_loss: 840.6906 - val_mae: 25.1624\n",
      "Epoch 6/100\n",
      "800/800 - 0s - loss: 824.9106 - mae: 24.7668 - val_loss: 840.6894 - val_mae: 25.1618\n",
      "Epoch 7/100\n",
      "800/800 - 0s - loss: 824.9008 - mae: 24.7665 - val_loss: 840.6890 - val_mae: 25.1615\n",
      "Epoch 8/100\n",
      "800/800 - 0s - loss: 824.8949 - mae: 24.7667 - val_loss: 840.6881 - val_mae: 25.1609\n",
      "Epoch 9/100\n",
      "800/800 - 0s - loss: 824.8851 - mae: 24.7667 - val_loss: 840.6867 - val_mae: 25.1608\n",
      "Epoch 10/100\n",
      "800/800 - 0s - loss: 824.8526 - mae: 24.7658 - val_loss: 840.6859 - val_mae: 25.1608\n",
      "Epoch 11/100\n",
      "800/800 - 0s - loss: 824.8233 - mae: 24.7656 - val_loss: 840.6769 - val_mae: 25.1602\n",
      "Epoch 12/100\n",
      "800/800 - 0s - loss: 824.7589 - mae: 24.7650 - val_loss: 840.6515 - val_mae: 25.1606\n",
      "Epoch 13/100\n",
      "800/800 - 0s - loss: 824.5479 - mae: 24.7618 - val_loss: 840.6407 - val_mae: 25.1603\n",
      "Epoch 14/100\n",
      "800/800 - 0s - loss: 824.3262 - mae: 24.7602 - val_loss: 840.5018 - val_mae: 25.1639\n",
      "Epoch 15/100\n",
      "800/800 - 0s - loss: 823.7670 - mae: 24.7503 - val_loss: 840.4843 - val_mae: 25.1671\n",
      "Epoch 16/100\n",
      "800/800 - 0s - loss: 823.6085 - mae: 24.7458 - val_loss: 840.5577 - val_mae: 25.1694\n",
      "Epoch 17/100\n",
      "800/800 - 0s - loss: 823.3979 - mae: 24.7398 - val_loss: 840.5369 - val_mae: 25.1720\n",
      "Epoch 18/100\n",
      "800/800 - 0s - loss: 823.4130 - mae: 24.7423 - val_loss: 840.6711 - val_mae: 25.1788\n",
      "Epoch 19/100\n",
      "800/800 - 0s - loss: 822.9441 - mae: 24.7321 - val_loss: 840.7327 - val_mae: 25.1799\n",
      "Epoch 20/100\n",
      "800/800 - 0s - loss: 822.7561 - mae: 24.7306 - val_loss: 840.7549 - val_mae: 25.1781\n",
      "Epoch 21/100\n",
      "800/800 - 0s - loss: 822.2279 - mae: 24.7213 - val_loss: 840.9478 - val_mae: 25.1878\n",
      "Epoch 22/100\n",
      "800/800 - 0s - loss: 823.0485 - mae: 24.7409 - val_loss: 841.0782 - val_mae: 25.1885\n",
      "Epoch 23/100\n",
      "800/800 - 0s - loss: 822.0573 - mae: 24.7157 - val_loss: 841.2405 - val_mae: 25.1936\n",
      "Epoch 24/100\n",
      "800/800 - 0s - loss: 821.9301 - mae: 24.7136 - val_loss: 841.4504 - val_mae: 25.1994\n",
      "Epoch 25/100\n",
      "800/800 - 0s - loss: 821.6135 - mae: 24.7077 - val_loss: 842.2443 - val_mae: 25.2234\n",
      "Epoch 26/100\n",
      "800/800 - 0s - loss: 821.4475 - mae: 24.7039 - val_loss: 842.1013 - val_mae: 25.2152\n",
      "Epoch 27/100\n",
      "800/800 - 0s - loss: 821.5626 - mae: 24.7068 - val_loss: 842.5938 - val_mae: 25.2254\n",
      "Epoch 28/100\n",
      "800/800 - 0s - loss: 820.5574 - mae: 24.6847 - val_loss: 842.4926 - val_mae: 25.2205\n",
      "Epoch 29/100\n",
      "800/800 - 0s - loss: 820.3997 - mae: 24.6845 - val_loss: 842.7046 - val_mae: 25.2243\n",
      "Epoch 30/100\n",
      "800/800 - 0s - loss: 820.1996 - mae: 24.6762 - val_loss: 844.0528 - val_mae: 25.2610\n",
      "Epoch 31/100\n",
      "800/800 - 0s - loss: 819.9203 - mae: 24.6729 - val_loss: 844.0911 - val_mae: 25.2571\n",
      "Epoch 32/100\n",
      "800/800 - 0s - loss: 820.0834 - mae: 24.6819 - val_loss: 843.4597 - val_mae: 25.2384\n",
      "Epoch 33/100\n",
      "800/800 - 0s - loss: 819.8360 - mae: 24.6653 - val_loss: 844.1308 - val_mae: 25.2529\n",
      "Epoch 34/100\n",
      "800/800 - 0s - loss: 819.1613 - mae: 24.6603 - val_loss: 844.5437 - val_mae: 25.2613\n",
      "Epoch 35/100\n",
      "800/800 - 0s - loss: 819.1916 - mae: 24.6600 - val_loss: 844.4124 - val_mae: 25.2570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13bcb4310>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          batch_size=16,\n",
    "          verbose=2, \n",
    "          callbacks=[es]\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General questions\n",
    "\n",
    "❓What should there be a train/test split additionnaly to the validation set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓How do you select the loss of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
