{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two moons dataset\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If one of the following package is not installed, write, in a shell terminal : \n",
    "\n",
    "`pip install name_of_the_package`\n",
    "\n",
    "For instance, you will probably install Keras thanks to `pip install tensorflow`, Tensorflow being the package that manage the latest Keras version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Classification Exercise\n",
    "\n",
    "In this notebook, you will create 2D data that corresponds to two _moons_: each moon corresponds to a given number of samples, each being a 2D input. Each moon, i.e. all the points of the moon, corresponds to a label, `0` or `1`. These moons are more or less noisy, depending on a `noise` parameters, and the corresponding labels cannot be linearly separated. \n",
    "\n",
    "Here is an example of such moons : \n",
    "\n",
    "![Two moons](moons_example.png)\n",
    "\n",
    "To separate the moons, you will build your first Neural Network that takes as input the 2D values of each point and outputs a label, `0` or `1`. In this notebook, we will go progressively through different aspects of the neural network architecture and training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data\n",
    "\n",
    "Here, we will use the scikit-learn `make_moons` function [(see documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) that creates 2 moons that cannot be linearly separated. Each moon correspond to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data generation\n",
    "X, y = make_moons(n_samples=300, noise=0.25, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Write a `plot_moons` function\n",
    "\n",
    "\n",
    "The 0 should be colored in red and the 1 should be colors in blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_moons(X, y):\n",
    "   ### TO DO \n",
    "\n",
    "plot_moons(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Split the initial dataset into a train and test set (size: 70/30%)\n",
    "\n",
    "Remark : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model\n",
    "\n",
    "Once the data is set, we will initialize your first Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \n",
    "    ### First: `Sequential` informs the model that it will be initialize by \"Sequential\" model, i.e. a stack of layers \n",
    "    ### (These layers are not defined yet)\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    ### Second: The first layer is a fully connected layer (=Dense), i.e. all the outputs of the k-th layer\n",
    "    ### are considered as inputs of the (k+1)-th layer\n",
    "    ### The input dimension is the dimention of the input data. The output is of size 500\n",
    "    model.add(layers.Dense(500, input_dim=2, activation='relu'))\n",
    "    \n",
    "    ### Third: The second layer is another fully connected layer. We only give the output_size (being 1) as the\n",
    "    ### first \"Sequential\" instructions informed the model about the input size, which is the output size of the \n",
    "    ### previous layer. Moreover, the output_size is one because the output of the neural network is a class, 0 or 1.\n",
    "    ### In fact, it is more than a class : a probabiliy of belonging to class 1, so that it is a continuous output\n",
    "    ### between 0 and 1. This is why the last activation function is a sigmoid function, whose outputs are always\n",
    "    ### between 0 and 1.\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    ### Fourth : We defined the parameters that are used to optimize and train the NN.\n",
    "    ### - the loss corresponds to the measure that the model optimize\n",
    "    ### - the optimizer actually updates the parameters based on the loss value\n",
    "    ### - the optional `metrics` argument corresponds to other metrics that are computed along with the loss\n",
    "    ### For instance, you might use the binary cross-entropy to train, but at the end of the day, you want to \n",
    "    ### know how many correctly classified samples you have, i.e. the accuracy\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Why is it important to re-initialize the model (by calling the function `initialize_model`) each time you want to run the model, rather than initializing it once and training it for different data (X_train, y_train)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer : \n",
    "\n",
    "        => ### TO DO \n",
    "<hr><hr>\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "Reminder : One step of parameter update is \"calledâ€œ a _backpropagation_ as you _propagate_ a measure of the error back to your network, from the output to the input. It is done by evaluating the prediction on a set of `N` data, called the batch. `N` is thus the batch size. One iteration is when the updates has been made considering all the batches, i.e. it went through all the data once, and only once.\n",
    "\n",
    "This is an example of the model fitting, with :\n",
    "- training data (input and output)\n",
    "- a validation set that corresponds to unused data for training but on which the model compute some estimation to see its generalization\n",
    "- the epochs, i.e. the number of iterations \n",
    "- a batch_size\n",
    "- verbose: commonly used arguments to output some logs. It usually goes from 0 (no logs) to greated numbers, each being associated to a certain amount of logs.\n",
    "\n",
    "### Question : Run the following commands with different verbose value and be sure you understand the meaning of the different logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=60, \n",
    "                    batch_size=10,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on the train set with the following command. The results contains the list of evaluated values that are, first, the loss (here, the binary_crossentropy) and then, the list of metrics that were listed in the `metrics` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss: {} - Train accuracy (MAE): {}'.format(results[0], results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : There are (at least) two important reasons why this accuracy cannot be considered as \"good\" (idependently of its value), what are they? \n",
    "\n",
    "_Hint_ : You can plot the following cell, which plots the loss of the train and validation set (which here is the test set), during the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer: \n",
    "\n",
    "        => ### TO DO \n",
    "\n",
    "### Question Write some `evaluation` to remedy to one of these reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : According to the plot, what can you see about the relative positioning of the train and test set? Is this good or bad? How is it related to overfitting, what is its affect and how to remedy it? Could you guess what might be the reason of the overfitting, if any?\n",
    "\n",
    "##### Answer : \n",
    "\n",
    "            => ### TO DO \n",
    "\n",
    "<hr><hr>\n",
    "\n",
    "To prevent the model from overfitting, one of the easy technique is called the Early stopping criterion. It uses a part of the dataset, called the validation set (which is not used to update the model parameters) to check if the loss, on this validation set, is decreasing or not.\n",
    "\n",
    "The `verbose` argument is made to output some logs during the training. Set to 1, it outputs the iteration it stops as.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=400, \n",
    "                    batch_size=25,\n",
    "                    verbose=0, \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Why is it not a good idea to stop at the first iteration that is not improving anymore?\n",
    "\n",
    "###### Answer : \n",
    "\n",
    "\n",
    "            => ### TO DO \n",
    "\n",
    "<hr><hr>\n",
    "\n",
    "`EarlyStopping` has the `patience` argument which corresponds to the number of non-decreasing iterations that the algorithm waits until stopping the algorithm.\n",
    "\n",
    "### Question : Run the algorithm with a patience number between 30 and 50 and comment your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### Answer :\n",
    "\n",
    "### TO DO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Is this correct to report the final loss and accuracy based on the validation set? If yes, explain why? If no, what should you do?\n",
    "\n",
    "\n",
    "<hr>\n",
    "Take some time before reading the answer\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Answer : \n",
    "\n",
    "No, it is not correct because the validation set has been used to score the algorithm. Let's take the example of a validation set that has been biased during the split. Therefore, the whole algorithm is biased towards the data in this validation set.\n",
    "\n",
    "To this end, the result should be reported on another set. Finally, we will have : \n",
    "- a train set : used to update the parameters of the NN during the training\n",
    "- a validation set : used to check if the parameter updates are improving the prediction on a hold-out set\n",
    "- a test set : used to get a final estimation of the error on an unseen set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Construct a train, test and validation set\n",
    "\n",
    "_Hint_ : You can split the initial set into the train and test set, as previously. \n",
    "Then, in the `model.fit`, use the argument `validation split` [(See documentation)](https://keras.io/models/sequential/) to split a fraction of the training data to use the Early Criterion on. Then, report the value based on the test set (that should not have been used in the `model.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Write a function that, given the input X, the real output y and the predicted output y_pred plots where the wrong predictions are on the moons.\n",
    "\n",
    "Then, apply it to the previous test set. Print :\n",
    "- in grey the good prediction\n",
    "- in red the 0 poorly predicted\n",
    "- in blue the 1 poorly predicted\n",
    "\n",
    "_Hints_: \n",
    "- you can obtain `y_pred` with the `predict` function of the `model` object.\n",
    "- `np.round` rounds the probability of `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(X, y, y_pred):\n",
    "   \n",
    "    ### TO DO \n",
    "   \n",
    "    \n",
    "y_pred = model.predict(X_test)\n",
    "plot_errors(X_test, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moons(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question/Answer/Remark : \n",
    "\n",
    "You might say that reporting one test value is not correct as we should do a proper K-fold cross-validation. This is perfectly correct. As the stream within each fold is the same, we consider that you can do the K-fold on your own in a real setting. However, here, training K algorithm each time is too long to be done within the allocated time (For your information, some Facebook/Google Neural networks are trained for weeks on heavy distributed computers).\n",
    "\n",
    "\n",
    "<hr><hr>\n",
    "\n",
    "### Question : Write a function that, given data (X,y) simulated thanks to `make_moons` and given a batch_size,   does : \n",
    "\n",
    "- split the data into train and test (70%/30%) - deterministic split, no shuffle, as in the first example\n",
    "- initialize a model\n",
    "- initialize a early stopping criterion (with `patience = 20`)\n",
    "- fit the model with a `validation_split=0.7`\n",
    "- return the loss and accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, y, batch_size):    \n",
    "    ### Data split\n",
    "    # TO DO\n",
    "    \n",
    "    ### Model initialization\n",
    "    # TO DO\n",
    "    \n",
    "    ### Early stopping criterion\n",
    "    # TO DO\n",
    "    \n",
    "    ### Fitting the model \n",
    "    # TO DO\n",
    "    \n",
    "    ### Evaluating the model\n",
    "    # TO DO\n",
    "    \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Simulate data (X, y) with 400 samples and noise equal to 0.20 (with `make_moons`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Launch `run_model` for different values of the batch_size, from 1 to 50. Represent the `loss` and `accuracy` on a graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "batch_sizes = [1, 5, 10, 15, 20, 50]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot\n",
    "\n",
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : What is your conclusion regarding the batch_size? Why same accuracy might correspond to different loss values?\n",
    "\n",
    "##### Answer : \n",
    "\n",
    "            => ### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Write a function `initialize_model_2` \n",
    "\n",
    "This function is intended to be exactly the same as the `initialize_model` *EXCEPT* for the `activation` of the first layer that is an argument `act_function` of the function. \n",
    "\n",
    "\n",
    "! Warning ! only for the first layer, not the sigmoid which is used to get a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2(act_function):\n",
    "    \n",
    "    #### First: initialize a \"Sequential\" model, i.e. a stack of layers\n",
    "    ### TO DO \n",
    "\n",
    "    ### Add the layers where the activation is the argument of the function\n",
    "    ### TO DO \n",
    "\n",
    "    ### Compile the model\n",
    "    ### TO DO \n",
    "\n",
    "    ### Return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Evaluate the final loss with respect to different activation functions\n",
    "\n",
    "- Create the data, and split them in train and test set\n",
    "- loop over different activation functions that you can find [here](https://keras.io/activations/)\n",
    "- `fit` the model for each activation function (use a `validation_split` and an Early Stopping Criterion)\n",
    "- `evaluate` the score (`loss` and `accuracy` on the unseen test set)\n",
    "- report the values into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=400, noise=0.25)\n",
    "\n",
    "n_train = int(0.7*len(X))\n",
    "X_train, X_test = X[:n_train, :], X[n_train:, :]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "results = []\n",
    "\n",
    "### Model initialization\n",
    "for act in ['relu', 'sigmoid', 'softmax', 'linear', 'exponential', 'tanh']:\n",
    "    ### Initialize the model\n",
    "    ### TO DO \n",
    "\n",
    "    ### Early stopping criterion\n",
    "    ### TO DO \n",
    "\n",
    "    ### Fitting the model \n",
    "    ### TO DO \n",
    "\n",
    "    ### Evaluate the model\n",
    "    ### TO DO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot\n",
    "\n",
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Write a function `initialize_model_3`\n",
    "\n",
    "Similarly to `initialize_model_2`, write a `initialize_model_3` where you can pass as an input the different optimizer values [(See documentation here)](https://keras.io/optimizers/). Then, plot the results of the loss and accuracy for these different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_3(optimizer):\n",
    "    \n",
    "    ### TO DO \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=400, noise=0.20)\n",
    "\n",
    "n_train = int(0.7*len(X))\n",
    "X_train, X_test = X[:n_train, :], X[n_train:, :]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "results = []\n",
    "\n",
    "### Model initialization\n",
    "for opt in ['adam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta']:\n",
    "    \n",
    "    ### Initilaize the model\n",
    "    model = initialize_model_3(opt)\n",
    "\n",
    "    ### Early stopping criterion, fitting and evaluation\n",
    "    ### TO DO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot\n",
    "\n",
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Write a function `initialize_model_4`\n",
    "\n",
    "Similarly to the previous question, write a new function `initialize_model_4` to look at the impact of the loss function on the accuracy.\n",
    "\n",
    "### Question bis: Can you compare the results based on the final loss value? If yes, what is the best? If no, how would you compare them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_4(loss):\n",
    "    \n",
    "    #### Initialization, adding layers and compiling\n",
    "    ### TO DO \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=400, noise=0.20)\n",
    "\n",
    "n_train = int(0.7*len(X))\n",
    "X_train, X_test = X[:n_train, :], X[n_train:, :]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "results = []\n",
    "\n",
    "for loss in ['binary_crossentropy', 'hinge', 'squared_hinge']:\n",
    "    \n",
    "    ### Initialization, early stopping criterion, fitting and then, evaluation\n",
    "    ### TO DO \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot\n",
    "\n",
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: You have probably noticed that the accuracy is quite the same no matter what the loss is, no matter what the batch_size is and no matter what the loss is. First, do you have any thoughts about the reason of this plateau, and, second, is it a good final accuracy, would it be possible to be better?\n",
    "\n",
    "##### Answer : \n",
    "\n",
    "            => ### TO DO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: Optional\n",
    "\n",
    "You can now go further in this tutorial by looking at the following features : \n",
    "- impact of the noise in the data on the accuracy and loss output\n",
    "- add new layers in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "lewagon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
