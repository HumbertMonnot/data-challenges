{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Multiclass Classification Exercise\n",
    "\n",
    "The data are created from the `make_blob` function of scikit learn. \n",
    "It returns categorical data, so that this notebook is a multiclass classification task : based on the input data $x$, tells whether the sample belongs to the first, second, third, ... category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data\n",
    "\n",
    "The `make_blob` function [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) enables to draw : \n",
    "- an arbitrary number of data sample, argument `n_samples`\n",
    "- an arbitrary number of features per data sample, argument `n_features`\n",
    "- an arbitrary number of categories, argument `centers`\n",
    "- a distance between the categories, argument `cluster_std`\n",
    "\n",
    "There is also the `random_state` argument that allows to draw the data deterministically, in order to reproduce the same data. Two persons that choose the same random_state will have the same data.\n",
    "\n",
    "### Question : Generate data with : \n",
    "- 1200 samples\n",
    "- 8 features per sample\n",
    "- 7 categories of data\n",
    "- 8 as the distance between the categories\n",
    "\n",
    "Select a `random_state` equal to 1.\n",
    "\n",
    "Print the shape and check that it corresponds to (1200, 8) for `X` and (1200) for `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(### TODO) \n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "\n",
    "Thanks to matplotlib, plot two (arbitrary) dimensions of the input data. Each dot should be colored by the category it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Plot\n",
    "\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : repeat the operation on other dimensions, to visualy that the data are not easily separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Plot\n",
    "\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for now, `y` is the list of integers, each correspoding to the category of the related input data.\n",
    "It looks like `[3, 2, 2, 3, 0, 5, 1, 1, 0, 5, ...]` (in this example, we have 6 categories, from 0 to 5).\n",
    "\n",
    "However, for categorical task in Keras, the output should have a number of columns equal to the number of different categories. Each row, corresponding to an input data, is a list of the probabilities that this input belongs to the corresponding category. AS here, the probabilities to belong to each category is equal to 1, it should look like\n",
    "\n",
    "```\n",
    "[\n",
    "[0, 0, 0, 1, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0], \n",
    "[0, 0, 1, 0, 0, 0], \n",
    "[1, 0, 0, 0, 0, 0], \n",
    "[0, 0, 0, 0, 0, 1], \n",
    "[0, 1, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 1],\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "Each column corresponds to a category. Each row corresponds to a target, the 1 being the category the input data belongs to.\n",
    "\n",
    "To transform `y` to categories, use `to_categorical` function from Keras (already imported). \n",
    "\n",
    "\n",
    "### Question: First print `y`, then apply it and store it into `y_cat` and reprint `y_cat` to see the new structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "\n",
    "### TODO \n",
    "\n",
    "print(y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Split the initial dataset into a train and test set (size: 70/30%)\n",
    "\n",
    "Remark : Please call the variables `X_train`, `y_train`, `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For technical reasons, the data should be rescaled, so that the data are _approximately_ all in [-10, 10].\n",
    "To do so, the `StandardScaler` function from Scikit-Learn [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) allows to do that easily.\n",
    "\n",
    "[Advanced notion] The technical reason for this standardisation/normalisation/centering is partly due to the activation function, whose non-linearity  and _variations_ are around 0.\n",
    "\n",
    "The function should be applied as \n",
    "```\n",
    "SScaler = StandardScaler()\n",
    "SScaler.fit(X)             ### Used to fit the coefficients of the standardisation\n",
    "X = Sscaler.transform(X)   ### Used to rescale X\n",
    "```\n",
    "\n",
    "### Question: Given that you splited you dataset into `X_train` and `X_test`, how would you perform this task? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model\n",
    "\n",
    "Once the data is set, we will initialize your first Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \n",
    "    ### The first lines are as in the previous model, except for the input_dimension that corresponds to the\n",
    "    ### number of features per sample we have, i.e. 8.\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(100, input_dim=8, activation='relu'))\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "    \n",
    "    ### Here, the real different is the name of the loss. The loss is not designed to distinguish between two categories\n",
    "    ### but between multiple categories.\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model - Reminder\n",
    "\n",
    "Reminder : One step of parameter update is \"calledâ€œ a _backpropagation_. It is done by evaluating the prediction on a set of `N` data, called the batch. `N` is thus the batch size. One iteration is when the updates has been made considering all the batches, i.e. it went through all the data once, and only once.\n",
    "\n",
    "This is an example of the model fitting, with :\n",
    "- training data (input and output)\n",
    "- a validation set that corresponds to unused data for training but on which the model compute some estimation to see its generalization\n",
    "- the epochs, i.e. the number of iterations \n",
    "- a batch_size\n",
    "- verbose: commonly used arguments to output some logs. It usually goes from 0 (no logs) to greated numbers, each being associated to a certain amount of logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=400, \n",
    "                    batch_size=50,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results on the train set with the following command. The results contains the list of evaluated values that are, first, the loss (here, the binary_crossentropy) and then, the list of metrics that were listed in the `metrics` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train loss: {} - Train accuracy (MAE): {}'.format(results[0], results[1]))\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: {} - Test accuracy (MAE): {}'.format(results[0], results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Write a function, that given the `history` returned by the `model.fit`, plots two figures:\n",
    "\n",
    "\n",
    "- The first figure represents two curves, the first being the value of the train loss during the iterations, the second being the value of the test loss during the iterations.\n",
    "\n",
    "- The second figure has also two curves, the train accuracy and the test accuracy at each iteration.\n",
    "\n",
    "### Question bis : Use this function on the history you got previously and comment it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    ### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You again see a strong effect of the overfitting : the Neural Network gets better and better on the examples it sees but it lacks generalization in the sense that the test loss and accuracy are getting worse.\n",
    "\n",
    "### Question: As in the previous notebook, use the following Early Stopping Criterion : \n",
    "\n",
    "`es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)`\n",
    "\n",
    "To call it, you should just add `callbacks=[es]` to your `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the model, initialize the stopping criterion, and fit the model (store the output in `history`)\n",
    "### TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Use the previously defined plot function to look at both the loss and accuracy now. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous set-up, you used the test set as the validation set. \n",
    "\n",
    "### Question: Would it be correct to say that your final accuracy is thus the one on this validation set?\n",
    "\n",
    "\n",
    "To use a validation set independant from the test_set, you can use due argument `validation_split` [(See documentation)](https://keras.io/models/sequential/) in the `model.fit`. Given a split value of `0.7` so that 70% of the `X_train` data are used for training and 30% of the `X_train` are used for validation set.\n",
    "\n",
    "### Question: Report now the loss and accuracy value on the `X_test` set thanks to `model.evaluate`\n",
    "\n",
    "_Hint_ : It writes `results = model.evaluate(X_test, y_test, verbose=0)` after training the model again (and initialization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is now your best accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question/Answer/Remark : \n",
    "\n",
    "You might say that reporting one test value is not correct as we should do a proper K-fold cross-validation. This is perfectly correct. As the stream within each fold is the same, we consider that you can do the K-fold on your own in a real setting. However, here, training K algorithm each time is too long to be done within the allocated time (For your information, some Facebook/Google Neural networks are trained for week on heavy distributed computers).\n",
    "\n",
    "\n",
    "<hr><hr>\n",
    "\n",
    "We will now look at the effect of the batch_size on the learning\n",
    "\n",
    "### Question : Complete the fonction that, given `X`, `y` and the `batch_size` does the following : \n",
    "- splits the X and y into a training and test set\n",
    "- initializes the model\n",
    "- Fits the model\n",
    "- Evaluates the loss and accuracy on the test set\n",
    "- Return the `history` of the fit and the `results` of the evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_batch(X, y, batch_size):    \n",
    "    ### Data split\n",
    "    ### TODO \n",
    "    \n",
    "    ### Model initialization\n",
    "    ### TODO \n",
    "    \n",
    "    ### Fitting the model \n",
    "    ### TODO \n",
    "    \n",
    "    ### Evaluate on the test set\n",
    "    ### TODO \n",
    "    \n",
    "    ### Return \n",
    "    return history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Run this function for `batch_size=50`, then plot the `history` and print the results (loss and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Run this function on multiple batch_sizes. \n",
    "\n",
    "! Warning : The function `time.time()` returns the current time. Use it twice to compute the time it takes for each `run_model_batch` to run.\n",
    "\n",
    "At each iteration, plot the history, store the loss and accuracy and store the time it takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "accuracy = []\n",
    "elapsed_time = []\n",
    "batch_sizes = [20, 50, 100, 250, 500]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    ### TODO : Starting time\n",
    "    \n",
    "    ### Computing history and results, and appending to the correct list\n",
    "    ### TODO \n",
    "    \n",
    "    ### TODO : Final time, and appending it to elapsed_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: plot the loss and accuracy with respect to the batch_size. \n",
    "\n",
    "### Question bis: Also plot the elapsed_time with respect to the `batch_size`. What is the reason of such trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PLOT\n",
    "\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will fix the `batch_size` to 50 and the patience to of the Early Stopping Criterion `30`.\n",
    "\n",
    "### Write a function that, given `X_train`, `y_train`, `X_test`, `y_test`, `model` does the following : \n",
    "- Initializes the early stopping criterion (verbose to 1)\n",
    "- Fit the model with a `validation_split` equal to 0.7, with 2000 `epochs` (Do not forget the batch_size and the early stopping criterion.\n",
    "- Evaluates the model on the test set\n",
    "- Return this evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, y_train, X_test, y_test, model):    \n",
    "    ### Early stopping criterion\n",
    "    ### TODO \n",
    "    \n",
    "    ### Fitting the model \n",
    "    ### TODO \n",
    "    \n",
    "    ### Evaluation on the test set\n",
    "    ### TODO \n",
    "    \n",
    "    ### Return the results\n",
    "    return history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Run the previous function on a newly initialized model, and, print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the model\n",
    "### TODO \n",
    "\n",
    "### Run the model \n",
    "### TODO \n",
    "\n",
    "### Plot\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Write a function that does intialize a model as similarly (`initialize_model`) except that the activation function of the first layer _AND_ the loss functions are parameters of the initialize_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(activation, loss):\n",
    "    ### TODO \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Use the previous functions to do : \n",
    "- initialize a model with a the `categorical_crossentropy` loss and `relu` activation function\n",
    "- use `run_model` to run the model\n",
    "- print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Now, loop over the different activation function you can find [here](https://keras.io/activations/) (`relu`, previously used, is one of them) to see which one gives the best result\n",
    "\n",
    "\n",
    "Store the results so that you can plot them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for activation in ['relu', 'softmax', 'linear', 'tanh']:\n",
    "    ### TODO \n",
    "\n",
    "plt.plot(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `categorical_crossentropy` is not the only loss you can use. There are two more for multiclass classification tasks offered by Keras [(see here)](https://keras.io/losses/).\n",
    "\n",
    "### Question: Do the same as previously, but for the `kullback_leibler_divergence` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's look deeper at the optimizer.\n",
    "\n",
    "In the 2 category example (first tutorial), we initialize the optimizer with a string : `'sgd'`, `'adam'`, `'adadelta'`, ... In fact, each of this optimizer depends on hyperparameters that have default values. There are no reasons for these default values to be the best for the problem at hand, therefore, we will dig a bit deeper into their optimization.\n",
    "\n",
    "!! Essential !! : If there was _one_ essential to remember, it is the _learning rate_.\n",
    "\n",
    "### Question: Write an `init` function similar to the previous one, but instead of having the activation and the loss as arguments, put the `optimizer`.\n",
    "\n",
    "Set the loss to be `kullback_leibler_divergence` and the activation to be `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_2(optimizer):\n",
    "    ### TODO \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Initialize a model, and run it as previously. As for the optimizer, you can put any string you want amond the previous one mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look on how to initialize an optimizer with _not_ default values of the optimizer - as it is done when you give a string. This is an example where the learning rate `lr` is equal to 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "model = init_2(sgd)\n",
    "history, results = run_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Now, do the same with different values of the learning rate. Store and plot the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PLOT\n",
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how many iterations it took to stop the early stopping criterion, for different values of the learning rate. \n",
    "\n",
    "The reason for this is that the learning rate is the coefficient that makes the parameter change as in this picture : \n",
    "\n",
    "![Learning rate](learning_rate.png)\n",
    "\n",
    "Therefore, a too large learning rate makes the algorithm not converge well. On the other hand, a too small learning rate makes the algorithm converge very very slowly.\n",
    "\n",
    "<hr><hr>\n",
    "\n",
    "Now, lets try the Adam optimizer that has three parameters, `learning rate` and `beta_1`, `beta_2` that are both between 0 and 1, closer to 1 in general.\n",
    "\n",
    "It all writes as : `adam = optimizers.Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.99)`\n",
    "\n",
    "### Question: Run the model with the adam optimizer and different values of the three above mentioned parameters. Look at the different accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for lr in [0.001, 0.01, 0.1]:\n",
    "    for beta_1 in [0.8, 0.9, 0.95, 0.99]:\n",
    "        for beta_2 in [0.8, 0.9, 0.95, 0.99]:\n",
    "            # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### PLOT\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another optimizer in the documentation (https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's change the architecture of the model ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a new function to initialize the model `init_model`, where you can change the number of layers.\n",
    "\n",
    "The parameter of the `init_model` is `latents_dim` which is a list of integers: the length of `latent_dims` is the number of additional layers you add, and each integer is the number of neurons in the layer.\n",
    "\n",
    "Therefore, the Neural Network is made of \n",
    "- a first layer of `input_dim = 8`, output being the first integer in `latent_dims`\n",
    "- as many layers as integers in `latent_dims` - 1, each of output_dim being the related integer\n",
    "- a last layer whose input-dim is the last integer in `latent_dims`, the output_dim is the number of classes in the dataset.\n",
    "\n",
    "For example `latent_dims=[10, 3, 10]` means that the neural net is made of\n",
    "- a layer of input dim 8, output dim 10\n",
    "- a layer of input dim 10, output dim 3\n",
    "- a layer of input dim 3, output dim 10\n",
    "- a layer of input dim 10, output dim 7\n",
    "\n",
    "You can use any loss and optimizer you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_3(latent_dims):\n",
    "    ### TODO \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: init a model with latents_dim=[15, 6, 15], and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Test multiple architectures for only one additional layer but different number of neurons. Look at their relative predictive power (i.e. their accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for ld in [[8], [16], [32], [50], [100], [200], [400]]:\n",
    "    #### TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: You are now set to try any architecture you want, feel free to add additional layers with different number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for ld in [[10], [10,10], [10,10,10], [25,10,25], [30,30], [10,10,10,10], [50,50]]:\n",
    "    ### TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PLOT\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Now, for a given architecture, remake the dataset with an increasing number of samples. For each, look at the time to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "elapsed_time = []\n",
    "\n",
    "for s in [100, 500, 1000, 2000, 5000]:\n",
    "    ### Create data\n",
    "    ### TODO \n",
    "\n",
    "    ### Init the model, run it and store the results\n",
    "    ### TODO \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### PLOT\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional : Go beyond by studying the effect of the dataset : \n",
    "- the number of features\n",
    "- the number of categories\n",
    "- the distance between the groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "lewagon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
