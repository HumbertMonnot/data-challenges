{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "###  Imports  ###\n",
    "#################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning week - Day 1 - Regression Exercise \n",
    "\n",
    "\n",
    "The overall objectif of this exercise is to predict the house pricing in the Boston area (USA) based on input features. This will be done with a Neural Network.\n",
    "\n",
    "The intention of this exercise is to :\n",
    "- prepare the data for a NN (Neural Network)\n",
    "- train a _regression_ NN\n",
    "- check the NN loss during the training and adapt accordingly\n",
    "- select the hyperparameters of the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will predict the price of houses in Boston and suburbs, based on input variables as the is the pupil-teacher ratio (in the related town), nitric oxides concentration, the crime rate per capita or the weighted distances to five Boston employment centers.\n",
    "\n",
    "You can check additional information about the dataset here https://towardsdatascience.com/machine-learning-project-predicting-boston-house-prices-with-regression-b4e47493633d\n",
    "\n",
    "This classic dataset is provided in the Keras library. It can be loaded as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shape` is an interesting attribute of the data object. It gives the (row, column) shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training data: {}\".format(X_train.shape))\n",
    "print(\"Size of test data: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #1 : What kind of Machine Learning is this problem related to? Supervised, regression, unsupervised, clustering, classification, ... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Due to the form of some non-linear activation functions, it is important to center and normalize (i.e. divided by its  1) the data so that they are centered around 0 with a variance of 1. \n",
    "\n",
    "### Question: Use the StandardScaler from scikit learn [(see documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardise the data \n",
    "\n",
    "Warning : Use it wisely on the train and test set. _Hint_ : you can check what was done on the multiclass classification tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question #3 : Plot each of your variable within the train set to check that it is somehow centered around 0 with small variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "Now that we have the data, we will define a first architecture and run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "\n",
    "    ### Model architecture\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(13,)))\n",
    "    model.add(layers.Dense(32, activation='linear'))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    \n",
    "    # Model optimization : Optimized, loss and metric to \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "### Always reinitialize the model! Otherwise, it would be a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    validation_split=0.7,\n",
    "                    epochs=300,\n",
    "                    batch_size=10,\n",
    "                    verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: {} - Test accuracy (MAE): {}'.format(results[0], results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's look at different parameters effects on the result : the loss, the optimizer and different architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, y_train, X_test, y_test, model):    \n",
    "    ### Early stopping criterion\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "    \n",
    "    ### Fitting the model \n",
    "    history = model.fit(X_train, y_train, \n",
    "                    validation_split=0.7,\n",
    "                    epochs=3000, \n",
    "                    batch_size=50,\n",
    "                    verbose=0, \n",
    "                    callbacks=[es])\n",
    "    \n",
    "    ### Evaluation on the test set\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    ### Return the results\n",
    "    return history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Write a function `init_model_1` that initializes an model such that the parameters are the loss and the optimizer. Launch it with loss being `mse` and optimizer being `adam`.\n",
    "\n",
    "Take the same architecture as in the `initialize_model` above.\n",
    "\n",
    "Notes : \n",
    "- MSE stands for Mean Square Error. It corresponds to te L2 norm, i.e. the square of the error between $f_{\\theta}(x)$ and $y$, i.e. $||f_{\\theta}(x) - y ||^2$\n",
    "- MAE (which is another type or regression error) stands for Mean Absolute Error. It corresponds to te L1 norm, i.e. the absolute error between $f_{\\theta}(x)$ and $y$, i.e. $|f_{\\theta}(x) - y |$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### Answer :\n",
    "\n",
    "def init_model_1(loss, optimizer):\n",
    "    \n",
    "    ### Model architecture\n",
    "    ### TODO \n",
    "    \n",
    "    \n",
    "    # Model optimization : Optimized, loss and metric to \n",
    "    ### TODO \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Launch the function and the `plot_loss_mae` function\n",
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the output MAE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Now, compare the result (on the MAE) while training with `loss='mae'` (`optimizer` still being `adam`), especially by printing the final `mae` on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important : Even though your final estimation is the `MAE`, it does not mean that the `MAE` as the loss would be better to optimize the model than the `MSE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_2(latent_dims):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(latent_dims[0], activation='relu', input_shape=(13,)))\n",
    "    for ld in latent_dims[1:]:\n",
    "        model.add(layers.Dense(ld, activation='relu', input_shape=(13,)))\n",
    "\n",
    "    model.compile(optimizer='sgd', loss='mae', metrics=['mae'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function allows to run model with different architectures. For instance, `init_model_2([2])` outputs a NN with \n",
    "\n",
    "- a layer of 13 input dimension and 2 output dimension\n",
    "- a layer of 2 input dimension and 1 output dimension.\n",
    "\n",
    "Similarly, `init_model_2([10, 5])` outputs a NN with \n",
    "\n",
    "- a layer of 13 input dimension and 10 output dimension\n",
    "- a layer of 10 input dimension and 5 output dimension\n",
    "- a layer of 5 input dimension and 1 output dimension.\n",
    "\n",
    "### Question: Look at final `mae` on the test set for the following architectures : \n",
    "- latent_dims = [1]\n",
    "- latent_dims = [5]\n",
    "- latent_dims = [10]\n",
    "- latent_dims = [20]\n",
    "- latent_dims = [60]\n",
    "- latent_dims = [100]\n",
    "\n",
    "## Important Remark : If the early stopping has not been called, what does it mean and what should you do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## PLOT\n",
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Do the same reasoning for the following architectures:\n",
    "\n",
    "- latent_dims = [10, 1]\n",
    "- latent_dims = [10, 5]\n",
    "- latent_dims = [10, 10]\n",
    "- latent_dims = [10, 15]\n",
    "- latent_dims = [10, 20]\n",
    "\n",
    "\n",
    "## Important Remark : If the early stopping has not been called, what does it mean and what should you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question : Optional - Get the best score of the class - you are advised to look at both the architecture and the optimizer with a good selection of the hyperpameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "lewagon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
