{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this exercise is to use the tools and methods that you learned during the previous weeks, in order to solve a **real challenge**.\n",
    "\n",
    "The problem to solve is a **Kaggle Competition**: [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction). The goal is to predict the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a machine learning model requires a few different steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Get the data\n",
    "2. Explore the data\n",
    "3. Data cleaning\n",
    "4. Evaluation metric\n",
    "5. Model baseline\n",
    "6. Build your first model\n",
    "7. Model evaluation\n",
    "8. Kaggle submission\n",
    "9. Model iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available on [Kaggle](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data).\n",
    "\n",
    "First of all:\n",
    "- Follow the instructions to download the training and test sets\n",
    "- Put the datasets in a separate folder on your local disk. You can name it \"data\" for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use Pandas to read and explore the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is relatively big (~5GB).\n",
    "So let's only open a portion of it.\n",
    "üëâ Go to [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/) to see how to open a portion of CSV file and store it into a DataFrame (ex: just read 1 million rows maximum)\n",
    "\n",
    "üí° NB: here we will read portion of a file **directly from a URL**, the same can be done with local file\n",
    "\n",
    "‚ö†Ô∏è Make sure that you have installed the `s3fs` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 's3://wagon-public-datasets/taxi-fare-train.csv'\n",
    "df = pd.read_csv(url, nrows=1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display the first rows to understand the different fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to solve the prediction problem, we need to get a better understanding of the data.\n",
    "In order to do that, we are going to use libraries such as Pandas and Seaborn.\n",
    "First of all, make sure you have [Seaborn](https://seaborn.pydata.org/) installed and import it into your notebook.\n",
    "Note that it can also be useful to import `matplotlib.pyplot` in order to customize a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.figure(figsize=(12,5))\n",
    "palette = sns.color_palette('Paired', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are multiple things we want to do in terms of data exploration\n",
    "\n",
    "- You first want to look at the distribution of the variable that you are going to predict: \"fare_amount\"\n",
    "- Then you want to visualize other variable distributions\n",
    "- And finally it is often very helpful to compute and visualize the correlation between the target variable and other variables\n",
    "- Also, lets look for any missing values, or other irregularities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the target variable\n",
    "- Compute simple statistics for the target variable (min, max, mean, std, etc)\n",
    "- Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fare_amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(series=df[\"fare_amount\"], title=\"Fare Distribution\"):\n",
    "    sns.histplot(series, kde=True, stat='density', discrete=True)\n",
    "    sns.despine()\n",
    "    plt.title(title);\n",
    "    plt.show()\n",
    "plot_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop absurd values \n",
    "df = df[df.fare_amount.between(0, 60)]\n",
    "plot_dist(df.fare_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# we can also visualize binned fare_amount variable\n",
    "df['fare-bin'] = pd.cut(df['fare_amount'], bins = list(range(0, 50, 5)), include_lowest=True).astype('str')\n",
    "\n",
    "# uppermost bin\n",
    "df['fare-bin'] = df['fare-bin'].replace(np.nan, '[45+]')\n",
    "# df.loc[df['fare-bin'] == 'nan', 'fare-bin'] = '[45+]'\n",
    "\n",
    "# apply this to clean up the label of the first bin\n",
    "df['fare-bin'] = df['fare-bin'].apply(lambda x: x.replace('-0.001', '0'))\n",
    "\n",
    "# sort by fare the correct look in the chart\n",
    "df = df.sort_values(by='fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"fare-bin\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore other variables\n",
    "\n",
    "- passenger_count (statistics + distribution)\n",
    "- pickup_datetime (you need to build time features out of the pickup datetime)\n",
    "- Geospatial features (pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude)\n",
    "- Find other variables that you can compute from existing data and that might explain the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"passenger_count\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.title('Passenger Count');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickup Datetime\n",
    "\n",
    "- Extract time features from pickup_datetime (hour, day of week, month, year)\n",
    "- Create a method `def extract_time_features(_df)` that you will be able to re-use later\n",
    "- Be careful with the timezone\n",
    "- Explore the newly created features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df):\n",
    "    timezone_name = 'America/New_York'\n",
    "    time_column = \"pickup_datetime\"\n",
    "    df.index = pd.to_datetime(df[time_column])\n",
    "    df.index = df.index.tz_convert(timezone_name)\n",
    "    df[\"dow\"] = df.index.weekday\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"month\"] = df.index.month\n",
    "    df[\"year\"] = df.index.year\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = extract_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour of day\n",
    "sns.catplot(x=\"hour\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.title('Hour of Day');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of week\n",
    "sns.catplot(x=\"dow\", kind=\"count\", palette=palette, data=df, height=5, aspect=3);\n",
    "sns.despine()\n",
    "plt.title('Day of Week');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add timezone features\n",
    "\n",
    "- Extract time features from pickup_datetime (hour, day of week, month, year)\n",
    "- Create a method `def extract_time_features(_df)` that you will be able to re-use later\n",
    "- Be careful of timezone\n",
    "- Explore the newly created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the boudaries from the test set and remove the outliers from the training set\n",
    "for col in [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]:\n",
    "    MIN = df_test[col].min()\n",
    "    MAX = df_test[col].max()\n",
    "    print(col, MIN, MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"pickup_latitude\"].between(left = 40, right = 42 )]\n",
    "df = df[df[\"pickup_longitude\"].between(left = -74.3, right = -72.9 )]\n",
    "df = df[df[\"dropoff_latitude\"].between(left = 40, right = 42 )]\n",
    "df = df[df[\"dropoff_longitude\"].between(left = -74, right = -72.9 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that you install folium first\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "center_location = [40.758896, -73.985130]\n",
    "m = folium.Map(location=center_location, control_scale=True, zoom_start=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"count\"] =1\n",
    "heatmap_data = df.head(10000)[['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist()\n",
    "gradient = {0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'}\n",
    "HeatMap(data=heatmap_data, radius=5, gradient=gradient, max_zoom=13).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data_by_hour = []\n",
    "__df__ = df.head(10000)\n",
    "for hour in df.hour.sort_values().unique():\n",
    "    _df = __df__[__df__.hour == hour][['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist()\n",
    "    heatmap_data_by_hour.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2 = folium.Map(location=center_location, control_scale=True, zoom_start=11)\n",
    "HeatMapWithTime(heatmap_data_by_hour, radius=5,\n",
    "                gradient=gradient,\n",
    "                min_opacity=0.5, max_opacity=0.8,\n",
    "                use_local_extrema=False).add_to(m2)\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance\n",
    "\n",
    "- Compute the distance between pickup and dropoff locations (tip: https://en.wikipedia.org/wiki/Haversine_formula)\n",
    "- Write a method `def haversine_distance(df, **kwargs)` that you will be able to reuse later\n",
    "- Compute a few statistics for distance and plot distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def haversine_distance(df,\n",
    "                       start_lat=\"start_lat\",\n",
    "                       start_lon=\"start_lon\",\n",
    "                       end_lat=\"end_lat\",\n",
    "                       end_lon=\"end_lon\"):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees).\n",
    "    Vectorized version of the haversine distance for pandas df\n",
    "    Computes distance in kms\n",
    "    \"\"\"\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n",
    "    dlon = lon_2_rad - lon_1_rad\n",
    "    dlat = lat_2_rad - lat_1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_distance = 6371 * c\n",
    "    return haversine_distance\n",
    "\n",
    "df[\"distance\"] = haversine_distance(df, \n",
    "                                    start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "                                    end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_dist(series=df[df.distance < 50].distance, title='Distance distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore how target variable correlate with other variables\n",
    "\n",
    "- As a first step, you can visualize the target variable vs another variable. For categorical variables, it is often useful to compute the average target variable for each category (Seaborn has plots that do it for you!). For continuous variables (like distance, you can use scatter plots, or regression plots, or bucket the distance into different bins\n",
    "- But there many different ways to visualize correlation between features, so be creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"passenger_count\", y=\"fare_amount\", palette=palette, data=df, kind=\"bar\", aspect=3)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"hour\", y=\"fare_amount\", palette=palette, data=df, kind=\"bar\", aspect=3)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dow\", y=\"fare_amount\", palette=palette, data=df, kind=\"bar\", aspect=3)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"distance\", y=\"fare_amount\", data=df[df.distance < 80].sample(100000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"distance\", y=\"fare_amount\", hue=\"passenger_count\", data=df[df.distance < 80].sample(100000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably saw in the previous section during your data exploration, there are some values that do not seem valid.\n",
    "In this section, you will take a few steps to clean the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all the trips that look incorrect. We recommend that you write a method called `clean_data(df)` that you will be able to re-use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"trips with negative fares:\", len(df[df.fare_amount <= 0]))\n",
    "print(\"trips with too high distance:\", len(df[df.distance >= 100]))\n",
    "print(\"trips with too many passengers:\", len(df[df.passenger_count > 8]))\n",
    "print(\"trips with zero passenger:\", len(df[df.passenger_count == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, test=False):\n",
    "    df = df.dropna(how='any', axis='rows')\n",
    "    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n",
    "    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "    df = df[df.fare_amount.between(0, 4000)]\n",
    "    df = df[df.passenger_count < 8]\n",
    "    df = df[df.passenger_count >= 0]\n",
    "    df = df[df[\"pickup_latitude\"].between(40, 42)]\n",
    "    df = df[df[\"pickup_longitude\"].between(-74.3, -72.9 )]\n",
    "    df = df[df[\"dropoff_latitude\"].between(40, 42)]\n",
    "    df = df[df[\"dropoff_longitude\"].between(-74, -72.9)]\n",
    "    return df\n",
    "\n",
    "df_cleaned = clean_data(df)\n",
    "\"% data removed\", (1 - len(df_cleaned) / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation metric <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric for this competition is the root mean-squared error or RMSE. The RMSE measures the difference between the predictions of a model, and the corresponding ground truth. A large RMSE is equivalent to a large average error, so smaller values of RMSE are better.\n",
    "\n",
    "More details here https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method `def compute_rmse(y_pred, y_true)` that computes the RMSE given `y_pred` and `y_true` which are two numpy arrays corresponding to model predictions and ground truth values.\n",
    "\n",
    "This method will be useful in order to evaluate the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_pred, y_true):\n",
    "    return np.sqrt(((y_pred - y_true)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Model baseline <a id='part5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before building your model, it is often useful to get a performance benchmark. For this, you will use a baseline model that is a very dumb model and compute the evaluation metric on that model.\n",
    "Then, you will be able to see how much better your model is compared to the baseline. It is very common to see ML teams coming up with very sophisticated approaches without knowing by how much their model beats the very simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Generate predictions based on a simple heuristic\n",
    "- Evaluate the RMSE for these predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['y_pred'] = df_cleaned['fare_amount'].mean()\n",
    "\n",
    "compute_rmse(df['y_pred'], df['fare_amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6. Build your first model <a id='part6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now it is time to build your model!\n",
    "\n",
    "For starters we are going to use a linear model only. We will try more sophisticated models later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are the different steps that you have to follow:\n",
    "\n",
    "1. Split the data into two different sets (training and validation). You will be measuring the performance of your model on the validation set\n",
    "2. Make sure that you apply the data cleaning on your training set\n",
    "3. Think about the different features you want to add in your model\n",
    "4. For each of these features, make sure you apply the correct transformation so that the model can correctly learn from them (this is true for categorical variables like `hour of day` or `day of week`)\n",
    "5. Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# training/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Apply data cleaning on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train = clean_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### List features (continuous vs categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# features\n",
    "target = \"fare_amount\"\n",
    "features = [\"distance\", \"hour\", \"dow\", \"passenger_count\"]\n",
    "categorical_features = [\"hour\", \"dow\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Features transformation\n",
    "\n",
    "- Write a method `def transform_features(df, **kwargs)` because you will have to make sure that you apply the same transformations on the validation (or test set) before making predictions\n",
    "- For categorical features transformation, you can use `pandas.get_dummies` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transform_features(_df, dummy_features=None):\n",
    "    encode = True if dummy_features is None else False\n",
    "    dummy_features = dummy_features if dummy_features is not None else []\n",
    "    for c in categorical_features:\n",
    "        dummies = pd.get_dummies(_df[c], prefix=c)\n",
    "        _df = pd.concat([_df, dummies], axis=1)\n",
    "        if encode:\n",
    "            dummy_features = dummy_features + (list(dummies.columns.values))\n",
    "    for dummy_feature in [f for f in dummy_features if f not in _df.columns]:\n",
    "        _df[dummy_feature] = 0 \n",
    "    _df = _df[dummy_features + features]\n",
    "    return _df, dummy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "from sklearn.linear_model import LassoCV\n",
    "model = LassoCV(cv=5, n_alphas=5)\n",
    "X_train, dummy_features = transform_features(df_train)\n",
    "X_train = df_train[features]\n",
    "y_train = df_train.fare_amount\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7. Model evaluation <a id='part7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now in order to evaluate your model, you need to use your previously trained model in order to make predictions on the validation set.\n",
    "\n",
    "For this, follow these steps:\n",
    "1. Apply the same transformations on the validation set\n",
    "2. Make predictions\n",
    "3. Evaluate predictions using `compute_rmse` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X_val, _ = transform_features(df_val, dummy_features=dummy_features)\n",
    "X_test = df_test[features]\n",
    "df_test[\"y_pred\"] = model.predict(X_test)\n",
    "compute_rmse(df_test.y_pred, df_test.fare_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8. Kaggle submission <a id='part8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that you have a model, you can now make predictions on Kaggle test set and be evaluated by Kaggle directly.\n",
    "\n",
    "- Download the test data from Kaggle\n",
    "- Follow the [instructions](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation) to make sure that your predictions are in the right format\n",
    "- Re-train your model using all the data (do not split with train/validation)\n",
    "- Apply the feature engineering and transformation methods on the test set\n",
    "- Use the model to make predictions on the test set\n",
    "- Submit your predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Re-train the model with all the data\n",
    "df_cleaned = clean_data(df)\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned.fare_amount\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=5)\n",
    "lasso.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load Kaggle's test set\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "df_test[\"distance\"] = haversine_distance(df_test, \n",
    "                                         start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "                                         end_lat=\"dropoff_latitude\", end_lon=\"pickup_longitude\")\n",
    "df_test = extract_time_features(df_test)\n",
    "# X_test, _ = transform_features(df_test, dummy_features=dummy_features) \n",
    "X_test = df_test[features]\n",
    "\n",
    "# prediction\n",
    "df_test[\"y_pred\"] = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True)[[\"key\", \"y_pred\"]].rename(columns={\"y_pred\": \"fare_amount\"}).to_csv(\"lasso_v0_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. [OPTIONAL] Push further Feature Engineering <a id='part9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve your model by trying different things (But do not worry, some of these things will be covered during the next days).\n",
    "- Use more data to train\n",
    "- Build and add more features\n",
    "- Try different estimators\n",
    "- Adjust your data cleaning to remove more or less data\n",
    "- Tune the hyperparameters of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we will focus on advanced feature engineering (keep in mind that relevant feateng is often key to significant increase in model performances):\n",
    "\n",
    "üëâ **Manhattan distance** better suited to our problem  \n",
    "üëâ **Distance to NYC center** to highlight interesting pattern...\n",
    "üëâ **Direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Another Distance ?\n",
    "- Think about the distance you used, try and find a more adapted distance for our problem (ask TAs for insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D(A,B) = \\left( \\sum_{i=1}^{n} \\lvert x_{A_i} - x_{B_i} \\rvert ^p \\right)^\\frac{1}{p}$$\n",
    "with $A=(x_{A_1}, x_{A_2}, ..., x_{A_n})$ and $B=(x_{B_1}, x_{B_2}, ..., x_{B_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Minkowski Distance is actually the generic distance to compute different distances\n",
    "\n",
    "# in a cartesion system of reference of 2 dimensions (x,y), the Minkowski distance can be implemented as follow:\n",
    "def minkowski_distance(x1, x2, y1, y2, p):\n",
    "    delta_x = x1 - x2\n",
    "    delta_y = y1 - y2\n",
    "    return ((abs(delta_x) ** p) + (abs(delta_y)) ** p) ** (1 / p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a GPS coordinates system, the Minkowksi distance should be implented as follows:\n",
    "# convert degrees to radians\n",
    "def deg2rad(coordinate):\n",
    "    return coordinate * np.pi / 180\n",
    "\n",
    "# convert radians into distance\n",
    "def rad2dist(coordinate):\n",
    "    earth_radius = 6371 # km\n",
    "    return earth_radius * coordinate\n",
    "\n",
    "# correct the longitude distance regarding the latitude (https://jonisalonen.com/2014/computing-distance-between-coordinates-can-be-simple-and-fast/)\n",
    "def lng_dist_corrected(lng_dist, lat):\n",
    "    return lng_dist * np.cos(lat)\n",
    "\n",
    "def minkowski_distance_gps(lat1, lat2, lon1, lon2, p):\n",
    "    lat1, lat2, lon1, lon2 = [deg2rad(coordinate) for coordinate in [lat1, lat2, lon1, lon2]]\n",
    "    y1, y2, x1, x2 = [rad2dist(angle) for angle in [lat1, lat2, lon1, lon2]]\n",
    "    x1, x2 = [lng_dist_corrected(elt['x'], elt['lat']) for elt in [{'x': x1, 'lat': lat1}, {'x': x2, 'lat': lat2}]]\n",
    "    return minkowski_distance(x1, x2, y1, y2, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidian distance <=> minkowski_distance(x1, x2, y1, y2, 2)\n",
    "df['manhattan_dist'] = minkowski_distance_gps(df['pickup_longitude'], df['dropoff_longitude'],\n",
    "                                              df['pickup_latitude'], df['dropoff_latitude'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manhattan distance <=> minkowski_distance(x1, x2, y1, y2, 1)\n",
    "df['euclidian_dist'] = minkowski_distance_gps(df['pickup_longitude'], df['dropoff_longitude'],\n",
    "                                              df['pickup_latitude'], df['dropoff_latitude'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance from the center \n",
    "\n",
    "- Compute a new feature calculating the distance of pickup location from the center\n",
    "- Scatter Plot *distance_from_center* regarding *distance* \n",
    "- What do you observe ? What new features could you add ? How are these new features correlated to the target ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute the distance from the NYC center\n",
    "nyc_center = (40.7141667, -74.0063889)\n",
    "df[\"nyc_lat\"], df[\"nyc_lng\"] = nyc_center[0], nyc_center[1]\n",
    "args =  dict(start_lat=\"nyc_lat\", start_lon=\"nyc_lng\",\n",
    "             end_lat=\"pickup_latitude\", end_lon=\"pickup_longitude\")\n",
    "\n",
    "df['distance_to_center'] = haversine_distance(df, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df.distance < 40) & (df.distance_to_center < 40)\n",
    "sns.scatterplot(x=\"distance_to_center\", y=\"distance\", data=df[idx].sample(10000), hue=\"fare-bin\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distance_to_center.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ **Take some time to step back and try to observe an interesting pattern here. What are these clusters with a similar distance to the center?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to be fixed distance_to_center\n",
    "jfk_center = (40.6441666667, -73.7822222222)\n",
    "\n",
    "df[\"jfk_lat\"], df[\"jfk_lng\"] = jfk_center[0], jfk_center[1]\n",
    "args_pickup =  dict(start_lat=\"jfk_lat\", start_lon=\"jfk_lng\",\n",
    "                    end_lat=\"pickup_latitude\", end_lon=\"pickup_longitude\")\n",
    "args_dropoff =  dict(start_lat=\"jfk_lat\", start_lon=\"jfk_lng\",\n",
    "                     end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\")\n",
    "\n",
    "jfk = (-73.7822222222, 40.6441666667)\n",
    "df['pickup_distance_to_jfk'] = haversine_distance(df, **args_pickup)\n",
    "df['dropoff_distance_to_jfk'] = haversine_distance(df, **args_dropoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pickup_distance_to_jfk.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which direction  are you heading to ?\n",
    "\n",
    "- Compute a new feature calculating the direction your are heading to\n",
    "- What do you observe ? What new features could you add ? How are these new features correlated to the target ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_direction(d_lon, d_lat):\n",
    "    result = np.zeros(len(d_lon))\n",
    "    l = np.sqrt(d_lon**2 + d_lat**2)\n",
    "    result[d_lon>0] = (180/np.pi)*np.arcsin(d_lat[d_lon>0]/l[d_lon>0])\n",
    "    idx = (d_lon<0) & (d_lat>0)\n",
    "    result[idx] = 180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n",
    "    idx = (d_lon<0) & (d_lat<0)\n",
    "    result[idx] = -180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_lon'] = df.pickup_longitude - df.dropoff_longitude\n",
    "df['delta_lat'] = df.pickup_latitude - df.dropoff_latitude\n",
    "df['direction'] = calculate_direction(df.delta_lon, df.delta_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "df.direction.hist(bins=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot direction vs average fare amount for fares inside manhattan\n",
    "def select_within_boundingbox(df, BB):\n",
    "    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "BB_manhattan = (-74.025, -73.925, 40.7, 40.8)\n",
    "idx_manhattan = select_within_boundingbox(df, BB_manhattan)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,6))\n",
    "direc = pd.cut(df[idx_manhattan]['direction'], np.linspace(-180, 180, 37))\n",
    "df[idx_manhattan].pivot_table('fare_amount', index=[direc], columns='year', aggfunc='mean').plot(ax=ax)\n",
    "plt.xlabel('direction (degrees)')\n",
    "plt.xticks(range(36), np.arange(-170, 190, 10))\n",
    "plt.ylabel('average fare amount $USD');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df.corr()\n",
    "l = list(corrs)\n",
    "l.remove(\"fare_amount\")\n",
    "corrs['fare_amount'][l].plot.bar(color = 'b');\n",
    "plt.title('Correlation with Fare Amount');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
