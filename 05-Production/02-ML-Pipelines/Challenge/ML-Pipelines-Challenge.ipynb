{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of exercices, you will learn how a build robust a ML pipeline using [Sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "An important part of a ML pipeline is the pre-processing part. For this, you will learn how to master \n",
    "Sklearn encoders and tranformers as part of the [Preprocessing Sklearn module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Scaling with StandardScaler](#exo1)\n",
    "2. [Encoding Categorical Features](#exo2)\n",
    "3. [Dealing with missing data](#exo3)\n",
    "4. [Custom Transformers and Encoders](#exo4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scaling with StandardScaler <a id='exo1'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features by removing the mean and scaling to unit variance is a common pre-processing step we apply to help many machine learning algorithms behave more efficiently.\n",
    "\n",
    "[Sklearn StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) can do the scaling transformation for you.\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "The goal of this exercice is to re-implement it.\n",
    "\n",
    "As you know, there are 2 main methods for any encoder/transformer. \n",
    "- `fit` which computes the mean and std to be used for later scaling.\n",
    "- `tranform` which performs standardization by centering and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "- Given the numpy arrays `data` and `test_data`, write a simple custom implementation of standard scaler. To test it, fit the scaler with `data` and tranform `test_data` with it.\n",
    "- Compare your results with `StandardScaler`\n",
    "- Make the custom implementation using a python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1, 10], [2, -1], [0, 22], [3, 15]])\n",
    "test_data = np.array([[2, 1], [5, 1], [3, 55], [3, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5, 11.5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis \n",
    "np.nanmean(data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Simple custom implementation of Standard Scaler\n",
    "\n",
    "def fit(X):\n",
    "    \"\"\"implement fit method\"\"\"\n",
    "    mean = np.nanmean(X, axis=0)\n",
    "    std = np.nanstd(X, axis=0)\n",
    "    return {\"mean\" : mean, \"std\" : std}\n",
    "\n",
    "\n",
    "def transform(X, params):\n",
    "    \"\"\"implement transformation method\"\"\"\n",
    "    return (X - params[\"mean\"]) / params[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472136 , -1.25275497],\n",
       "       [ 3.13049517, -1.25275497],\n",
       "       [ 1.34164079,  5.18998488],\n",
       "       [ 1.34164079, -1.25275497]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = fit(data)\n",
    "transformed_test_data = transform(test_data,params)\n",
    "transformed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472136 , -1.25275497],\n",
       "       [ 3.13049517, -1.25275497],\n",
       "       [ 1.34164079,  5.18998488],\n",
       "       [ 1.34164079, -1.25275497]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Sklearn StandardScaler and compare results\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# your code that uses StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "transformed_test_data_2 = scaler.transform(test_data)\n",
    "transformed_test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mamal\n",
      "I'm a mamal\n",
      "I'm a dog\n"
     ]
    }
   ],
   "source": [
    "# Python program to demonstrate instantiating a class \n",
    "\n",
    "# Class has attributes (attr1 and attr2)\n",
    "# x.attr1 will return the value of the attribute\n",
    "# x.attr2 will return the value of the attribute\n",
    "\n",
    "# Class has methods (functions). That you can use with ()\n",
    "\n",
    "# Self refers to the instance of the class once it gets created\n",
    "# Object will be Rodger. fun() applies to itself (Rodger)\n",
    "# And uses the attr1 and attr2 of Roger (itself)\n",
    "\n",
    "# Class methods must have an extra first parameter in method definition. We do not give a value for this parameter when we call the method, Python provides it.\n",
    "# If we have a method which takes no arguments, then we still have to have one argument.\n",
    "# This is similar to this pointer in C++ and this reference in Java.\n",
    "# When we call a method of this object as myobject.method(arg1, arg2), this is automatically converted by Python into MyClass.method(myobject, arg1, arg2) â€“ this is all the special self is about.\n",
    "  \n",
    "class Dog:  \n",
    "      \n",
    "    # A simple class attribute \n",
    "    attr1 = \"mamal\"\n",
    "    attr2 = \"dog\"\n",
    "  \n",
    "    # A sample method   \n",
    "    def fun(self):  \n",
    "        print(\"I'm a\", self.attr1) \n",
    "        print(\"I'm a\", self.attr2) \n",
    "\n",
    "#Driver code \n",
    "# Object instantiation \n",
    "Rodger = Dog() \n",
    "  \n",
    "# Accessing class attributes \n",
    "# and method through objects \n",
    "print(Rodger.attr1) \n",
    "Rodger.fun() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Nikhil\n"
     ]
    }
   ],
   "source": [
    "# Class definition\n",
    "# https://www.geeksforgeeks.org/python-classes-and-objects/ \n",
    "class Person:  \n",
    "    \n",
    "    # init method or constructor   \n",
    "    def __init__(self, name):  \n",
    "        self.name = name  \n",
    "    \n",
    "    # Sample Method   \n",
    "    def say_hi(self):  \n",
    "        print('Hello, my name is', self.name)  \n",
    "    \n",
    "p = Person('Nikhil')  \n",
    "p.say_hi() \n",
    "\n",
    "## THIS RETURNS AN ERROR \n",
    "## WE NEED TO INITIALIZE THE CLASS WITH NAME\n",
    "#p = Person()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472136 , -1.25275497],\n",
       "       [ 3.13049517, -1.25275497],\n",
       "       [ 1.34164079,  5.18998488],\n",
       "       [ 1.34164079, -1.25275497]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Implementation with a Class\n",
    "\n",
    "class Scaler(object):\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.nanmean(X, axis=0)\n",
    "        self.std = np.nanstd(X, axis=0)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "        \n",
    "scaler = Scaler()\n",
    "scaler.fit(data)\n",
    "transformed_test_data_3 = scaler.transform(test_data)\n",
    "transformed_test_data_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Scaler at 0x11be25590>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encoding Categorical Variables <a id='exo2'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often features are not given as continuous values but categorical. However, machine learning algorithms only accept numerical data as inputs. That is why we need to make sure categorical variables are encoded before passed in ML estimators.\n",
    "\n",
    "One encoder that is commonly used for categorical variables is [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Given `data` and `test_data`, implement a OneHotEncoder by yourself and then use the Sklearn implementation to make you got it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([['France'], ['USA'], ['Italy'], ['Japan'], ['UK'], ['Germany'], ['USA'], ['Japan']])\n",
    "test_data = np.array([['China'], ['USA'], ['Italy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOneHotEncoder(object):\n",
    "    \"\"\"re-implement one hot encoder\"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 'Germany', 'Italy', 'Japan', 'UK', 'USA'], dtype='<U7')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_columns = np.unique(test_data)\n",
    "list_of_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([['France'], ['USA'], ['Italy'], ['Japan'], ['UK'], ['Germany'], ['USA'], ['Japan']])\n",
    "data[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Germany'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-2c68514b38e2>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-2c68514b38e2>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    var i\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = np.array([['France'], ['USA'], ['Italy'], ['Japan'], ['UK'], ['Germany'], ['USA'], ['Japan']])\n",
    "\n",
    "#list_of_columns = np.unique(data)\n",
    "# i = 0\n",
    "# k = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l = []\n",
    "\n",
    "var i:\n",
    "for i in data[i][0]:\n",
    "    var k;\n",
    "    for k in list_of_columns[k]:\n",
    "        if i == k:\n",
    "            x = 1\n",
    "        else:\n",
    "            x=0\n",
    "    l.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the final l: [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0]]\n",
      "this is the final l: [0, 1, 0, 0, 0, 0, 1, 0]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0]]\n",
      "this is the final l: [0, 0, 1, 0, 0, 0, 0, 0]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0]]\n",
      "this is the final l: [0, 0, 0, 1, 0, 0, 0, 1]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1]]\n",
      "this is the final l: [0, 0, 0, 0, 1, 0, 0, 0]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0]]\n",
      "this is the final l: [0, 0, 0, 0, 0, 1, 0, 0]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0]]\n",
      "this is the final l: [0, 1, 0, 0, 0, 0, 1, 0]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0]]\n",
      "this is the final l: [0, 0, 0, 1, 0, 0, 0, 1]\n",
      "this is the status of the array:  [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for k in range(len(data)):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        if data[k][0] == data[i][0]:\n",
    "            l.append(1)\n",
    "            #print(\"this is l:\", l)\n",
    "        else :\n",
    "            l.append(0)\n",
    "            #print(\"this is l:\", l)\n",
    "    print(\"this is the final l:\", l)\n",
    "    x.append(l)\n",
    "    #x = np.append(x, l, axis = 1)\n",
    "    print(\"this is the status of the array: \",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='ignore', sparse=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use Sklearn OneHotEncoder and compare results on test_data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "test_data\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(data)\n",
    "enc.transform(test_data).toarray()\n",
    "# your code to use OneHotEncoder and check that you get the samed transformed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(data)\n",
    "enc.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dealing with Missing Data <a id='exo3'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning.\n",
    "\n",
    "For this, Sklearn has multiple ways to impute from missing data with the [Inpute module](https://scikit-learn.org/stable/modules/impute.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Re-implement the [`SimpleInputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) tranformer using `mean` strategy.\n",
    "- Test your implementation with `data` and `test_data`\n",
    "- Compare with transformed data using `Sklearn SimpleInputer`\n",
    "- Bonus: Implement for all 4 strategies (`mean`, `median`, `most_frequent` and `constant`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data solution\n",
    "\n",
    "import numpy as np\n",
    "data = np.array([[1, 3, 3], [2, np.nan, 6], [3, 9, 9]])\n",
    "test_data = np.array([[1, 1, 1], [1, np.nan, 1], [1, 1, 1]])\n",
    "data[np.isnan(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1, 3, 3], [2, np.nan, 6], [3, 9, 9]])\n",
    "test_data = np.array([[1, 1, 1], [1, np.nan, 1], [1, 1, 1]])\n",
    "\n",
    "data_other = np.array([[1, 3, 3], [2, np.nan, 6], [np.nan, 9, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.where(np.isnan(data))\n",
    "print(ind)\n",
    "\n",
    "ind_other = np.where(np.isnan(data_other))\n",
    "print(ind_other)\n",
    "#data[ind]\n",
    "#data[ind]\n",
    "ind_other[1]\n",
    "\n",
    "\n",
    "data_other[ind_other[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSimpleInputer(object):\n",
    "    \"\"\"Implement SimpleInputer \"\"\"\n",
    "\n",
    "    def ___init__(self, strategy=\"mean\"):\n",
    "        self.strategy = strategy\n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X, **kwargs):\n",
    "        self.mean = np.nanmean(X, axis=0)\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        #Find indicies that you need to replace\n",
    "        inds = np.where(np.isnan(X))\n",
    "        X[inds] = np.take(self.mean, inds[1])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "csi = CustomSimpleInputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To instance csi, apply method fit().\n",
    "# Fit is going to need the instance (called self)\n",
    "# And the output of fit is assigning a *mean* to the instance (called self)\n",
    "# Fit assigns self.mean. So in our case, self = csi\n",
    "# Value given to csi.mean\n",
    "\n",
    "csi.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 6., 6.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which is available here, now\n",
    "csi.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which is then used in transform - where we use self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use Sklearn Simple Inputer and compare transformed data using your custom implementation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "inpute = SimpleImputer(strategy=\"mean\")\n",
    "inpute.fit(data)\n",
    "inpute.transform(test_data)\n",
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Custom Transformers and Encoders <a id='exo4'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides a large collection of transformers and encoders but you might need to implement you own encoder to fit the needs of your data and problem.\n",
    "\n",
    "For this, there are two very useful Sklearn classes:\n",
    "1. [FunctionTransfomer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) which lets you Construct a transformer from an arbitrary callable.\n",
    "2. [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) and [TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) are base classes one can use to implement completely new custom encoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "With the Taxi Fare Prediction Challenge data:\n",
    "\n",
    "- Using `FunctionTransformer` implement a transformer that computes haversine distance between pickup and dropoff location\n",
    "- With `BaseEstimator` and `TransformerMixin`, implement a custom encoder that extract time features from `pickup_datetime`\n",
    "- Use these two new encoders to fit and transform the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/nicolasbancel/git/data')\n",
    "\n",
    "df = pd.read_csv('train.csv', nrows = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_vectorized(df, \n",
    "    start_lat=\"start_lat\", \n",
    "    start_lon=\"start_lon\", \n",
    "    end_lat=\"end_lat\", \n",
    "    end_lon=\"end_lon\"):\n",
    "\n",
    "    \"\"\" \n",
    "        Calculate the great circle distance between two points \n",
    "        on the earth (specified in decimal degrees).\n",
    "        Vectorized version of the haversine distance for pandas df\n",
    "        Computes distance in kms\n",
    "    \"\"\"\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n",
    "    dlon = lon_2_rad - lon_1_rad\n",
    "    dlat = lat_2_rad - lat_1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"hav_dist\"] = haversine_vectorized(df, start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "            end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>hav_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-15 17:26:21.0000001</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "      <td>1.030764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05 16:52:16.0000002</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "      <td>8.450134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-08-18 00:35:00.00000049</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "      <td>1.389525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-21 04:30:42.0000001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "      <td>2.799270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-09 07:51:00.000000135</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "      <td>1.999157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  fare_amount          pickup_datetime  \\\n",
       "0    2009-06-15 17:26:21.0000001          4.5  2009-06-15 17:26:21 UTC   \n",
       "1    2010-01-05 16:52:16.0000002         16.9  2010-01-05 16:52:16 UTC   \n",
       "2   2011-08-18 00:35:00.00000049          5.7  2011-08-18 00:35:00 UTC   \n",
       "3    2012-04-21 04:30:42.0000001          7.7  2012-04-21 04:30:42 UTC   \n",
       "4  2010-03-09 07:51:00.000000135          5.3  2010-03-09 07:51:00 UTC   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.844311        40.721319         -73.841610         40.712278   \n",
       "1        -74.016048        40.711303         -73.979268         40.782004   \n",
       "2        -73.982738        40.761270         -73.991242         40.750562   \n",
       "3        -73.987130        40.733143         -73.991567         40.758092   \n",
       "4        -73.968095        40.768008         -73.956655         40.783762   \n",
       "\n",
       "   passenger_count  hav_dist  \n",
       "0                1  1.030764  \n",
       "1                1  8.450134  \n",
       "2                2  1.389525  \n",
       "3                1  2.799270  \n",
       "4                1  1.999157  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(haversine_vectorized, kw_args=dict(start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "                                                                end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\"))\n",
    "\n",
    "hav_dist = transformer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.030764\n",
       "1      8.450134\n",
       "2      1.389525\n",
       "3      2.799270\n",
       "4      1.999157\n",
       "         ...   \n",
       "995    8.131868\n",
       "996    6.833256\n",
       "997    9.991246\n",
       "998    1.544828\n",
       "999    3.169336\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hav_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hav_dist\"] = transformer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.030764\n",
       "1    8.450134\n",
       "2    1.389525\n",
       "3    2.799270\n",
       "4    1.999157\n",
       "5    3.787239\n",
       "6    1.555807\n",
       "7    4.155444\n",
       "8    1.253232\n",
       "9    2.849627\n",
       "Name: hav_dist, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"hav_dist\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeaturesEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, time_column, time_zone_name='America/New_York'):\n",
    "        self.time_column = time_column\n",
    "        self.time_zone_name = time_zone_name\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        X.index = pd.to_datetime(X[self.time_column])\n",
    "        X.index = X.index.tz_convert(self.time_zone_name)\n",
    "        X[\"dow\"] = X.index.weekday\n",
    "        X[\"hour\"] = X.index.hour\n",
    "        X[\"month\"] = X.index.month\n",
    "        X[\"year\"] = X.index.year\n",
    "        return X[[\"dow\", \"hour\", \"month\", \"year\"]].reset_index(drop=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TimeFeaturesEncoder(\"pickup_datetime\")\n",
    "tf.transform(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together as a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline is very useful concept. In Machine Learning, you often need to perform a sequence of different transformations (scaling, filling missing values, transforming, encoding) of raw dataset before applying a final estimator.\n",
    "\n",
    "A Pipeline gives you a simple interface for all these different steps of transformation and the resulting estimator. With that, it is easier to iterate and improve models because you can easily add, remove or re-order these different steps. Also, changing one or several parameters is very strightforward and does not require a lot code refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you will learn how to use 2 Sklearn modules:\n",
    "1. [ColumnTransformer](#exo11)\n",
    "2. [Pipeline](#exo12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Column Transformer <a id=\"exo11\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building your pipeline let's use a very useful Sklearn module called [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).\n",
    "\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space.\n",
    "\n",
    "This module is very useful when your input data is a pandas dataframe as you can select columns from their names.\n",
    "\n",
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a small dataset containing weights and heights for a few individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.DataFrame(\n",
    "    [\n",
    "        {'gender': 'Male', 'height': 180, 'weight': 82},\n",
    "        {'gender': 'Female', 'height': np.nan, 'weight': 72},\n",
    "        {'gender': 'Male', 'height': 175, 'weight': 75},\n",
    "        {'gender': 'Female', 'height': 175, 'weight': 60},\n",
    "        {'gender': 'Male', 'height': 170, 'weight': 76},\n",
    "    ])\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    [\n",
    "        {'gender': 'Male', 'height': 170, 'weight': 72},\n",
    "        {'gender': 'Female', 'height': np.nan, 'weight': 60}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ColumnTransformer`, build a single encoder that apply these transformations:\n",
    "- encode `gender` with OneHot\n",
    "- fill missing values for height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = ColumnTransformer([\n",
    "    ('gender', OneHotEncoder(), ['weight']),\n",
    "    ('fill_missing', SimpleImputer(), ['height'])])\n",
    "transformed_data = encoder.fit_transform(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('height',\n",
       "                                 SimpleImputer(add_indicator=False, copy=True,\n",
       "                                               fill_value=None,\n",
       "                                               missing_values=nan,\n",
       "                                               strategy='mean', verbose=0),\n",
       "                                 ['height', 'weight']),\n",
       "                                ('gender',\n",
       "                                 OneHotEncoder(categories='auto', drop=None,\n",
       "                                               dtype=<class 'numpy.float64'>,\n",
       "                                               handle_unknown='ignore',\n",
       "                                               sparse=True),\n",
       "                                 ['gender'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = ColumnTransformer([\n",
    "    ('height', SimpleImputer(missing_values=np.nan, strategy='mean'), ['height', 'weight']),\n",
    "    ('gender', OneHotEncoder(handle_unknown='ignore'), ['gender'])]\n",
    ")\n",
    "encoder.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b59808506bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# need the transformed data) to have consistent output type in predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_names_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_transformers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "# Version de Raphael\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = ColumnTransformer(\n",
    "    (['height', 'weight'], SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    (['gender'], OneHotEncoder(handle_unknown='ignore'))\n",
    ")\n",
    "encoder.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ColumnTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bac8bd929501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m ct = ColumnTransformer(\n\u001b[0m\u001b[1;32m      4\u001b[0m      [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n\u001b[1;32m      5\u001b[0m       (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ColumnTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
    "      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
    "X = np.array([[0., 1., 2., 2.],\n",
    "               [1., 1., 0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pipeline <a id=\"exo12\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to use a Sklearn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "With the weight/height dataset, build a pipeline to predict the weight of individuals in the test set.\n",
    "\n",
    "This pipeline should have:\n",
    "- a oneHotEncode for `gender`\n",
    "- fill missing values for height\n",
    "- a scaler for height\n",
    "- a simple estimator like a linear regression\n",
    "\n",
    "**Tip** You can also use [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) which is an alias of `Pipeline` to easily generate a pipeline without giving names to the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "encoder = ColumnTransformer([\n",
    "    ('gender', OneHotEncoder(), ['gender']),\n",
    "    ('height_scaled', make_pipeline(SimpleImputer(), StandardScaler()), ['height'])\n",
    "                            ])\n",
    "\n",
    "pipe  = Pipeline(steps=[ ('features', encoder),\n",
    "                         ('clf', LassoCV()) ])\n",
    "\n",
    "pipe.fit(data, data.weight)\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor Taxi Fare Prediction Problem with a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor the model you built yesterday for the Taxi Fare Prediction Problem using:\n",
    "- Custom encoders you wrote for distance and time features\n",
    "- OneHot Encoder to encoder hour and day of week features\n",
    "- SimpleImputer to fill missing values\n",
    "- A simple linear regression\n",
    "- A pipeline to put all together\n",
    "\n",
    "\n",
    "Then: \n",
    "- train this pipeline\n",
    "- apply the pipeline on test data\n",
    "- generate predictions and submit these new predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
