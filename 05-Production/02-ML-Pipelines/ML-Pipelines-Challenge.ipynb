{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of exercices, you will learn how a build robust a ML pipeline using [Sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "An important part of a ML pipeline is the pre-processing part. For this, you will learn how to master \n",
    "Sklearn encoders and tranformers as part of the [Preprocessing Sklearn module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Scaling with StandardScaler](#exo1)\n",
    "2. [Encoding Categorical Features](#exo2)\n",
    "3. [Dealing with missing data](#exo3)\n",
    "4. [Custom Transformers and Encoders](#exo4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scaling with StandardScaler <a id='exo1'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features by removing the mean and scaling to unit variance is a common pre-processing step we apply to help many machine learning algorithms behave more efficiently.\n",
    "\n",
    "[Sklearn StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) can do the scaling transformation for you.\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "The goal of this exercice is to re-implement it.\n",
    "\n",
    "As you know, there are 2 main methods for any encoder/transformer. \n",
    "- `fit` which computes the mean and std to be used for later scaling.\n",
    "- `tranform` which performs standardization by centering and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "- Given the numpy arrays `data` and `test_data`, write a simple custom implementation of standard scaler. To test it, fit the scaler with `data` and tranform `test_data` with it.\n",
    "- Compare your results with `StandardScaler`\n",
    "- Make the custom implementation using a python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1, 10], [2, -1], [0, 22], [3, 15]])\n",
    "test_data = np.array([[2, 1], [5, 1], [3, 55], [3, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5, 11.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis \n",
    "np.nanmean(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Simple custom implementation of Standard Scaler\n",
    "\n",
    "def fit(X):\n",
    "    \"\"\"implement fit method\"\"\"\n",
    "    mean = np.nanmean(X, axis=0)\n",
    "    std = np.nanstd(X, axis=0)\n",
    "    return {\"mean\" : mean, \"std\" : std}\n",
    "\n",
    "\n",
    "def transform(X, params):\n",
    "    \"\"\"implement transformation method\"\"\"\n",
    "    return (X - params[\"mean\"]) / params[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472136 , -1.25275497],\n",
       "       [ 3.13049517, -1.25275497],\n",
       "       [ 1.34164079,  5.18998488],\n",
       "       [ 1.34164079, -1.25275497]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = fit(data)\n",
    "transformed_test_data = transform(test_data,params)\n",
    "transformed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472136 , -1.25275497],\n",
       "       [ 3.13049517, -1.25275497],\n",
       "       [ 1.34164079,  5.18998488],\n",
       "       [ 1.34164079, -1.25275497]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Sklearn StandardScaler and compare results\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# your code that uses StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "transformed_test_data_2 = scaler.transform(test_data)\n",
    "transformed_test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472136 , -1.25275497],\n",
       "       [ 3.13049517, -1.25275497],\n",
       "       [ 1.34164079,  5.18998488],\n",
       "       [ 1.34164079, -1.25275497]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Implementation with a Class\n",
    "\n",
    "class Scaler(object):\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.nanmean(X, axis=0)\n",
    "        self.std = np.nanstd(X, axis=0)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "        \n",
    "scaler = Scaler()\n",
    "scaler.fit(data)\n",
    "transformed_test_data_3 = scaler.transform(test_data)\n",
    "transformed_test_data_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encoding Categorical Variables <a id='exo2'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often features are not given as continuous values but categorical. However, machine learning algorithms only accept numerical data as inputs. That is why we need to make sure categorical variables are encoded before passed in ML estimators.\n",
    "\n",
    "One encoder that is commonly used for categorical variables is [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Given `data` and `test_data`, implement a OneHotEncoder by yourself and then use the Sklearn implementation to make you got it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([['France'], ['USA'], ['Italy'], ['Japan'], ['UK'], ['Germany'], ['USA'], ['Japan']])\n",
    "test_data = np.array([['China'], ['USA'], ['Italy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOneHotEncoder(object):\n",
    "    \"\"\"re-implement one hot encoder\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.mean = np.nanmean(X, axis=0)\n",
    "        self.std = np.nanstd(X, axis=0)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        for elem in X\n",
    "            \n",
    "        return (X - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='ignore', sparse=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use Sklearn OneHotEncoder and compare results on test_data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "test_data\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(test_data)\n",
    "enc.transform(test_data).toarray()\n",
    "# your code to use OneHotEncoder and check that you get the samed transformed_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dealing with Missing Data <a id='exo3'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning.\n",
    "\n",
    "For this, Sklearn has multiple ways to impute from missing data with the [Inpute module](https://scikit-learn.org/stable/modules/impute.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Re-implement the [`SimpleInputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) tranformer using `mean` strategy.\n",
    "- Test your implementation with `data` and `test_data`\n",
    "- Compare with transformed data using `Sklearn SimpleInputer`\n",
    "- Bonus: Implement for all 4 strategies (`mean`, `median`, `most_frequent` and `constant`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[1, 3, 3], [2, np.nan, 6], [3, 9, 9]])\n",
    "test_data = np.array([[1, 1, 1], [1, np.nan, 1], [1, 1, 1]])\n",
    "\n",
    "data_other = np.array([[1, 3, 3], [2, np.nan, 6], [np.nan, 9, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1]), array([1]))\n",
      "(array([1, 2]), array([1, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2., nan,  6.],\n",
       "       [ 1.,  3.,  3.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.where(np.isnan(data))\n",
    "print(ind)\n",
    "\n",
    "ind_other = np.where(np.isnan(data_other))\n",
    "print(ind_other)\n",
    "#data[ind]\n",
    "#data[ind]\n",
    "ind_other[1]\n",
    "\n",
    "\n",
    "data_other[ind_other[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSimpleInputer(object):\n",
    "    \"\"\"Implement SimpleInputer \"\"\"\n",
    "\n",
    "    def ___init__(self, strategy=\"mean\"):\n",
    "        self.strategy = strategy\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.mean = np.nanmean(X, axis=0)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 6., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use Sklearn Simple Inputer and compare transformed data using your custom implementation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "inpute = SimpleImputer(strategy=\"mean\")\n",
    "inpute.fit(data)\n",
    "inpute.transform(test_data)\n",
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Custom Transformers and Encoders <a id='exo4'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides a large collection of transformers and encoders but you might need to implement you own encoder to fit the needs of your data and problem.\n",
    "\n",
    "For this, there are two very useful Sklearn classes:\n",
    "1. [FunctionTransfomer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) which lets you Construct a transformer from an arbitrary callable.\n",
    "2. [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) and [TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) are base classes one can use to implement completely new custom encoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "With the Taxi Fare Prediction Challenge data:\n",
    "\n",
    "- Using `FunctionTransformer` implement a transformer that computes haversine distance between pickup and dropoff location\n",
    "- With `BaseEstimator` and `TransformerMixin`, implement a custom encoder that extract time features from `pickup_datetime`\n",
    "- Use these two new encoders to fit and transform the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/nicolasbancel/git/data')\n",
    "\n",
    "df = pd.read_csv('train.csv', nrows = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_vectorized(df, \n",
    "    start_lat=\"start_lat\", \n",
    "    start_lon=\"start_lon\", \n",
    "    end_lat=\"end_lat\", \n",
    "    end_lon=\"end_lon\"):\n",
    "\n",
    "    \"\"\" \n",
    "        Calculate the great circle distance between two points \n",
    "        on the earth (specified in decimal degrees).\n",
    "        Vectorized version of the haversine distance for pandas df\n",
    "        Computes distance in kms\n",
    "    \"\"\"\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n",
    "    dlon = lon_2_rad - lon_1_rad\n",
    "    dlat = lat_2_rad - lat_1_rad\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hav_dist\"] = haversine_vectorized(df, start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "            end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(haversine_vectorized, kw_args=dict(start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n",
    "                                                                end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\"))\n",
    "\n",
    "hav_dist = transformer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.030764\n",
       "1      8.450134\n",
       "2      1.389525\n",
       "3      2.799270\n",
       "4      1.999157\n",
       "         ...   \n",
       "995    8.131868\n",
       "996    6.833256\n",
       "997    9.991246\n",
       "998    1.544828\n",
       "999    3.169336\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hav_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hav_dist\"] = transformer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.030764\n",
       "1    8.450134\n",
       "2    1.389525\n",
       "3    2.799270\n",
       "4    1.999157\n",
       "5    3.787239\n",
       "6    1.555807\n",
       "7    4.155444\n",
       "8    1.253232\n",
       "9    2.849627\n",
       "Name: hav_dist, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"hav_dist\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeaturesEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, time_column, time_zone_name='America/New_York'):\n",
    "        self.time_column = time_column\n",
    "        self.time_zone_name = time_zone_name\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        X.index = pd.to_datetime(X[self.time_column])\n",
    "        X.index = X.index.tz_convert(self.time_zone_name)\n",
    "        X[\"dow\"] = X.index.weekday\n",
    "        X[\"hour\"] = X.index.hour\n",
    "        X[\"month\"] = X.index.month\n",
    "        X[\"year\"] = X.index.year\n",
    "        return X[[\"dow\", \"hour\", \"month\", \"year\"]].reset_index(drop=True)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow  hour  month  year\n",
       "0    0    13      6  2009\n",
       "1    1    11      1  2010\n",
       "2    2    20      8  2011\n",
       "3    5     0      4  2012\n",
       "4    1     2      3  2010"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TimeFeaturesEncoder(\"pickup_datetime\")\n",
    "tf.transform(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together as a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline is very useful concept. In Machine Learning, you often need to perform a sequence of different transformations (scaling, filling missing values, transforming, encoding) of raw dataset before applying a final estimator.\n",
    "\n",
    "A Pipeline gives you a simple interface for all these different steps of transformation and the resulting estimator. With that, it is easier to iterate and improve models because you can easily add, remove or re-order these different steps. Also, changing one or several parameters is very strightforward and does not require a lot code refactoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you will learn how to use 2 Sklearn modules:\n",
    "1. [ColumnTransformer](#exo11)\n",
    "2. [Pipeline](#exo12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Column Transformer <a id=\"exo11\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building your pipeline let's use a very useful Sklearn module called [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html).\n",
    "\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space.\n",
    "\n",
    "This module is very useful when your input data is a pandas dataframe as you can select columns from their names.\n",
    "\n",
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a small dataset containing weights and heights for a few individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    [\n",
    "        {'gender': 'Male', 'height': 180, 'weight': 82},\n",
    "        {'gender': 'Female', 'height': np.nan, 'weight': 72},\n",
    "        {'gender': 'Male', 'height': 175, 'weight': 75},\n",
    "        {'gender': 'Female', 'height': 175, 'weight': 60},\n",
    "        {'gender': 'Male', 'height': 170, 'weight': 76},\n",
    "    ])\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    [\n",
    "        {'gender': 'Male', 'height': 170, 'weight': 72},\n",
    "        {'gender': 'Female', 'height': np.nan, 'weight': 60}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ColumnTransformer`, build a single encoder that apply these transformations:\n",
    "- encode `gender` with OneHot\n",
    "- fill missing values for height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = ColumnTransformer([\n",
    "    ('gender', OneHotEncoder(), ['weight']),\n",
    "    ('fill_missing', SimpleImputer(), ['height'])])\n",
    "transformed_data = encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   1., 180.],\n",
       "       [  0.,   1.,   0.,   0.,   0., 175.],\n",
       "       [  0.,   0.,   1.,   0.,   0., 175.],\n",
       "       [  1.,   0.,   0.,   0.,   0., 175.],\n",
       "       [  0.,   0.,   0.,   1.,   0., 170.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>180.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>175.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>175.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>170.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  height  weight\n",
       "0    Male   180.0      82\n",
       "1  Female     NaN      72\n",
       "2    Male   175.0      75\n",
       "3  Female   175.0      60\n",
       "4    Male   170.0      76"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(encoder.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>180.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>175.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>175.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>170.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  height  weight\n",
       "0    Male   180.0      82\n",
       "1  Female     NaN      72\n",
       "2    Male   175.0      75\n",
       "3  Female   175.0      60\n",
       "4    Male   170.0      76"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(3, 0), dtype=float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
    "      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
    "X = np.array([[0., 1., 2., 2.],\n",
    "               [1., 1., 0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 0.5, 0.5],\n",
       "       [0.5, 0.5, 0. , 1. ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,   0.,   0.,   0., 170.],\n",
       "       [  1.,   0.,   0.,   0.,   0., 175.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pipeline <a id=\"exo12\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to use a Sklearn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "With the weight/height dataset, build a pipeline to predict the weight of individuals in the test set.\n",
    "\n",
    "This pipeline should have:\n",
    "- a oneHotEncode for `gender`\n",
    "- fill missing values for height\n",
    "- a scaler for height\n",
    "- a simple estimator like a linear regression\n",
    "\n",
    "**Tip** You can also use [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) which is an alias of `Pipeline` to easily generate a pipeline without giving names to the transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('gender',\n",
       "                                                  OneHotEncoder(categories='auto',\n",
       "                                                                drop=None,\n",
       "                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                handle_unknown='error',\n",
       "                                                                sparse=True),\n",
       "                                                  ['gender']),\n",
       "                                                 ('height_scaled',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImpute...\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  with_mean=True,\n",
       "                                                                                  with_std=True))],\n",
       "                                                           verbose=False),\n",
       "                                                  ['height'])],\n",
       "                                   verbose=False)),\n",
       "                ('clf',\n",
       "                 LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001,\n",
       "                         fit_intercept=True, max_iter=1000, n_alphas=100,\n",
       "                         n_jobs=None, normalize=False, positive=False,\n",
       "                         precompute='auto', random_state=None,\n",
       "                         selection='cyclic', tol=0.0001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "encoder = ColumnTransformer([\n",
    "    ('gender', OneHotEncoder(), ['gender']),\n",
    "    ('height_scaled', make_pipeline(SimpleImputer(), StandardScaler()), ['height'])\n",
    "                            ])\n",
    "\n",
    "pipe  = Pipeline(steps=[ ('features', encoder),\n",
    "                         ('clf', LassoCV()) ])\n",
    "\n",
    "pipe.fit(data, data.weight)\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74.66391325, 66.08048299])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor Taxi Fare Prediction Problem with a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor the model you built yesterday for the Taxi Fare Prediction Problem using:\n",
    "- Custom encoders you wrote for distance and time features\n",
    "- OneHot Encoder to encoder hour and day of week features\n",
    "- SimpleImputer to fill missing values\n",
    "- A simple linear regression\n",
    "- A pipeline to put all together\n",
    "\n",
    "\n",
    "Then: \n",
    "- train this pipeline\n",
    "- apply the pipeline on test data\n",
    "- generate predictions and submit these new predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
