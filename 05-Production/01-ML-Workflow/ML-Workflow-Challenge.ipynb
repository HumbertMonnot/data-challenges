{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - ML Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this exercise is to use the tools and methods you learnt during the previous weeks, in order to solve a **Kaggle challenge**.\n",
    "\n",
    "#### The problem to solve is the [New York City Taxi Fare Prediction Challenge](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction). \n",
    "\n",
    "The goal is to predict the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, you are going to follow the different steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. [Get the data](#part1)\n",
    "2. [Explore the data](#part2)\n",
    "3. [Data cleaning](#part3)\n",
    "4. [Evaluation metric](#part4)\n",
    "5. [Model baseline](#part5)\n",
    "6. [Build your first model](#part6)\n",
    "7. [Model evaluation](#part7)\n",
    "8. [Kaggle submission](#part8)\n",
    "9. [Model iteration](#part9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available on [Kaggle](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data)\n",
    "\n",
    "First of all:\n",
    "- Follow the instructions to download the training and test sets\n",
    "- Put the datasets in a separate folder on your local disk, that you can name `data` for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use Pandas to read and explore the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is relatively big (~2GB). \n",
    "So let's only open a portion of it.\n",
    "Go to [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/) to see how to open a portion of a csv file and store it into a dataframe. (ex: just read 1 million rows maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display the first rows to understand the different fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to solve the prediction problem, we need to get a better understanding of the data. \n",
    "For that, libraries like Pandas and Seaborn are your best friends. \n",
    "Firt of all, make you sure you have [Seaborn](https://seaborn.pydata.org/) installed and import it into your notebook. Note that this can be also useful to import `matplotlib.pyplot` to customize a few things like default `figsize` or `font.size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use(style)\n",
    "#plt.rcParams[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are multiple things we want to do in terms of data exploration.\n",
    "\n",
    "- You first want to look at the distribution of the variable you are going to predict: `fare_amount`\n",
    "- Then you want to vizualize other variable distributions\n",
    "- Then it is helpful to compute and visualize correlations between target variable and other variables.\n",
    "- Also, look for any missing or wrong data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the target variable\n",
    "- Compute simple statistics of the target variable (min, max, mean, std, ...)\n",
    "- Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore other variables\n",
    "\n",
    "- passenger_count (statistics + distribution)\n",
    "- pickup_datetime (you need to build time features out of pickup datetime)\n",
    "- Geospatial features (pickup_longitude, pickup_latitude,dropoff_longitude,dropoff_latitude)\n",
    "- Find other variables you can compute from existing data that might explain the target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passenger Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickup Datetime \n",
    "- Extract time features from pickup_datetime (hour, day of week, month, year)\n",
    "- Create a method `def extract_time_features(_df)` that you will be able to re-use later\n",
    "- Be careful of timezone\n",
    "- Explore the newly created features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(_df):\n",
    "    return _df\n",
    "\n",
    "# df = extract_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to explore time feature (hour, day of week, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geospatial Data\n",
    "- check for absurd (lat, lng) coordinates and inspect test set\n",
    "- To visuzalize geospatial data, you can use libraries like [Folium](https://python-visualization.github.io/folium/)\n",
    "- check out this [Great example](https://www.kaggle.com/daveianhickey/how-to-folium-for-maps-heatmaps-time-data) on kaggle\n",
    "- Tip: Look for HeatMap to generate a heatmap\n",
    "- Bonus: Look for HeatMapWithTime to generate an animated heatmmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import HeatMapWithTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code to explore geospatial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance\n",
    "- Compute distance between pickup and dropoff locations (tip: https://en.wikipedia.org/wiki/Haversine_formula)\n",
    "- Write a method `def haversine_distance(df, **kwargs)` that you will be able to reuse later\n",
    "- Compute a few statistics for distance and plot distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, **kwargs):\n",
    "    df[\"distance\"] = None\n",
    "    return df\n",
    "\n",
    "# df = haversine_distance(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to explore distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore how target variable correlate with other variables\n",
    "- As a first step, you can vizualize the target variable vs another variable. For categorical variables, it is often useful to compute the average target variable for each category (Seaborn as plots that do it for you!). For continuous variables (like distance, you can use scatter plots, or regression plots, or bucket the distance into different bins.\n",
    "- There many different ways to visualize correlation between features, so be creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data cleaning <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably identified during data exploration, there are some values that do not seem valid.\n",
    "In this section, you will take a few steps to clean the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all trips that look incorrect.\n",
    "- Write a method `clean_data(df)` that you will be able to re-use in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(_df):\n",
    "    return _df\n",
    "\n",
    "#df_cleaned = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation metric <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric for this competition is the root mean-squared error or RMSE. RMSE measures the difference between the predictions of a model, and the corresponding ground truth. A large RMSE is equivalent to a large average error, so smaller values of RMSE are better.\n",
    "\n",
    "More details here https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method `def compute_rmse(y_pred, y_true)` that computes the RMSE given `y_pred` and `y_true` which are two numpy arrays corresponding to model predictions and ground truth values.\n",
    "\n",
    "This method will be useful to evaluate performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(y_pred, y_true):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model baseline <a id='part5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building your model, it is often useful to get a performance benchmark. For this, you will use a baseline model that is a very stupid model and compute the evualation metric on that model.\n",
    "Then, you will be able to see how much better your model is compared to the baseline. It is very common to see ML teams comming up with very sophisticated approaches without knowing by how much their model beats the very simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate predictions based on a simple heuristic\n",
    "- Evaluate RMSE for these predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fare_amount_predicted\"] = None # heuristic to make simple predictions\n",
    "compute_rmse(df.fare_amount_predicted, df.fare_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build your first model <a id='part6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the different steps you have to follow:\n",
    "\n",
    "1. Split the data into two different sets (training and validation). You will be measuring the performance of your model on the validation set.\n",
    "2. Make sure you apply the data cleaning on your training set\n",
    "3. Think about the different features you want to add in your model\n",
    "4. For each of these features, make sure you apply the correct transformation so that the model can correctly learn from them (this is true for categorical variables like `hour of day` or `day of week`)\n",
    "5. Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for raining/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply data cleaning on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List features (continuous vs categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your features\n",
    "target = \"\"\n",
    "features = []\n",
    "categorical_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features transformation\n",
    "- Write a method `def transform_features(df, **kwargs)` because you will have to make sure you apply the same transformation on the validation (or test set) before making predictions\n",
    "- For categorical features transformation, you can use `pandas.get_dummies` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(_df, **kwargs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model evaluation <a id='part7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to evaluate your model, you need to use your previously trained model to make predictions on the validation set. \n",
    "\n",
    "For this, follow these steps:\n",
    "1. Apply the same transformations on the validation set\n",
    "2. Make predictions\n",
    "3. Evaluate predictions using `compute_rmse` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for model evaluation of the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Kaggle submission <a id='part8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a model, you can now make predictions on Kaggle test set and be evaluated by Kaggle directly.\n",
    "\n",
    "- Download test data from Kaggle\n",
    "- Follow [instructions](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation) to make sure your predictions are in the right format\n",
    "- Re-train your model using all the data (do not split between train/validation)\n",
    "- Apply all features engineering and transformations methods on the test set\n",
    "- Use the model to make predictions on the test set\n",
    "- Submit your predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model iteration <a id='part9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve your model by trying different things (But dont' worry, some of these things will be covered in the next days).\n",
    "- Use more data to train\n",
    "- Build and add more features \n",
    "- Try different estimators\n",
    "- Adjust your data cleaning to remove more or less data\n",
    "- Tune the hyperparameters of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Another Distance ?\n",
    "- Think about the distance you used, try and find a more adapted distance to our problem (Ask TA for insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distance from the center \n",
    "- Compute a new Feature calculating distance of pickup location from the center\n",
    "- Scatter Plot *distance_from_center* regarding *distance* \n",
    "- What do you observe ? What new features could you add ? How are these new features correlated to the target ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which direction  you heading to ?\n",
    "- Compute a new Feature calculating the direction your heading to\n",
    "- What do you observe ? What new features could you add ? How are these new features correlated to the target ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
