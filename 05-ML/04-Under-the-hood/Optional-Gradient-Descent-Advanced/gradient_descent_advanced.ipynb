{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercice, we will\n",
    "- create a very large dataset in terms of both observations and features\n",
    "- model it with a OLS linear regression\n",
    "- code our gradient descent (and a stochastic one) in vectorized form\n",
    "- Investigate early stopping to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "X,y = datasets.load_diabetes(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCreate a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code your vectorial gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're modelling a linear regression $\\hat{y} = X\\beta$\n",
    "\n",
    "Let's recall the definition of the gradient descent algorithm\n",
    "\n",
    "$$\\text{Gradient descent - vector formula}$$\n",
    "$$\\beta^{\\color {red}{(k+1)}} = \\beta^{\\color {red}{(k)}} - \\eta \\ \\nabla L(\\beta^{\\color{red}{(k)}})$$\n",
    "\n",
    "The MSE Loss for an OLS regression is\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n}\\|X \\beta - y\\|^2 = \\frac{1}{n}(X \\beta - y)^T(X \\beta - y)$$\n",
    "\n",
    "and its gradient is\n",
    "$${\\displaystyle \\nabla L(\\beta)=\n",
    "{\\begin{bmatrix}{\\frac {\\partial L}{\\partial \\beta_{0}}}(\\beta)\\\\\\vdots \\\\{\\frac {\\partial L}{\\partial \\beta_{p}}}(\\beta)\\end{bmatrix}} = \\frac{2}{n} X^T (X\\beta - y) \n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store below our main problem parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # default learning rate\n",
    "n_epochs = 10000 # default number of epochs in our gradient descent\n",
    "n = X.shape[0] # n observations\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "p = X.shape[1] # p features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "‚ùì Initialize a $\\beta$ vector as an ndarray of the right shape for our problem with the values of your choice (zeros, for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using the vectorized formula given above, create a gradient descent that loops over `n_epochs` to find the best $\\beta$ of an OLS using the `train` set\n",
    "- make use of numpy's matrix operations and broadcasting capabilities\n",
    "- this shouldn't take more than 4 lines of code!\n",
    "- use `%%time` to keep track of computation time, and and [`tqdm`](https://pypi.org/project/tqdm/) to display the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results with sklearn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute your `mse_test` metrics, and compare your results (mse and computation time) with\n",
    "- a Sklearn LinearRegression()\n",
    "- SGDRegressor, with similar epochs and learning_rate, adding 'penalty'= None to remove regularization that we will see in ML-05-Model-Tuning)\n",
    "- (optional) cross_validate all your metrics to be sure of your scores\n",
    "\n",
    "Are you better? Faster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Wrap this logic into a function `gradient_descent`, which takes any (X_train, y_train, X_test, y_test, eta, n_epoch) as input, and returns \n",
    "- the final value for $\\beta$ fitted on the train set\n",
    "- the values of the `loss_train` at each epoch as a list `loss_train_history`\n",
    "- the values of the `loss_test` at each epoch as a list `loss_test_history`\n",
    "- (optional) make the fonction robust to call with only a train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_test=None, y_test=None, eta=eta, n_epochs=n_epochs):\n",
    "    # YOUR CODE HERE    \n",
    "    return beta, loss_train_history, loss_test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping criteria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìPlot the loss as a function of epochs, on your train dataset. \n",
    "- Try it with `n_epochs=50000` and `eta=0.1` as per initially set\n",
    "- What can you conclude? Should you always descent gradient down to the absolute minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Try to improve your own model MSE by coding an **early stopping criteria** where you would stop descending gradient as soon as `loss_test` increases again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Did you notice...you have just done a data-leak! Can you guess why? Think about a solution to this problem!\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "- You have used your test set to decide when to stop descending gradient\n",
    "- You should never use your test set to optimize your model `hyperparameters`\n",
    "- Create a train/test split **within** your current training set and optmize your early stopping based on the loss on this new test set only. This one is called a \"validation set\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code your own MiniBatch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìModify your `gradient_descent` function into a `minibatch_gradient_descent` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X_train, y_train, X_test=None, y_test=None, batch_size=16, eta=eta, n_epochs=n_epochs):\n",
    "    # YOUR CODE HERE\n",
    "    return beta, loss_train_history, loss_test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Plot the evolution of your train and val losses per epoch. What if you choosed minibatch = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to narrow performance gap between your self-made model and SKLearn by\n",
    "- changing initial learning_rate `eta`\n",
    "- gradually reduce learning_rate in your `minibatch_gradient_descent` function\n",
    "- optimize your minibatch code so as to paralellize it better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†‚ö†Ô∏è Please, push your exercice when you are done üôÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
