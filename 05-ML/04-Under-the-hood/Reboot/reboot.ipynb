{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our test dataset for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"x\":x, \"y\":y}).to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\"data.csv\",index_col=0)\n",
    "\n",
    "x = tmp[\"x\"].values\n",
    "y = tmp[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y,s=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put ourselves in different modeling complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's create polynomial features again, up to 10 degrees using the [Polynomial transformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) (don't forget to remove the degree 0 polynomial feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 10)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=10,include_bias=False)\n",
    "\n",
    "xx = pf.fit_transform(x.reshape(-1,1))\n",
    "\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's put aside some of the data in order to evaluate the general performance - split into `xtrain, xtest, ytrain, ytest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We fit a linear regression over the **original** dataset, save the prediction in `y_predict_1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lg1 = LinearRegression()\n",
    "\n",
    "lg1.fit(xtrain[:,0].reshape(-1,1),ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We fit two more linear regression, one using 5 features and one using all the features. Name them respectively `y_predict_5` and `y_predict_10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x5 = xtrain[:,:3]\n",
    "\n",
    "lg5 = LinearRegression()\n",
    "lg5.fit(x5,ytrain)\n",
    "\n",
    "lg15 = LinearRegression()\n",
    "lg15.fit(xtrain,ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 3 different trained models, with three different capacities of prediction. The higher the polynome used, the more complex the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We can now plot the three lines corresponding to the three different models on three different plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ordered = np.sort(xtrain,axis=0)\n",
    "y_predict_1 = lg1.predict(x_ordered[:,0].reshape(-1,1))\n",
    "y_predict_3 = lg5.predict(x_ordered[:,:3])\n",
    "y_predict_15 = lg15.predict(x_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(x,y,s=1.5)\n",
    "plt.plot(x_ordered[:,0], y_predict_1, c=\"r\")\n",
    "plt.title('1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(x,y,s=1.5)\n",
    "plt.plot(x_ordered[:,0], y_predict_3, c=\"r\")\n",
    "plt.title('3')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(x,y,s=1.5)\n",
    "plt.plot(x_ordered[:,0], y_predict_15, c=\"r\")\n",
    "plt.title('15')\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, because \n",
    "1. We are sensitive to outliers that don't represent our data\n",
    "2. We are overfitting the training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the MSE performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(lg1.predict(xtest[:,0].reshape(-1,1)),ytest))\n",
    "print(mean_squared_error(lg5.predict(xtest[:,:3]),ytest))\n",
    "print(mean_squared_error(lg15.predict(xtest),ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which regularization term do we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a Ridge (L2) and a Lasso (L1), with different alpha values (how much influence the penalization terms will have).\n",
    "\n",
    "❓Train an Ridge for both L1 and L2, for each alpha values listed below. Save those into an array, as well as the score and the best ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "coefs = []\n",
    "alphas = np.logspace(-10, 3, 100);\n",
    "\n",
    "best_score=0;\n",
    "best_ridge=0;\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    # create a ridge regression with given alpha\n",
    "    \n",
    "    # fit the regression\n",
    "    \n",
    "    # append the coefs found by the regression\n",
    " \n",
    "    # save the score\n",
    "    \n",
    "    # see if the score is better than the best score we got so far. If so, save this regression as the best one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓We can now compare our best ridge to the previous best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x,y,s=1.5)\n",
    "plt.plot(x_ordered[:,0], best_ridge.predict(x_ordered), c=\"r\")\n",
    "plt.title('ridge');\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x,y,s=1.5)\n",
    "plt.plot(x_ordered[:,0], y_predict_15, c=\"r\")\n",
    "plt.title('linear reg 15 polynome')\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓We want to see the impact of the penalization term. For every alpha, we can plot the coeffs/weights of the associated linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot every coef as a function of alpha (very straightforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Ridge regression is impacting the weights smoothly but L2 is even removing some"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothetically, we could go further in the modelisation work and try other "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
