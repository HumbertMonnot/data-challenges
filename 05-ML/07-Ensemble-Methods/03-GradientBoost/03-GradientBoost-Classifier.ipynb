{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, weâ€™ll use Gradient Boosting to classify tumors. We'll assume that tumors can be benign or malignant. To make this exercise, we'll use the dataset from the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "y = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "As usual, try to get more comfortable with the data and try to understand the meaning of each feature (column) and have a look at the target column. Also you should try to make some plots to visualize the data.\n",
    "\n",
    "Answer the following questions : \n",
    "1. what is the type of each column (for the features)\n",
    "2. how many features do we have for each sample?\n",
    "3. how many samples do we have in the dataset?\n",
    "4. Is this a classification or regression problem? If classification, what are the different categories?\n",
    "5. How many malignant tumors do we have in the dataset?\n",
    "6. How many benign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and \"clean\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should have noticed, the target is a **categorical feature**. So we should start by convert this categories into numbers, using the `LabelEncoder` class of scikit learn. For this problem, weâ€™ll set malignant to 1 and benign to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 60/40 between training and test sets thanks to **train_test_split** from scikit learn. Add the option `random_state=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, train, predict and measure with Gradient Boosting\n",
    "\n",
    "You should be familiar with these steps by now : \n",
    "- initialize your model\n",
    "- train (\"fit\") your model on the training data\n",
    "- make predictions on the test data\n",
    "- evaluate the performance of your model\n",
    "\n",
    "We will use max_depth=1. As we explained before, we want weak (but fast) learner. max_depth=1 is a good way to do that. Basically we tell our model that weâ€™d like our forest to be composed of trees with a single decision node and two leaves. \n",
    "\n",
    "**quick reminder**: n_estimators specifies the number of trees in our forest.\n",
    "\n",
    "For this part of the exercise, you have to: \n",
    "- create an instance of GradientBoostingClassifier apply on a DecisionTreeClassifier with max_depth=1 and 50 trees in your forest\n",
    "- train your classifier on the training data\n",
    "- predict whether a tumor is malignant or benign on the test set\n",
    "- evaluate the model using a **confusion matrix**. Note : do not use this method blindly. False Positive, True Negative,... are an important concept in machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you answer these questions : \n",
    "1. How many false positive do we have? How does that translate in that context?\n",
    "2. How many false negative do we have? How does that translate in that context?\n",
    "3. How many true negative do we have? How does that translate in that context?\n",
    "4. What are the index(es) of the false positive? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise further our classifier\n",
    "\n",
    "### Find the number of optimal trees\n",
    "\n",
    "We can plot the number of estimators needed to get the best performance for our current problem. You should see when the error gets asymptotical by plotting increasing number of trainings.\n",
    "\n",
    "**ðŸ‘‰Plot the training error for the Gradient Boosting in function of the number of trees to find the optimal number of trees needed**\n",
    "\n",
    "Hint: you can access to those submodel trainings through the `staged_predict` method of Gradient Boosting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We could feed this classifier to a logistic regression** in order to improve the decision. **Each leaf position is gonna be the new feature we construct for our samples**, with what is called a \"one-hot\" encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰Look at [the `apply` method of the gradient boosting class](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.apply)\n",
    "\n",
    "The idea is to use this `apply` method to get new features, that we can encode as one hot encoding vectors.\n",
    "\n",
    "**ðŸ‘‰Fit and transform a one hot encoder over the leaf position of every entry in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ‘‰Use those new hot encoded features as an input for a logistic regression model, and compare the score with the previous score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This additional prediction layer can be useful for more complex modelisation problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "ROC curves are extensively used for classification problems.\n",
    "\n",
    "You can plot both ROC curves zoomed out on the decision area \n",
    "\n",
    "Hint: [to plot the ROC curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html) (False Positive Rate (FPR) on the X-axis and True Positive Rate on the Y-axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Here we have nearly similar results because the dataset is small but encoding new features from a boosting classifier to feed another simpler estimator can be very powerful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
