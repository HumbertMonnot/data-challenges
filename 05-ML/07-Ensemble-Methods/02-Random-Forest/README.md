# Random Forest

This exercice builds onto the previous one and demonstrates the increased performance of Random Forests over single Decision Trees.

Again, you will:

- Train and score a default Random Forest
- Optimize key parameters `min_samples_split` and `max_depth` to control overfitting

You will then use the optimal model to rank features by order of importance and find the most informative one.

To start the exercise, open `Random Forest.ipynb` in `jupyter notebook` and follow the instructions.

ðŸš€ Your turn!
