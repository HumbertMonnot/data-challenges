{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:01:57.488979Z",
     "start_time": "2021-01-27T14:01:55.676791Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:01:57.634986Z",
     "start_time": "2021-01-27T14:01:57.491814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to regroup all your imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• LeWagon Kaggle Batch Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/lewagon/data-images/blob/master/ML/kaggle-batch-challenge.png?raw=true' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to your first Kaggle competition!\n",
    "\n",
    "Your objective is to **submit online an answer** to the open competition [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "- Fortunately, you have already come across the house dataset in the bootcamp!\n",
    "- You will be semi-guided up to a **baseline model**\n",
    "- Only after will you be free to improve & refine your models\n",
    "- We will approach the problem through **pipelines** (the best practice to take!)\n",
    "\n",
    "A word on Kaggle:\n",
    "- Kaggle will rank your submission amongst all participants!\n",
    "- But don't worry, everyone is publicly removed from the leaderboard after 2 months\n",
    "- You can make to 10 submissions per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Kaggle\n",
    "\n",
    "üëâ First, create an account on Kaggle if you want to participate in the competition. \n",
    "\n",
    "üëâ Then, join the [House Prices Challenge](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) \n",
    "\n",
    "üëâ Finally, send your Kaggle `username` to [this google doc](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0) and your teacher will add you to your batch team üòé \n",
    "\n",
    "**Your whole class will compete as a group against the team of TAs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already downloaded for you everything from Kaggle into your current notebook folder:\n",
    "- `train.csv` is your (1460 * 81) training set containing `X` and `y`\n",
    "- `test.csv` is your (1459 * 80) testing set without the associated target `y`!\n",
    "- `sample_submission.csv` describing the format required to submit your answer\n",
    "- `data_description.txt` describing all columns\n",
    "\n",
    "Your goal is to predict the `y_pred` missing from your test set and submit it to discover your test_score & ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Load the train dataset in a DataFrame `data` and create your `X` and `y`. Inspect their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:01:58.016614Z",
     "start_time": "2021-01-27T14:01:57.637725Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can use this direct download link if you don't want to create a Kaggle account\n",
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:01:58.023811Z",
     "start_time": "2021-01-27T14:01:58.018665Z"
    }
   },
   "outputs": [],
   "source": [
    "X = ?\n",
    "y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê£ BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial feature overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80 features is too much to deal with one-by-one for a first baseline pipeline! Let's treat them solely based on their `dtype`:\n",
    "\n",
    "‚ùì How many numerical features vs. categorical features do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:01:58.030920Z",
     "start_time": "2021-01-27T14:01:58.025776Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Store the Series `feat_categorical_nunique` containing the number of **unique values** for each categorical feature in our training set. How many unique categories are there in total ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:02:43.831732Z",
     "start_time": "2021-01-27T14:02:43.814143Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_categorical_nunique = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:02:44.803438Z",
     "start_time": "2021-01-27T14:02:44.797436Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î If we were to `OneHotEncode` all categorical features, our feature matrix `X_preproc` would become pretty big and spare, with almost 300 (highly correlated) features for only 1400 observations. Ideally, we should aim at feeding our model with 50-100 features max (üìö Read this [rule of thumb](https://datascience.stackexchange.com/a/11480/98300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 main strategies to reduce the number of categorical features post-preprocessing:\n",
    "- **[Remove](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)** features that bring too little explanation to our model. This may require statistical analysis of feature importance \n",
    "- **[Ordinally encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** (instead of one-hot-encode) categorical features into integers. However this forces a notion of \"order\" (1>2>3...) that can be detrimental if not set properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the **histogram** of number of unique value per categorical feature. Do you see some quick wins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:02:46.018511Z",
     "start_time": "2021-01-27T14:02:45.887087Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As a starter, what about simply removing all features that have **7 unique values or more**, and one-hot-encoding every others? Let's keep OrdinalEncoding and statistical feature selection for the next iteration.\n",
    "\n",
    "‚ùì Store features to OHE in a list `feat_categorical_small` below.  How many features will be OHE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:02:52.906553Z",
     "start_time": "2021-01-27T14:02:52.902568Z"
    }
   },
   "outputs": [],
   "source": [
    "# categorical features to one-hot-encode\n",
    "feat_categorical_small = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below (and clear the cell once it passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:02:57.371764Z",
     "start_time": "2021-01-27T14:02:55.766895Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('features_overview',\n",
    "    n=len(feat_categorical_small))\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline - V1 minimal baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "‚ùì Let's code the basic preprocessing pipeline described below. Save it under `preproc_baseline`\n",
    "\n",
    "For categorical features\n",
    "- Simple-Impute with most frequent values\n",
    "- One-Hot-Encode features that have less than 7 unique values to start with\n",
    "- Drop all others features\n",
    "\n",
    "\n",
    "As for numerical features\n",
    "- Simple-Impute with strategy 'mean'\n",
    "- Min-Max Scale \n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>‚ÑπÔ∏è Pro tips</summary>\n",
    "\n",
    "If you are confident, you can try sklearn's shorter syntax `make_pipeline` or `make_column_transformer` instead of the longer syntax `Pipeline` or `ColumnTransformer` if you want to avoid giving names manually to every steps.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:11.932871Z",
     "start_time": "2021-01-27T14:03:11.902515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy code your pipeline here\n",
    "\n",
    "# Then, code it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the **shape** of your preprocessed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:14.686023Z",
     "start_time": "2021-01-27T14:03:14.645812Z"
    }
   },
   "outputs": [],
   "source": [
    "shape_preproc_baseline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:17.810815Z",
     "start_time": "2021-01-27T14:03:16.764784Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('preproc_baseline',\n",
    "    shape=shape_preproc_baseline)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Pipe a basic Ridge regressor to your `preproc_baseline` and store it to `pipe_baseline` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:20.397660Z",
     "start_time": "2021-01-27T14:03:20.335738Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_baseline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Read the Kaggle [contest evaluation rules](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation):\n",
    "- Which performance metric do you need? Is it readily available in sklearn?\n",
    "- Create a scorer using [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and store it into a variable named `rmsle`\n",
    "- Create also the negative score `rmsle_neg` which is best when _maximized_. This will come handy later as `GridSearchCV` requires a score to _maximize_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:21.733506Z",
     "start_time": "2021-01-27T14:03:21.730357Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì5-fold cross_validate your `pipe_baseline` using this metric to get a first glance at your baseline perf.    \n",
    "Store your **mean** score as `score_baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:22.849371Z",
     "start_time": "2021-01-27T14:03:22.594839Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_baseline = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì predict `y_pred_baseline` from the `X_test` file given to you by Kaggle `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:24.312872Z",
     "start_time": "2021-01-27T14:03:23.920238Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Finally, store your CSV ready to be submitted as `submission_baseline.csv` in the `data` folder. Read carefully the Kaggle required format and test it below (you don't need to submit this baseline online for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:25.808955Z",
     "start_time": "2021-01-27T14:03:25.354803Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:27.406086Z",
     "start_time": "2021-01-27T14:03:26.248248Z"
    }
   },
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "tmp = pd.read_csv(\"data/submission_baseline.csv\")\n",
    "result = ChallengeResult('submission_baseline',\n",
    "    score_baseline = score_baseline,\n",
    "    submission_shape = tmp.shape,\n",
    "    submission_columns = list(tmp.columns),\n",
    "    submission_dtypes = str(list(tmp.dtypes)),\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è ITERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ üéâ Congratulation for having fully pipelined a basline model! You will see now how easier it is to iterate and improve performance üöÄ\n",
    "\n",
    "- Your goal is to improve your prediction and submit it by **16h30 max online**\n",
    "- We suggested you some improvements below\n",
    "- **Pick up your fights** and **incrementally** improve your pipeline as you see fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "- Linear Models: fine-tuning regularization to get rid of useless features ?\n",
    "- SVM / KNN: Why not? Number of observation is small enough to make sense\n",
    "- **Tree-based ensembles (must try today)**: Probably the best suited for many categorical-features problems\n",
    "- Stacking\n",
    "- (optional) XGBOOST library\n",
    "- ...\n",
    "\n",
    "**Preprocessing**\n",
    "\n",
    "- Ordinal Encoding of categorical features with a hidden notion of order in their values (e.g. \"bad\", \"average\", good\")\n",
    "- Statistical Feature Selection to remove useless features (avoid overfitting and reduce train time)\n",
    "- Predict log(SalePrice) instead?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Iteration\n",
    "‚è© **Jump directly to Model iterations first if you want to be sure to practice Ensemble Methods**\n",
    "\n",
    "‚è© Collapse me if you don't use me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the following feature below. Couldn't it be encoded numerically in a wise manner?\n",
    "```\n",
    "ExterQual: Evaluates the quality of the material on the exterior \n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Luckily, the `OrdinalEncoder` and its argument `categories`  allows us to do just that. Check it out below and make sure to understand how ths works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:34.009916Z",
     "start_time": "2021-01-27T14:03:33.998732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define specific order for features\n",
    "# Note: if you change this order, it will change the output for .transform()\n",
    "feature_A_sorted_values = ['bad', 'average', 'good'] \n",
    "feature_B_sorted_values = ['dirty', 'clean', 'new']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        feature_A_sorted_values,\n",
    "        feature_B_sorted_values\n",
    "    ],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Just some random training data\n",
    "XX = [\n",
    "    ['good', 'dirty'],\n",
    "    ['bad', 'new'],\n",
    "    ['average', 'clean'],\n",
    "]\n",
    "\n",
    "encoder.fit(XX)\n",
    "\n",
    "encoder.transform([\n",
    "        ['bad', \"dirty\"],\n",
    "        ['good', 'new'],\n",
    "        ['bad', 'oooops never seen this label before']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Your turn**: split your categorical preprocessor into\n",
    "- `preproc_ordinal` to ordinal encode\n",
    "- `preproc_nominal` to one hot encode\n",
    "\n",
    "and finalize your updated complete `preproc` pipeline\n",
    "\n",
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "- You won't avoid manually typing the name of the ordinal features and the ordering of their values in lists. Code cleanly!\n",
    "- Tips: It's a good practice to sort alphabetically your features to avoid bad surprises\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:35.110135Z",
     "start_time": "2021-01-27T14:03:35.100506Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:36.856902Z",
     "start_time": "2021-01-27T14:03:36.808155Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target engineering (‚ö†Ô∏è optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We are asked to minimize the RMS**L**E. Why don't we transform our target to directly predict its log?\n",
    "- Check-out historgram of your target `y`. Normally distributed variables should be easier to predict with linear models. \n",
    "- Create `y_log` and your new performance metrics\n",
    "- Don't forget at the end to take the exponential of your predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:38.341580Z",
     "start_time": "2021-01-27T14:03:37.983012Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Feature Selection (‚ö†Ô∏è optional - 1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to remove the least interesting features, to limit overfitting and shorten training time.  \n",
    "We will use sklearn's [feature selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) tools directly in your pipeline. Feel free to follow (or not) this semi-guided tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate feature selection based on relationship with target `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Use `SelectPercentile` along with `mutual_info_regression` to filter-out features that, - taken individually - least explain your target.  \n",
    "üìö Read more about how it works [here](https://heartbeat.fritz.ai/hands-on-with-feature-selection-techniques-filter-methods-f248e0436ce5#3e56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:42.030582Z",
     "start_time": "2021-01-27T14:03:40.716752Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering only on the `X` ?\n",
    "The following selection filter are much less powerfull than above but are easier to perform/compute. Feel free to try them out instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Check visually whether some **numerical** features almost entirely explain others (You can use Pearson's correlation combined with a heatmap). Then, remove the features that are correlated below a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:45.018445Z",
     "start_time": "2021-01-27T14:03:45.016553Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Check whether some **ordinally encoded** features are almost entirely \"ordered\" similarily than others. (You can use [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:45.799521Z",
     "start_time": "2021-01-27T14:03:45.546824Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìAnother way to filter out feature relies solely on removing those with the lowest variance.  \n",
    "Think about it: a feature which only takes one value is useless (and has a variance of 0).  \n",
    "Try to add a `VarianceThreshold` to the end of your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:48.319849Z",
     "start_time": "2021-01-27T14:03:46.941884Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final preproc pipeline\n",
    "‚ùì store here your final version of the preproc pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:03:49.539800Z",
     "start_time": "2021-01-27T14:03:49.428345Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Iteration\n",
    "‚ùì Your turn to shine\n",
    "- Improve your cross-val-score\n",
    "- Try ensemble methods\n",
    "- Keep using pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÖFINAL SUBMISSION (start at 4h30 max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:05:37.861551Z",
     "start_time": "2021-01-27T14:04:03.536Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/houses_test_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:05:37.867076Z",
     "start_time": "2021-01-27T14:04:03.680Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_stacking.fit(X,y_log)\n",
    "predictions_log = pipe_stacking.predict(X_test)\n",
    "predictions = np.exp(predictions_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:05:37.872326Z",
     "start_time": "2021-01-27T14:04:04.056Z"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.concat([X_test[\"Id\"], pd.Series(predictions, name=\"SalePrice\")], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:05:37.877574Z",
     "start_time": "2021-01-27T14:04:04.248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export to Kaggle format submission and submit it online!\n",
    "results.to_csv(\"submission_final.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T14:05:37.883834Z",
     "start_time": "2021-01-27T14:04:04.452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear the cache directory at the end\n",
    "rmtree(cachedir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
