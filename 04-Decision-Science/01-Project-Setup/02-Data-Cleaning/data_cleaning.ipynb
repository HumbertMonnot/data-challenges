{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Code get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to load all 8 csv as `pandas.DataFrame` in a single dict named `data` where each key is the name of the csv file, and each value the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 store the relative path to the `csv` folder in a `csv_path` variable. (relative to this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    \n",
    "You can check from where this notebook is being executed using\n",
    "```python\n",
    "import os\n",
    "os.getcwd()\n",
    "```\n",
    "    \n",
    "Have also a look at `os.path.join()` if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Create the list of `file_names` from all csv files in the directory, using `os.listdir()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3  Construct a function that takes a `file_name` string as input, and output a cleaner string by:\n",
    "- Removing its prefix \"olist_\" when it exists\n",
    "- Removing its suffix \"_dataset\" when it exists\n",
    "- Removing its suffix \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_from_file_name(f):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Construct the dictionnary `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its keys should look like `'sellers'` `'orders'` `'order_items'` etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Implement and save the fuction `get_data()` in `olist/data.py` that will return the dictionary `data` upon calling it as per below \n",
    "```python\n",
    "from olist.data import Olist\n",
    "Olist().get_data()\n",
    "```\n",
    "- Before rushing, take time to understand step by step what happens calling `Olist().get_data()`\n",
    "- Make extensive use of `import ipdb; ipdb.set_trace()` to investigate what happens step by step - this is a great exercice to learn to debug!\n",
    "- Your method `get_data()` needs to be callable from anywhere (the Command Terminal, this notebook, another one at a different place, etc...)\n",
    "- Using relative path will not work this time, as the current working directory `os.getcwd()` from wich relative path are computed depends on where you run the code in the first place\n",
    "- You will have to code the absolute path for the csv folder\n",
    " \n",
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "\n",
    "Have a look at `__file__` built-in python object\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "from olist.data import Olist\n",
    "Olist().get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Run an exploratory analysis with pandas profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an exploratory analysis for the sub-list of datasets below, using [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling): Create and save one HTML per dataset under a new `04-Decision-Science/reports` folder\n",
    "\n",
    "don't forget to `pip install pandas-profiling` and import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "profiling_data = ['orders', 'products', 'sellers',\n",
    "                  'customers', 'order_reviews',\n",
    "                  'order_items']\n",
    "\n",
    "# We create a new \"reports\" folder in which we will store the html reports\n",
    "!mkdir ../../data/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE: Create and save one html report per dataset (it takes some time to run!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to read the reports: \n",
    "Notice when columns have missing data (feel free to complete the list below), or add any other insights of your choice to your db.lewagon.org schema if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that have missing data\n",
    "columns_missing_data = [\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Create main matching table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our schema, it feels wise to create, ahead of our in-depth analysis, our central matching_table that will join the most important foreign keys altogether. We may re-use it often this week.\n",
    "\n",
    "‚ùìCreate the `matching_table` below. The DataFrame should have the following columns (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_matching_table = [\n",
    "    \"order_id\",\n",
    "    \"review_id\",\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"seller_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá During this week, we suggest you to name your various DataFrame in python the following convention  \n",
    "Make heavy use of \"Tab\" to auto-complete the key name of your dictionaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders = data['orders']\n",
    "# sellers = data['sellers']\n",
    "# products = data['products']\n",
    "# items = data['order_items']\n",
    "# reviews = data['order_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to risk loosing any information at that stage of pre-processing, so make sure to merge with outer joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns of interests in the various dataframes of interest, before proceeding to any merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the cardinality of each DataFrame using pd.DataFrame.shape and pd.Series.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspect the cardinality and `nunique` of the final DataFrame. It should match (114100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "‚ùìCopy your logic into `get_matching_table()` in `data.py` and test that it output exactly the same DataFrame than from your `matching_table` variable in your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olist.data import Olist\n",
    "Olist().get_matching_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_table.equals(Olist().get_matching_table())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
