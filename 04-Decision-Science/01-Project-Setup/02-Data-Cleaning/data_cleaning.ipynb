{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to load all 8 csv as `pandas.DataFrame` in a single `dict`ionary named `data` where each key is the name of the csv file, and each value the dataframe.\n",
    "\n",
    "üí°Hint: Make extensive use of the Python `os.path` package, because it computes file paths relative to the OS used (windows, unix etc...). This is more robust than hardcoding a file path as string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°Hint 2: List all files from a dir using `os.listdir`: see [stackoverflow](https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Step 1: store the path to csv folder in a `csv_path` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìStep 2: list all csv file names using `os.listdir()` and remove trailing \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìStep 3: Construct the dictionnary `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìStep 4: After checking that your code works, implement and save the fuction `get_data()` in `olist/data.py` that will return the dictionary `data` upon calling it as follows: \n",
    "\n",
    "```python\n",
    "from olist.data import Olist\n",
    "olist = Olist()\n",
    "data = olist.get_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this cell below to double check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Run an exploratory analysis with pandas profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an exploratory analysis for the sub-list of datasets below, using [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling): Create and save one HTML per dataset under a new `04-Decision-Science/reports` folder\n",
    "\n",
    "```python\n",
    "['olist_orders_dataset', 'olist_products_dataset', 'olist_customers_dataset', 'olist_order_reviews_dataset', 'olist_order_items_dataset']\n",
    "```\n",
    "\n",
    "don't forget to `pip install pandas-profiling` and import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "profiling_data = ['olist_orders_dataset', 'olist_products_dataset', \n",
    "            'olist_customers_dataset', 'olist_order_reviews_dataset', \n",
    "            'olist_order_items_dataset']\n",
    "\n",
    "# We create a new \"reports\" folder\n",
    "!mkdir ../../data/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE: Create and save one html report per dataset (it takes some time to run!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Analyze reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to read the reports.\n",
    "\n",
    "Notice when columns have missing data, or when columns will be worth converting to datetime format.\n",
    "\n",
    "Feel free to complete / double check your db.lewagon.org if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that have missing data\n",
    "columns_missing_data = [\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that should be converted to datetime\n",
    "columns_datetime = [\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Create main matching table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our schema, it feels wise to create, ahead of our in-depth analysis, our central `matching_table` that will join the most important foreign keys altogether. We may re-use it often this week.\n",
    "\n",
    "‚ùìCreate the `matching_table` below,and then copy the logic into `get_matching_table()` in `data.py`\n",
    "\n",
    "The DataFrame should have the following columns (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_matching_table = [\n",
    "    \"order_id\",\n",
    "    \"review_id\",\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"seller_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to loose any information at that stage of pre-processing, so make sure to merge with outer joints in order to keep every single orders, even those without reviews for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns of interests in the various dataframes, before proceeding to any merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the cardinality of each DataFrame with pd.DataFrame.shape and pd.Series.nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the cardinality of the final DataFrame. It should match (114100, 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.194px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
